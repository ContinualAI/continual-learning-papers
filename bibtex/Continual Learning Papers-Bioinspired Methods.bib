
@article{abbott2000,
  title = {Synaptic Plasticity: Taming the Beast},
  shorttitle = {Synaptic Plasticity},
  author = {Abbott, L F and Nelson, Sacha B},
  year = {2000},
  volume = {3},
  pages = {1178--1183},
  issn = {1546-1726},
  doi = {10.1038/81453},
  url = {https://www.nature.com/articles/nn1100_1178},
  abstract = {Synaptic plasticity provides the basis for most models of learning, memory and development in neural circuits. To generate realistic results, synapse-specific Hebbian forms of plasticity, such as long-term potentiation and depression, must be augmented by global processes that regulate overall levels of neuronal and network activity. Regulatory processes are often as important as the more intensively studied Hebbian processes in determining the consequences of synaptic plasticity for network function. Recent experimental results suggest several novel mechanisms for regulating levels of activity in conjunction with Hebbian synaptic modification. We review three of them\textemdash synaptic scaling, spike-timing dependent plasticity and synaptic redistribution\textemdash and discuss their functional implications.},
  journal = {Nature Neuroscience},
  keywords = {[hebbian]},
  language = {en},
  number = {11}
}

@article{ahmad2016,
  title = {How Do Neurons Operate on Sparse Distributed Representations? {{A}} Mathematical Theory of Sparsity, Neurons and Active Dendrites},
  author = {Ahmad, Subutai and Hawkins, Jeff},
  year = {2016},
  pages = {1--23},
  url = {http://arxiv.org/abs/1601.00720},
  abstract = {We propose a formal mathematical model for sparse representations and active dendrites in neocortex. Our model is inspired by recent experimental findings on active dendritic processing and NMDA spikes in pyramidal neurons. These experimental and modeling studies suggest that the basic unit of pattern memory in the neocortex is instantiated by small clusters of synapses operated on by localized non-linear dendritic processes. We derive a number of scaling laws that characterize the accuracy of such dendrites in detecting activation patterns in a neuronal population under adverse conditions. We introduce the union property which shows that synapses for multiple patterns can be randomly mixed together within a segment and still lead to highly accurate recognition. We describe simulation results that provide further insight into sparse representations as well as two primary results. First we show that pattern recognition by a neuron with active dendrites can be extremely accurate and robust with high dimensional sparse inputs even when using a tiny number of synapses to recognize large patterns. Second, equations representing recognition accuracy of a dendrite predict optimal NMDA spiking thresholds under a generous set of assumptions. The prediction tightly matches NMDA spiking thresholds measured in the literature. Our model matches many of the known properties of pyramidal neurons. As such the theory provides a mathematical framework for understanding the benefits and limits of sparse representations in cortical networks.},
  annotation = {\_eprint: 1601.00720},
  journal = {arXiv},
  keywords = {[hebbian],[sparsity],active dendrites,neocortex,neurons,nmda spike,sparse coding}
}

@inproceedings{aljundi2019c,
  title = {Selfless {{Sequential Learning}}},
  booktitle = {{{ICLR}}},
  author = {Aljundi, Rahaf and Rohrbach, Marcus and Tuytelaars, Tinne},
  year = {2019},
  url = {https://openreview.net/forum?id=Bkxbrn0cYX},
  abstract = {Sequential learning, also called lifelong learning, studies the problem of learning tasks in a sequence with access restricted to only the data of the current task. In this paper we look at a...},
  keywords = {[cifar],[mnist],[sparsity]},
  note = {The authors combine multiple penalizations to (1) induce sparse activations through lateral inhibitions between neurons and to (2) penalize changes in most important weights in order to prevent forgetting.}
}

@article{allred2020,
  title = {Controlled {{Forgetting}}: {{Targeted Stimulation}} and {{Dopaminergic Plasticity Modulation}} for {{Unsupervised Lifelong Learning}} in {{Spiking Neural Networks}}},
  author = {Allred, Jason M. and Roy, Kaushik},
  year = {2020},
  volume = {14},
  pages = {7},
  publisher = {{Frontiers Media S.A.}},
  issn = {1662-453X},
  doi = {10.3389/fnins.2020.00007},
  url = {https://www.frontiersin.org/article/10.3389/fnins.2020.00007/full},
  abstract = {Stochastic gradient descent requires that training samples be drawn from a uniformly random distribution of the data. For a deployed system that must learn online from an uncontrolled and unknown environment, the ordering of input samples often fails to meet this criterion, making lifelong learning a difficult challenge. We exploit the locality of the unsupervised Spike Timing Dependent Plasticity (STDP) learning rule to target local representations in a Spiking Neural Network (SNN) to adapt to novel information while protecting essential information in the remainder of the SNN from catastrophic forgetting. In our Controlled Forgetting Networks (CFNs), novel information triggers stimulated firing and heterogeneously modulated plasticity, inspired by biological dopamine signals, to cause rapid and isolated adaptation in the synapses of neurons associated with outlier information. This targeting controls the forgetting process in a way that reduces the degradation of accuracy for older tasks while learning new tasks. Our experimental results on the MNIST dataset validate the capability of CFNs to learn successfully over time from an unknown, changing environment, achieving 95.24\% accuracy, which we believe is the best unsupervised accuracy ever achieved by a fixed-size, single-layer SNN on a completely disjoint MNIST dataset.},
  journal = {Frontiers in Neuroscience},
  keywords = {[spiking],catastrophic forgetting,continual learning,controlled forgetting,dopaminergic learning,lifelong learning,Spike Timing Dependent Plasticity,Spiking Neural Networks,stability-plasticity dilemma}
}

@inproceedings{ayub2020,
  title = {Cognitively-{{Inspired Model}} for {{Incremental Learning Using}} a {{Few Examples}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}}},
  author = {Ayub, A. and Wagner, A. R.},
  year = {2020},
  url = {https://openaccess.thecvf.com/content_CVPRW_2020/html/w15/Ayub_Cognitively-Inspired_Model_for_Incremental_Learning_Using_a_Few_Examples_CVPRW_2020_paper.html},
  abstract = {Incremental learning attempts to develop a classifier which learns continuously from a stream of data segregated into different classes. Deep learning approaches suffer from catastrophic forgetting when learning classes incrementally, while most incremental learning approaches require a large amount of training data per class. We examine the problem of incremental learning using only a few training examples, referred to as Few-Shot Incremental Learning (FSIL). To solve this problem, we propose a novel approach inspired by the concept learning model of the hippocampus and the neocortex that represents each image class as centroids and does not suffer from catastrophic forgetting. We evaluate our approach on three class-incremental learning benchmarks: Caltech-101, CUBS-200-2011 and CIFAR-100 for incremental and few-shot incremental learning and show that our approach achieves state-of-the-art results in terms of classification accuracy over all learned classes.},
  keywords = {[cifar],[cubs],[dual],catastrophic forgetting,cognitively-inspired learning,continual learning}
}

@article{ayub2020a,
  title = {Storing {{Encoded Episodes}} as {{Concepts}} for {{Continual Learning}}},
  author = {Ayub, Ali and Wagner, Alan R.},
  year = {2020},
  url = {https://arxiv.org/abs/2007.06637 http://arxiv.org/abs/2007.06637},
  abstract = {The two main challenges faced by continual learning approaches are catastrophic forgetting and memory limitations on the storage of data. To cope with these challenges, we propose a novel, cognitively-inspired approach which trains autoencoders with Neural Style Transfer to encode and store images. Reconstructed images from encoded episodes are replayed when training the classifier model on a new task to avoid catastrophic forgetting. The loss function for the reconstructed images is weighted to reduce its effect during classifier training to cope with image degradation. When the system runs out of memory the encoded episodes are converted into centroids and covariance matrices, which are used to generate pseudo-images during classifier training, keeping classifier performance stable with less memory. Our approach increases classification accuracy by 13-17\% over state-of-the-art methods on benchmark datasets, while requiring 78\% less storage space.},
  annotation = {\_eprint: 2007.06637},
  journal = {arXiv},
  keywords = {[generative],[imagenet],[mnist],catastrophic forgetting,continual learning}
}

@inproceedings{coop2012,
  title = {Mitigation of Catastrophic Interference in Neural Networks Using a Fixed Expansion Layer},
  booktitle = {2012 {{IEEE}} 55th {{International Midwest Symposium}} on {{Circuits}} and {{Systems}} ({{MWSCAS}})},
  author = {Coop, Robert and Arel, Itamar},
  year = {2012},
  pages = {726--729},
  publisher = {{IEEE}},
  doi = {10.1109/MWSCAS.2012.6292123},
  url = {http://ieeexplore.ieee.org/document/6292123/},
  abstract = {In this paper we present the fixed expansion layer (FEL) feedforward neural network designed for balancing plasticity and stability in the presence of non-stationary inputs. Catastrophic interference (or catastrophic forgetting) refers to the drastic loss of previously learned information when a neural network is trained on new or different information. The goal of the FEL network is to reduce the effect of catastrophic interference by augmenting a multilayer perceptron with a layer of sparse neurons with binary activations. We compare the FEL network's performance to that of other algorithms designed to combat the effects of catastrophic interference and demonstrate that the FEL network is able to retain information for significantly longer periods of time with substantially lower computational requirements.},
  isbn = {978-1-4673-2527-1},
  keywords = {[sparsity],Accuracy,binary activations,Biological neural networks,catastrophic forgetting,catastrophic interference,Feedforward neural networks,fixed expansion layer feedforward neural network,Interference,multilayer perceptron,multilayer perceptrons,Neurons,non-stationary inputs,sparse neurons,Training},
  note = {ISSN: 1548-3746}
}

@inproceedings{coop2013,
  title = {Mitigation of Catastrophic Forgetting in Recurrent Neural Networks Using a {{Fixed Expansion Layer}}},
  booktitle = {The 2013 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  author = {Coop, Robert and Arel, Itamar},
  year = {2013},
  pages = {1--7},
  publisher = {{IEEE}},
  address = {{Dallas, TX, USA}},
  doi = {10.1109/IJCNN.2013.6707047},
  url = {http://ieeexplore.ieee.org/document/6707047/},
  abstract = {Catastrophic forgetting (or catastrophic interference) in supervised learning systems is the drastic loss of previously stored information caused by the learning of new information. While substantial work has been published on addressing catastrophic forgetting in memoryless supervised learning systems (e.g. feedforward neural networks), the problem has received limited attention in the context of dynamic systems, particularly recurrent neural networks. In this paper, we introduce a solution for mitigating catastrophic forgetting in RNNs based on enhancing the Fixed Expansion Layer (FEL) neural network which exploits sparse coding of hidden neuron activations. Simulation results on several non-stationary data sets clearly demonstrate the effectiveness of the proposed architecture.},
  isbn = {978-1-4673-6129-3 978-1-4673-6128-6},
  keywords = {[mnist],[rnn],[sparsity],fel,recurrent fel},
  language = {en}
}

@article{cui2016,
  title = {Continuous {{Online Sequence Learning}} with an {{Unsupervised Neural Network Model}}},
  author = {Cui, Yuwei and Ahmad, Subutai and Hawkins, Jeff},
  year = {2016},
  volume = {28},
  pages = {2474--2504},
  issn = {0899-7667},
  doi = {10.1162/NECO_a_00893},
  url = {https://doi.org/10.1162/NECO_a_00893},
  abstract = {The ability to recognize and predict temporal sequences of sensory inputs is vital for survival in natural environments. Based on many known properties of cortical neurons, hierarchical temporal memory (HTM) sequence memory recently has been proposed as a theoretical framework for sequence learning in the cortex. In this letter, we analyze properties of HTM sequence memory and apply it to sequence learning and prediction problems with streaming data. We show the model is able to continuously learn a large number of variable order temporal sequences using an unsupervised Hebbian-like learning rule. The sparse temporal codes formed by the model can robustly handle branching temporal sequences by maintaining multiple predictions until there is sufficient disambiguating evidence. We compare the HTM sequence memory with other sequence learning algorithms, including statistical methods\textemdash autoregressive integrated moving average; feedforward neural networks\textemdash time delay neural network and online sequential extreme learning machine; and recurrent neural networks\textemdash long short-term memory and echo-state networks on sequence prediction problems with both artificial and real-world data. The HTM model achieves comparable accuracy to other state-of-the-art algorithms. The model also exhibits properties that are critical for sequence learning, including continuous online learning, the ability to handle multiple predictions and branching sequences with high-order statistics, robustness to sensor noise and fault tolerance, and good performance without task-specific hyperparameter tuning. Therefore, the HTM sequence memory not only advances our understanding of how the brain may solve the sequence learning problem but is also applicable to real-world sequence learning problems from continuous data streams.},
  journal = {Neural Computation},
  keywords = {[spiking],htm},
  note = {Publisher: MIT Press},
  number = {11}
}

@article{garg2017,
  title = {Neurogenesis-Inspired Dictionary Learning: {{Online}} Model Adaption in a Changing World},
  author = {Garg, Sahil and Rish, Irina and Cecchi, Guillermo and Lozano, Aurelie},
  year = {2017},
  pages = {1696--1702},
  issn = {10450823},
  doi = {10.24963/ijcai.2017/235},
  url = {https://arxiv.org/abs/1701.06106},
  abstract = {We address the problem of online model adaptation when learning representations from non-stationary data streams. Specifically, we focus here on online dictionary learning (i.e. sparse linear autoencoder), and propose a simple but effective online modelselection approach involving "birth" (addition) and "death" (removal) of hidden units representing dictionary elements, in response to changing inputs; we draw inspiration from the adult neurogenesis phenomenon in the dentate gyrus of the hippocampus, known to be associated with better adaptation to new environments. Empirical evaluation on real-life datasets (images and text), as well as on synthetic data, demonstrates that the proposed approach can considerably outperform the state-of-art non-adaptive online sparse coding of [Mairal et al., 2009] in the presence of non-stationary data. Moreover, we identify certain data- and model properties associated with such improvements.},
  annotation = {\_eprint: 1701.06106},
  isbn = {9780999241103},
  journal = {IJCAI International Joint Conference on Artificial Intelligence},
  keywords = {[nlp],[vision]}
}

@inproceedings{kemker2018,
  title = {{{FearNet}}: {{Brain}}-{{Inspired Model}} for {{Incremental Learning}}},
  booktitle = {{{ICLR}}},
  author = {Kemker, Ronald and Kanan, Christopher},
  year = {2018},
  url = {https://openreview.net/pdf?id=SJ1Xmf-Rb},
  abstract = {Incremental class learning involves sequentially learning classes in bursts of examples from the same class. This violates the assumptions that underlie methods for training standard deep neural networks, and will cause them to suffer from catastrophic forgetting. Arguably, the best method for incremental class learning is iCaRL, but it requires storing training examples for each class, making it challenging to scale. Here, we propose FearNet for incremental class learning. FearNet is a generative model that does not store previous examples, making it memory efficient. FearNet uses a brain-inspired dual-memory system in which new memories are consolidated from a network for recent memories inspired by the mammalian hippocampal complex to a network for long-term storage inspired by medial prefrontal cortex. Memory consolidation is inspired by mechanisms that occur during sleep. FearNet also uses a module inspired by the basolateral amygdala for determining which memory system to use for recall. FearNet achieves state-of-the-art performance at incremental class learning on image (CIFAR-100, CUB-200) and audio classification (AudioSet) benchmarks.},
  keywords = {[audio],[cifar],[generative]}
}

@article{laborieux2021,
  title = {Synaptic Metaplasticity in Binarized Neural Networks},
  author = {Laborieux, Axel and Ernoult, Maxence and Hirtzlin, Tifenn and Querlioz, Damien},
  year = {2021},
  volume = {12},
  pages = {2549},
  publisher = {{Nature Publishing Group}},
  issn = {2041-1723},
  doi = {10.1038/s41467-021-22768-y},
  url = {https://www.nature.com/articles/s41467-021-22768-y},
  urldate = {2021-06-11},
  abstract = {While deep neural networks have surpassed human performance in multiple situations, they are prone to catastrophic forgetting: upon training a new task, they rapidly forget previously learned ones. Neuroscience studies, based on idealized tasks, suggest that in the brain, synapses overcome this issue by adjusting their plasticity depending on their past history. However, such ``metaplastic'' behaviors do not transfer directly to mitigate catastrophic forgetting in deep neural networks. In this work, we interpret the hidden weights used by binarized neural networks, a low-precision version of deep neural networks, as metaplastic variables, and modify their training technique to alleviate forgetting. Building on this idea, we propose and demonstrate experimentally, in situations of multitask and stream learning, a training technique that reduces catastrophic forgetting without needing previously presented data, nor formal boundaries between datasets and with performance approaching more mainstream techniques with task boundaries. We support our approach with a theoretical analysis on a tractable task. This work bridges computational neuroscience and deep learning, and presents significant assets for future embedded and neuromorphic systems, especially when using novel nanodevices featuring physics analogous to metaplasticity.},
  copyright = {2021 The Author(s)},
  journal = {Nature Communications},
  language = {en},
  number = {1}
}

@article{miconi2016,
  title = {Backpropagation of {{Hebbian}} Plasticity for Continual Learning},
  author = {Miconi, Thomas},
  year = {2016},
  pages = {5},
  url = {https://c38663e3-a-62cb3a1a-s-sites.googlegroups.com/site/cldlnips2016/CLDL-2016_paper_2.pdf?attachauth=ANoY7cpkpkdHxt2kA42TazATZVrBcNkcKZBbB_QkYQ2MQDe-Hz-inAnoBcb2Rl-6VCBWzWbjKjULT3tkSAtt1hdk66nh4Gy28ObAg7jKgLXNMzPTOYyB_roYB1nPaDNNkfQQhJJGXUdSexlxXDBUU0S},
  journal = {NIPS Workshop - Continual Learning},
  keywords = {hebbian,workshop},
  language = {en}
}

@inproceedings{miconi2018,
  title = {Differentiable Plasticity: Training Plastic Neural Networks with Backpropagation},
  shorttitle = {Differentiable Plasticity},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Miconi, Thomas and Stanley, Kenneth and Clune, Jeff},
  year = {2018},
  pages = {3559--3568},
  url = {http://proceedings.mlr.press/v80/miconi18a.html},
  abstract = {How can we build agents that keep learning from experience, quickly and efficiently, after their initial training? Here we take inspiration from the main mechanism of learning in biological brains:...},
  keywords = {plasticity,recurrent},
  language = {en}
}

@inproceedings{miconi2019,
  title = {Backpropamine: Training Self-Modifying Neural Networks with Differentiable Neuromodulated Plasticity},
  booktitle = {{{ICLR}}},
  author = {Miconi, Thomas and Rawal, Aditya and Clune, Jeff and Stanley, Kenneth O},
  year = {2019},
  url = {https://openreview.net/pdf?id=r1lrAiA5Ym},
  abstract = {The impressive lifelong learning in animal brains is primarily enabled by plastic changes in synaptic connectivity. Importantly, these changes are not passive, but are actively controlled by neuromodulation, which is itself under the control of the brain. The resulting self-modifying abilities of the brain play an important role in learning and adaptation, and are a major basis for biological reinforcement learning. Here we show for the first time that artificial neural networks with such neuromodulated plasticity can be trained with gradient descent. Extending previous work on differentiable Hebbian plasticity, we propose a differentiable formulation for the neuromodulation of plasticity. We show that neuromodulated plasticity improves the performance of neural networks on both reinforcement learning and supervised learning tasks. In one task, neuromodulated plastic LSTMs with millions of parameters outperform standard LSTMs on a benchmark language modeling task (controlling for the number of parameters). We conclude that differentiable neuromodulation of plasticity offers a powerful new framework for training neural networks.},
  keywords = {fashion,mnist,spiking}
}

@article{ororbia2019,
  title = {Continual {{Learning}} of {{Recurrent Neural Networks}} by {{Locally Aligning Distributed Representations}}},
  author = {Ororbia, Alexander and Mali, Ankur and Giles, C Lee and Kifer, Daniel},
  year = {2019},
  url = {http://arxiv.org/abs/1810.07411},
  abstract = {Temporal models based on recurrent neural networks have proven to be quite powerful in a wide variety of applications. However, training these models often relies on back-propagation through time, which entails unfolding the network over many time steps, making the process of conducting credit assignment considerably more challenging. Furthermore, the nature of back-propagation itself does not permit the use of non-differentiable activation functions and is inherently sequential, making parallelization of the underlying training process difficult. Here, we propose the Parallel Temporal Neural Coding Network (P-TNCN), a biologically inspired model trained by the learning algorithm we call Local Representation Alignment. It aims to resolve the difficulties and problems that plague recurrent networks trained by back-propagation through time. The architecture requires neither unrolling in time nor the derivatives of its internal activation functions. We compare our model and learning procedure to other back-propagation through time alternatives (which also tend to be computationally expensive), including real-time recurrent learning, echo state networks, and unbiased online recurrent optimization. We show that it outperforms these on sequence modeling benchmarks such as Bouncing MNIST, a new benchmark we denote as Bouncing NotMNIST, and Penn Treebank. Notably, our approach can in some instances outperform full back-propagation through time as well as variants such as sparse attentive back-tracking. Significantly, the hidden unit correction phase of P-TNCN allows it to adapt to new datasets even if its synaptic weights are held fixed (zero-shot adaptation) and facilitates retention of prior generative knowledge when faced with a task sequence. We present results that show the P-TNCN's ability to conduct zero-shot adaptation and online continual sequence modeling.},
  journal = {arXiv},
  keywords = {[mnist],[rnn],[spiking],Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computi,credi assignment},
  note = {Comment: Important revisions made throughout (additional items/results added, including a complexity analysis) arXiv: 1810.07411}
}

@article{ororbia2019a,
  title = {Lifelong {{Neural Predictive Coding}}: {{Sparsity Yields Less Forgetting}} When {{Learning Cumulatively}}},
  author = {Ororbia, Alexander and Mali, Ankur and Kifer, Daniel and Giles, C Lee},
  year = {2019},
  pages = {1--11},
  url = {http://arxiv.org/abs/1905.10696},
  abstract = {In lifelong learning systems, especially those based on artificial neural networks, one of the biggest obstacles is the severe inability to retain old knowledge as new information is encountered. This phenomenon is known as catastrophic forgetting. In this paper, we present a new connectionist model, the Sequential Neural Coding Network, and its learning procedure, grounded in the neurocognitive theory of predictive coding. The architecture experiences significantly less forgetting as compared to standard neural models and outperforms a variety of previously proposed remedies and methods when trained across multiple task datasets in a stream-like fashion. The promising performance demonstrated in our experiments offers motivation that directly incorporating mechanisms prominent in real neuronal systems, such as competition, sparse activation patterns, and iterative input processing, can create viable pathways for tackling the challenge of lifelong machine learning.},
  annotation = {\_eprint: 1905.10696},
  journal = {arXiv},
  keywords = {[fashion],[mnist],[sparsity]}
}

@article{ororbia2020,
  title = {Spiking {{Neural Predictive Coding}} for {{Continual Learning}} from {{Data Streams}}},
  author = {Ororbia, Alexander},
  year = {2020},
  url = {http://arxiv.org/abs/1908.08655},
  abstract = {For energy-efficient computation in specialized neuromorphic hardware, we present the Spiking Neural Coding Network, an instantiation of a family of artificial neural models strongly motivated by the theory of predictive coding. The model, in essence, works by operating in a never-ending process of "guess-and-check", where neurons predict the activity values of one another and then immediately adjust their own activities to make better future predictions. The interactive, iterative nature of our neural system fits well into the continuous time formulation of data sensory stream prediction and, as we show, the model's structure yields a simple, local synaptic update rule, which could be used to complement or replace online spike-timing dependent plasticity. In this article, we experiment with an instantiation of our model that consists of leaky integrate-and-fire units. However, the general framework within which our model is situated can naturally incorporate more complex, formal neurons such as the Hodgkin-Huxley model. Our experimental results in pattern recognition demonstrate the potential of the proposed model when binary spike trains are the primary paradigm for inter-neuron communication. Notably, our model is competitive in terms of classification performance, can conduct online semi-supervised learning, naturally experiences less forgetting when learning from a sequence of tasks, and is more computationally economical and biologically-plausible than popular artificial neural networks.},
  journal = {arXiv},
  keywords = {[spiking],Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computi,Quantitative Biology - Neurons and Cognition},
  note = {Comment: Revised version of manuscript \textendash{} includes updated experimental results arXiv: 1908.08655}
}

@article{parisi2018,
  title = {Lifelong {{Learning}} of {{Spatiotemporal Representations With Dual}}-{{Memory Recurrent Self}}-{{Organization}}},
  author = {Parisi, German I and Tani, Jun and Weber, Cornelius and Wermter, Stefan},
  year = {2018},
  volume = {12},
  issn = {1662-5218},
  doi = {10.3389/fnbot.2018.00078},
  url = {https://www.frontiersin.org/articles/10.3389/fnbot.2018.00078/full},
  abstract = {Artificial autonomous agents and robots interacting in complex environments are required to continually acquire and fine-tune knowledge over sustained periods of time. The ability to learn from continuous streams of information is referred to as lifelong learning and represents a long-standing challenge for neural network models due to catastrophic forgetting in which novel sensory experience interferes with existing representations and leads to abrupt decreases in the performance on previously acquired knowledge. Computational models of lifelong learning typically alleviate catastrophic forgetting in experimental scenarios with given datasets of static images and limited complexity, thereby differing significantly from the conditions artificial agents are exposed to. In more natural settings, sequential information may become progressively available over time and access to previous experience may be restricted. Therefore, specialized neural network mechanisms are required that adapt to novel sequential experience while preventing disruptive interference with existing representations. In this paper, we propose a dual-memory self-organizing architecture for lifelong learning scenarios. The architecture comprises two growing recurrent networks with the complementary tasks of learning object instances (episodic memory) and categories (semantic memory). Both growing networks can expand in response to novel sensory experience: the episodic memory learns fine-grained spatiotemporal representations of object instances in an unsupervised fashion while the semantic memory uses task-relevant signals to regulate structural plasticity levels and develop more compact representations from episodic experience. For the consolidation of knowledge in the absence of external sensory input, the episodic memory periodically replays trajectories of neural reactivations. We evaluate the proposed model on the CORe50 benchmark dataset for continuous object recognition, showing that we significantly outperform current methods of lifelong learning in three different incremental learning scenarios.},
  journal = {Frontiers in Neurorobotics},
  keywords = {[core50],[dual],[rnn],[som],CLS,Incremental Learning,Lifelong learning,Memory,object recognition systems,Self-organizing Network},
  language = {English}
}

@incollection{shrestha2018,
  title = {{{SLAYER}}: {{Spike Layer Error Reassignment}} in {{Time}}},
  shorttitle = {{{SLAYER}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 31},
  author = {Shrestha, Sumit Bam and Orchard, Garrick},
  editor = {Bengio, S and Wallach, H and Larochelle, H and Grauman, K and {Cesa-Bianchi}, N and Garnett, R},
  year = {2018},
  pages = {1412--1421},
  publisher = {{Curran Associates, Inc.}},
  url = {http://papers.nips.cc/paper/7415-slayer-spike-layer-error-reassignment-in-time.pdf}
}

@inproceedings{srivastava2013,
  title = {Compete to {{Compute}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 26},
  author = {Srivastava, Rupesh Kumar and Masci, Jonathan and Kazerounian, Sohrob and Gomez, Faustino and Schmidhuber, J{\"u}rgen},
  year = {2013},
  url = {http://papers.nips.cc/paper/5059-compete-to-compute.pdf},
  abstract = {Local competition among neighboring neurons is common in biological neu-ral networks (NNs). In this paper, we apply the concept to gradient-based, backprop-trained artificial multilayer NNs. NNs with competing linear units tend to outperform those with non-competing nonlinear units, and avoid catastrophic forgetting when training sets change over time.},
  keywords = {[mnist],[sparsity]}
}

@inproceedings{vandeven2020a,
  title = {Brain-like {{Replay}} for {{Continual Learning}} with {{Artificial Neural Networks}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{Workshop}} on {{Bridging AI}} and {{Cognitive Science}})},
  author = {{van de Ven}, Gido M. and Siegelmann, Hava T. and Tolias, Andreas S.},
  year = {2020},
  url = {https://baicsworkshop.github.io/pdf/BAICS_8.pdf},
  abstract = {Artificial neural networks suffer from catastrophic forgetting. Unlike humans, when these networks are trained on something new, they rapidly forget what was learned before. In the brain, a mechanism thought to be important for protecting memories is the replay of neuronal activity patterns representing those memories. In artificial neural networks, such memory replay has been implemented in the form of `generative replay', which can successfully prevent catastrophic forgetting in a range of toy examples. Scaling up generative replay to problems with more complex inputs, however, turns out to be challenging. We propose a new, more brain-like variant of replay in which internal or hidden representations are replayed that are generated by the network's own, context-modulated feedback connections. In contrast to established continual learning methods, our method achieves acceptable performance on the challenging problem of class-incremental learning on natural images without relying on stored data.},
  keywords = {[cifar]}
}

@article{velez2017,
  title = {Diffusion-Based Neuromodulation Can Eliminate Catastrophic Forgetting in Simple Neural Networks},
  author = {Velez, Roby and Clune, Jeff},
  year = {2017},
  volume = {12},
  pages = {1--31},
  issn = {19326203},
  doi = {10.1371/journal.pone.0187736},
  url = {http://arxiv.org/abs/1705.07241 http://dx.doi.org/10.1371/journal.pone.0187736},
  abstract = {A long-term goal of AI is to produce agents that can learn a diversity of skills throughout their lifetimes and continuously improve those skills via experience. A longstanding obstacle towards that goal is catastrophic forgetting, which is when learning new information erases previously learned information. Catastrophic forgetting occurs in artificial neural networks (ANNs), which have fueled most recent advances in AI. A recent paper proposed that catastrophic forgetting in ANNs can be reduced by promoting modularity, which can limit forgetting by isolating task information to specific clusters of nodes and connections (functional modules). While the prior work did show that modular ANNs suffered less from catastrophic forgetting, it was not able to produce ANNs that possessed task-specific functional modules, thereby leaving the main theory regarding modularity and forgetting untested. We introduce diffusion-based neuromodulation, which simulates the release of diffusing, neuromodulatory chemicals within an ANN that can modulate (i.e. up or down regulate) learning in a spatial region. On the simple diagnostic problem from the prior work, diffusion-based neuromodulation 1) induces task-specific learning in groups of nodes and connections (task-specific localized learning), which 2) produces functional modules for each subtask, and 3) yields higher performance by eliminating catastrophic forgetting. Overall, our results suggest that diffusion-based neuromodulation promotes task-specific localized learning and functional modularity, which can help solve the challenging, but important problem of catastrophic forgetting.},
  annotation = {\_eprint: 1705.07241},
  isbn = {1111111111},
  journal = {PLoS ONE},
  number = {11}
}


