
@article{carpenter1988,
  title = {The {{ART}} of {{Adaptive Pattern Recognition}} by a {{Self}}-{{Organizing Neural Network}}},
  author = {Carpenter, Gail A. and Grossberg, Stephen},
  year = {1988},
  volume = {21},
  pages = {77--88},
  issn = {00189162},
  doi = {10.1109/2.33},
  url = {https://ieeexplore.ieee.org/document/33},
  abstract = {The adaptive resonance theory (ART) suggests a solution to the stability-plasticity dilemma facing designers of learning systems, namely how to design a learning system that will remain plastic, or adaptive, in response to significant events and yet remain stable in response to irrelevant events. ART architectures are discussed that are neural networks that self-organize stable recognition codes in real time in response to arbitrary sequences of input patterns. Within such an ART architecture, the process of adaptive pattern recognition is a special case of the more general cognitive process of hypothesis discovery, testing, search, classification, and learning. This property opens up the possibility of applying ART systems to more general problems of adaptively processing large abstract information sources and databases. The main computational properties of these ART architectures are outlined and contrasted with those of alternative learning and recognition systems.\textbackslash textless \textbackslash textgreater},
  journal = {Computer},
  note = {Seminal paper on the stability-plasticity dilemma.},
  number = {3}
}

@inproceedings{french1991,
  title = {Using {{Semi}}-{{Distributed Representations}} to {{Overcome Catastrophic Forgetting}} in {{Connectionist Networks}}},
  booktitle = {In {{Proceedings}} of the 13th {{Annual Cognitive Science Society Conference}}},
  author = {French, Robert},
  year = {1991},
  pages = {173--178},
  publisher = {{Erlbaum}},
  url = {https://www.aaai.org/Papers/Symposia/Spring/1993/SS-93-06/SS93-06-007.pdf},
  abstract = {In connectionist networks, newly-learned information destroys previously-learned information unless the network is continually retrained on the old information. This behavior, known as catastrophic forgetting, is unacceptable both for practical purposes and as a model of mind. This paper advances the claim that catastrophic forgetting is a direct consequence of the overlap of the system's distributed representations and can be reduced by reducing this overlap. A simple algorithm is presented that allows a standard feedforward backpropagation network to develop semi-distributed representations, thereby significantly reducing the problem of catastrophic forgetting. 1 Introduction Catastrophic forgetting is the inability of a neural network to retain old information in the presence of new. New information destroys old unless the old information is continually relearned by the net. McCloskey \& Cohen [1990] and Ratcliff [1989] have demonstrated that this is a serious problem with c...},
  keywords = {[sparsity],activation sharpening}
}

@article{french1997,
  title = {Pseudo-Recurrent {{Connectionist Networks}}: {{An Approach}} to the '{{Sensitivity}}-{{Stability}}' {{Dilemma}}},
  shorttitle = {Pseudo-Recurrent {{Connectionist Networks}}},
  author = {French, Robert},
  year = {1997},
  volume = {9},
  pages = {353--380},
  issn = {0954-0091, 1360-0494},
  doi = {10.1080/095400997116595},
  url = {http://www.tandfonline.com/doi/abs/10.1080/095400997116595},
  abstract = {In order to solve the ``sensitivity-stability'' problem \textemdash{} and its immediate correlate, the problem of sequential learning \textemdash{} it is crucial to develop connectionist architectures that are simultaneously sensitive to, but not excessively disrupted by, new input. French (1992) suggested that to alleviate a particularly severe form of this disruption, catastrophic forgetting, it was necessary for networks to dynamically separate their internal representations during learning. McClelland, McNaughton, \& O'Reilly (1995) went even further. They suggested that nature's way of implementing this obligatory separation was the evolution of two separate areas of the brain, the hippocampus and the neocortex. In keeping with this idea of radical separation, a ``pseudo-recurrent'' memory model is presented here that partitions a connectionist network into two functionally distinct, but continually interacting areas. One area serves as a final-storage area for representations; the other is an early-processing area where new representations are first learned by the system. The final-storage area continually supplies internally generated patterns (pseudopatterns, Robins (1995)), which are approximations of its content, to the early-processing area, where they are interleaved with the new patterns to be learned. Transfer of the new learning is done either by weight-copying from the early-processing area to the final-storage area or by pseudopattern transfer. A number of experiments are presented that demonstrate the effectiveness of this approach, allowing, in particular, effective sequential learning with gradual forgetting in the presence of new input. Finally, it is shown that the two interacting areas automatically produce representational compaction and it is suggested that similar representational streamlining may exist in the brain.},
  journal = {Connection Science},
  keywords = {[dual],Catastrophic Interference,dilemma,Dual Memory,Keywords: Pseudopatterns,plasticity,Semi-distributed Representations,Sensitivity-stability Transfer,stability},
  language = {en},
  note = {In this seminal paper the author introduces many different forms of rehearsal in order to mitigate the catastrophic forgetting phenomenon},
  number = {4}
}

@article{grossberg1980,
  title = {How Does a Brain Build a Cognitive Code?},
  author = {Grossberg, Stephen},
  year = {1980},
  volume = {87},
  pages = {1--51},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-1471(Electronic),0033-295X(Print)},
  doi = {10.1037/0033-295X.87.1.1},
  url = {https://psycnet.apa.org/record/1980-06768-001},
  abstract = {Discusses how competition between afferent data and learned feedback expectancies can stabilize a developing code by buffering committed populations of detectors against continual erosion by new environmental demands. The gating phenomena that result lead to dynamically maintained critical periods and to attentional phenomena such as overshadowing in the adult. The functional unit of cognitive coding is suggested to be an adaptive resonance, or amplification and prolongation of neural activity, that occurs when afferent data and efferent expectancies reach consensus through a matching process. The resonant state embodies the perceptual event, and its amplified and sustained activities are capable of driving slow changes of long-term memory. These mechanisms help to explain and predict (a) positive and negative aftereffects, the McCollough effect, spatial frequency adaptation, monocular rivalry, binocular rivalry and hysteresis, pattern completion, and Gestalt switching; (b) analgesia, partial reinforcement acquisition effect, conditioned reinforcers, underaroused vs overaroused depression; (c) the contingent negative variation, P300, and pontogeniculo-occipital waves; and (d) olfactory coding, corticogeniculate feedback, matching of proprioceptive and terminal motor maps, and cerebral dominance. (125 ref) (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  journal = {Psychological Review},
  keywords = {Attention,Cognitive Processes,Electrical Activity,Expectations,Human Information Storage,Neurophysiology},
  note = {It introduces the stability-plasticity dilemma related to the catastrophic forgetting.},
  number = {1}
}

@book{hebb2002,
  title = {The {{Organization}} of {{Behavior}}: {{A Neuropsychological Theory}}},
  shorttitle = {The {{Organization}} of {{Behavior}}},
  author = {Hebb, D O},
  year = {2002},
  publisher = {{Psychology Press}},
  url = {https://www.amazon.com/Organization-Behavior-Neuropsychological-Theory/dp/0805843000 https://books.google.it/books/about/The_Organization_of_Behavior.html?id=ddB4AgAAQBAJ&printsec=frontcover&source=kp_read_button&redir_esc=y#v=onepage&q&f=false},
  abstract = {Since its publication in 1949, D.O. Hebb's, The Organization of Behavior has been one of the most influential books in the fields of psychology and neuroscience. However, the original edition has been unavailable since 1966, ensuring that Hebb's comment that a classic normally means "cited but not read" is true in his case. This new edition rectifies a long-standing problem for behavioral neuroscientists\textendash the inability to obtain one of the most cited publications in the field. The Organization of Behavior played a significant part in stimulating the investigation of the neural foundations of behavior and continues to be inspiring because it provides a general framework for relating behavior to synaptic organization through the dynamics of neural networks. D.O. Hebb was also the first to examine the mechanisms by which environment and experience can influence brain structure and function, and his ideas formed the basis for work on enriched environments as stimulants for behavioral development. References to Hebb, the Hebbian cell assembly, the Hebb synapse, and the Hebb rule increase each year. These forceful ideas of 1949 are now applied in engineering, robotics, and computer science, as well as neurophysiology, neuroscience, and psychology\textendash a tribute to Hebb's foresight in developing a foundational neuropsychological theory of the organization of behavior.},
  isbn = {978-1-135-63191-8},
  journal = {Lawrence Erlbaum},
  keywords = {[hebbian],Psychology / Cognitive Psychology \& Cognition,Psychology / General,Psychology / Neuropsychology,Psychology / Physiological Psychology},
  language = {en}
}

@article{ring1997,
  title = {{{CHILD}}: {{A First Step Towards Continual Learning}}},
  shorttitle = {{{CHILD}}},
  author = {Ring, Mark B},
  year = {1997},
  volume = {28},
  pages = {77--104},
  issn = {1573-0565},
  doi = {10.1023/A:1007331723572},
  url = {https://doi.org/10.1023/A:1007331723572},
  abstract = {Continual learning is the constant development of increasingly complex behaviors; the process of building more complicated skills on top of those already developed. A continual-learning agent should therefore learn incrementally and hierarchically. This paper describes CHILD, an agent capable of Continual, Hierarchical, Incremental Learning and Development. CHILD can quickly solve complicated non-Markovian reinforcement-learning tasks and can then transfer its skills to similar but even more complicated tasks, learning these faster still.},
  journal = {Machine Learning},
  keywords = {cl,continual learner,Continual learning,definition,hierarchical neural networks,reinforcement learning,sequence learning,transfer},
  language = {en},
  number = {1}
}

@inproceedings{thrun1996a,
  title = {Is {{Learning The}} N-Th {{Thing Any Easier Than Learning The First}}?},
  booktitle = {Advances in {{Neural Information Processing Systems}} 8},
  author = {Thrun, Sebastian},
  editor = {Touretzky, D S and Mozer, M C and Hasselmo, M E},
  year = {1996},
  pages = {640--646},
  publisher = {{MIT Press}},
  url = {http://papers.nips.cc/paper/1034-is-learning-the-n-th-thing-any-easier-than-learning-the-first.pdf},
  keywords = {[vision],lifelong,lifelong learning}
}

@article{widmer1996,
  title = {Learning in the Presence of Concept Drift and Hidden Contexts},
  author = {Widmer, Gerhard and Kubat, Miroslav},
  year = {1996},
  volume = {23},
  pages = {69--101},
  issn = {0885-6125},
  doi = {10.1007/BF00116900},
  url = {https://doi.org/10.1007/BF00116900 http://link.springer.com/10.1007/BF00116900},
  abstract = {On-line learning in domains where the target concept depends on some hidden context poses serious problems. A changing context can induce changes in the target concepts, producing what is known as concept drift. We describe a family of learning algorithms that flexibly react to concept drift and can take advantage of situations where contexts reappear. The general approach underlying all these algorithms consists of (1) keeping only a window of currently trusted examples and hypotheses; (2) storing concept descriptions and reusing them when a previous context re-appears; and (3) controlling both of these functions by a heuristic that constantly monitors the system's behavior. The paper reports on experiments that test the systems' perfomance under various conditions such as different levels of noise and different extent and rate of concept drift.},
  journal = {Machine Learning},
  language = {en},
  number = {1}
}


