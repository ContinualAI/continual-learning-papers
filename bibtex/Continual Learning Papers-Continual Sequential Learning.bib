
@article{ans2004,
  title = {Self-Refreshing Memory in Artificial Neural Networks: Learning Temporal Sequences without Catastrophic Forgetting},
  shorttitle = {Self-Refreshing Memory in Artificial Neural Networks},
  author = {Ans, Bernard and Rousset, St{\'e}phane and French, Robert M. and Musca, Serban},
  year = {2004},
  journal = {Connection Science},
  volume = {16},
  number = {2},
  pages = {71--99},
  publisher = {{Taylor \& Francis}},
  issn = {0954-0091},
  doi = {10/bbz9jp},
  url = {https://doi.org/10.1080/09540090412331271199},
  urldate = {2021-01-08},
  abstract = {While humans forget gradually, highly distributed connectionist networks forget catastrophically: newly learned information often completely erases previously learned information. This is not just implausible cognitively, but disastrous practically. However, it is not easy in connectionist cognitive modelling to keep away from highly distributed neural networks, if only because of their ability to generalize. A realistic and effective system that solves the problem of catastrophic interference in sequential learning of `static' (i.e. non-temporally ordered) patterns has been proposed recently (Robins 1995, Connection Science, 7: 123\textendash 146, 1996, Connection Science, 8: 259\textendash 275, Ans and Rousset 1997, CR Acad\'emie des Sciences Paris, Life Sciences, 320: 989\textendash 997, French 1997, Connection Science, 9: 353\textendash 379, 1999, Trends in Cognitive Sciences, 3: 128\textendash 135, Ans and Rousset 2000, Connection Science, 12: 1\textendash 19). The basic principle is to learn new external patterns interleaved with internally generated `pseudopatterns' (generated from random activation) that reflect the previously learned information. However, to be credible, this self-refreshing mechanism for static learning has to encompass our human ability to learn serially many temporal sequences of patterns without catastrophic forgetting. Temporal sequence learning is arguably more important than static pattern learning in the real world. In this paper, we develop a dual-network architecture in which self-generated pseudopatterns reflect (non-temporally) all the sequences of temporally ordered items previously learned. Using these pseudopatterns, several self-refreshing mechanisms that eliminate catastrophic forgetting in sequence learning are described and their efficiency is demonstrated through simulations. Finally, an experiment is presented that evidences a close similarity between human and simulated behaviour.},
  keywords = {[rnn],/unread,\#nosource,artificial neural networks,catastrophic forgetting,self-refreshing memory,temporal sequence learning},
  annotation = {\_eprint: https://doi.org/10.1080/09540090412331271199}
}

@inproceedings{cossu2020,
  title = {Continual {{Learning}} with {{Gated Incremental Memories}} for Sequential Data Processing},
  booktitle = {Proceedings of the 2020 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}} 2020)},
  author = {Cossu, Andrea and Carta, Antonio and Bacciu, Davide},
  year = {2020},
  url = {http://arxiv.org/abs/2004.04077},
  abstract = {The ability to learn in dynamic, nonstationary environments without forgetting previous knowledge, also known as Continual Learning (CL), is a key enabler for scalable and trustworthy deployments of adaptive solutions. While the importance of continual learning is largely acknowledged in machine vision and reinforcement learning problems, this is mostly under-documented for sequence processing tasks. This work proposes a Recurrent Neural Network (RNN) model for CL that is able to deal with concept drift in input distribution without forgetting previously acquired knowledge. We also implement and test a popular CL approach, Elastic Weight Consolidation (EWC), on top of two different types of RNNs. Finally, we compare the performances of our enhanced architecture against EWC and RNNs on a set of standard CL benchmarks, adapted to the sequential data processing scenario. Results show the superior performance of our architecture and highlight the need for special solutions designed to address CL in RNNs.},
  keywords = {[mnist],[rnn],/unread,\#nosource,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computi,Statistics - Machine Learning},
  note = {An evaluation of RNNs (LSTM and LMN) inspired by Progressive networks, leading to the Gated Incremental Memory approach to overcome catastrophic forgetting.
\par
An evaluation of RNNs (LSTM and LMN) inspired by Progressive networks, leading to the Gated Incremental Memory approach to overcome catastrophic forgetting.}
}

@article{cossu2021,
  title = {Continual Learning for Recurrent Neural Networks: {{An}} Empirical Evaluation},
  shorttitle = {Continual Learning for Recurrent Neural Networks},
  author = {Cossu, Andrea and Carta, Antonio and Lomonaco, Vincenzo and Bacciu, Davide},
  year = {2021},
  journal = {Neural Networks},
  volume = {143},
  pages = {607--627},
  issn = {0893-6080},
  doi = {10/gnq33k},
  url = {https://www.sciencedirect.com/science/article/pii/S0893608021002847},
  urldate = {2021-08-12},
  abstract = {Learning continuously during all model lifetime is fundamental to deploy machine learning solutions robust to drifts in the data distribution. Advances in Continual Learning (CL) with recurrent neural networks could pave the way to a large number of applications where incoming data is non stationary, like natural language processing and robotics. However, the existing body of work on the topic is still fragmented, with approaches which are application-specific and whose assessment is based on heterogeneous learning protocols and datasets. In this paper, we organize the literature on CL for sequential data processing by providing a categorization of the contributions and a review of the benchmarks. We propose two new benchmarks for CL with sequential data based on existing datasets, whose characteristics resemble real-world applications. We also provide a broad empirical evaluation of CL and Recurrent Neural Networks in class-incremental scenario, by testing their ability to mitigate forgetting with a number of different strategies which are not specific to sequential data processing. Our results highlight the key role played by the sequence length and the importance of a clear specification of the CL scenario.},
  langid = {english},
  keywords = {[rnn],/unread,\#nosource,Benchmarks,Continual learning,Evaluation,Recurrent neural networks}
}

@inproceedings{duncker2020,
  title = {Organizing Recurrent Network Dynamics by Task-Computation to Enable Continual Learning},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Duncker, Lea and Driscoll, Laura N and Shenoy, Krishna V and Sahani, Maneesh and Sussillo, David},
  year = {2020},
  volume = {33},
  url = {https://proceedings.neurips.cc/paper/2020/file/a576eafbce762079f7d1f77fca1c5cc2-Paper.pdf},
  abstract = {Biological systems face dynamic environments that require continual learning. It is not well understood how these systems balance the tension between flexibility for learning and robustness for memory of previous behaviors. Continual learning without catastrophic interference also remains a challenging problem in machine learning. Here, we develop a novel learning rule designed to minimize interference between sequentially learned tasks in recurrent networks. Our learning rule preserves network dynamics within activity-defined subspaces used for previously learned tasks. It encourages dynamics associated with new tasks that might otherwise interfere to instead explore orthogonal subspaces, and it allows for reuse of previously established dynamical motifs where possible. Employing a set of tasks used in neuroscience, we demonstrate that our approach successfully eliminates catastrophic interference and offers a substantial improvement over previous continual learning algorithms. Using dynamical systems analysis, we show that networks trained using our approach can reuse similar dynamical structures across similar tasks. This possibility for shared computation allows for faster learning during sequential training. Finally, we identify organizational differences that emerge when training tasks sequentially versus simultaneously.},
  keywords = {[rnn],/unread,\#nosource,⛔ No DOI found},
  note = {The hidden state pre-activation and the input are projected during learning in a subspace orthogonal to all the ones of the previous tasks, if new task is dissimilar. Projection on orthogonal subspace avoid interference with previous knowledge.}
}

@inproceedings{french1997,
  title = {Using {{Pseudo-Recurrent Connectionist Networks}} to {{Solve}} the {{Problem}} of {{Sequential Learning}}},
  booktitle = {Proceedings of the 19th {{Annual Cognitive Science Society Conference}}},
  author = {French, Robert},
  year = {1997},
  url = {http://leadserv.u-bourgogne.fr/IMG/pdf/psdnet1.pdf},
  abstract = {A major problem facing connectionist models of memory is  how to make them simultaneously sensitive to, but not  disrupted by, new input. This paper describes one solution  to this problem. The resulting connectionist architecture is  capable of sequential learning and exhibits gradual  forgetting where standard connectionist architectures may  forget catastrophically. The proposed architecture relies on  separating previously learned representations from those  that are currently being learned. Crucially, a method is  described in which approximations of the previously  learned data, called pseudopatterns, will be extracted from  the network and mixed in with the new patterns to be  learned, thereby alleviating sudden forgetting caused by  new learning and allowing the network to forget gracefully.  Introduction  One of the most important problems facing connectionist models of memory --- in fact, facing any model of memory --- is how to make them simultaneously sensitive to, but not ...},
  keywords = {[dual],/unread,\#nosource,⛔ No DOI found}
}

@inproceedings{joseph2020,
  title = {Meta-{{Consolidation}} for {{Continual Learning}}},
  booktitle = {{{NeurIPS}}},
  author = {Joseph, K J and Balasubramanian, Vineeth N},
  year = {2020},
  url = {http://arxiv.org/abs/2010.00352},
  abstract = {The ability to continuously learn and adapt itself to new tasks, without losing grasp of already acquired knowledge is a hallmark of biological learning systems, which current deep learning systems fall short of. In this work, we present a novel methodology for continual learning called MERLIN: Meta-Consolidation for Continual Learning. We assume that weights of a neural network \$\textbackslash backslashboldsymbol \textbackslash backslashpsi\$, for solving task \$\textbackslash backslashboldsymbol t\$, come from a meta-distribution \$p(\textbackslash backslashboldsymbol\{\textbackslash backslashpsi|t\})\$. This meta-distribution is learned and consolidated incrementally. We operate in the challenging online continual learning setting, where a data point is seen by the model only once. Our experiments with continual learning benchmarks of MNIST, CIFAR-10, CIFAR-100 and Mini-ImageNet datasets show consistent improvement over five baselines, including a recent state-of-the-art, corroborating the promise of MERLIN.},
  keywords = {[bayes],[cifar],[imagenet],[mnist],/unread,\#nosource},
  annotation = {\_eprint: 2010.00352},
  note = {The authors leverage a bayesian framework in which the parameters of a model are sampled from a generating distribution. This distribution, parameterized by a task label, is used together with a VAE to consolidate online previous and current knowledge. Inference does not require task labels and exploit an ensemble of model, sampled from the generating distribution.
\par
The authors leverage a bayesian framework in which the parameters of a model are sampled from a generating distribution. This distribution, parameterized by a task label, is used together with a VAE to consolidate online previous and current knowledge. Inference does not require task labels and exploit an ensemble of model, sampled from the generating distribution.}
}

@inproceedings{li2020,
  title = {Compositional {{Language Continual Learning}}},
  booktitle = {Eighth {{International Conference}} on {{Learning Representations}}},
  author = {Li, Yuanpeng and Zhao, Liang and Church, Kenneth and Elhoseiny, Mohamed},
  year = {2020},
  url = {https://iclr.cc/virtual_2020/poster_rklnDgHtDS.html},
  urldate = {2021-01-01},
  abstract = {Motivated by the human's ability to continually learn and gain knowledge over time, several research efforts have been pushing the limits of machines to constantly learn while alleviating catastrophic forgetting. Most of the existing methods have been focusing on continual learning of label prediction tasks, which have fixed input and output sizes. In this paper, we propose a new scenario of continual learning which handles sequence-to-sequence tasks common in language learning. We further propose an approach to use label prediction continual learning algorithm for sequence-to-sequence continual learning by leveraging compositionality. Experimental results show that the proposed method has significant improvement over state-of-the-art methods. It enables knowledge transfer and prevents catastrophic forgetting, resulting in more than 85\% accuracy up to 100 stages, compared with less than 50\% accuracy for baselines in instruction learning task. It also shows significant improvement in machine translation task. This is the first work to combine continual learning and compositionality for language learning, and we hope this work will make machines more helpful in various tasks.},
  langid = {english},
  keywords = {[nlp],[rnn],/unread,\#nosource}
}

@inproceedings{maltoni2016,
  title = {Semi-Supervised Tuning from Temporal Coherence},
  booktitle = {2016 23rd {{International Conference}} on {{Pattern Recognition}} ({{ICPR}})},
  author = {Maltoni, Davide and Lomonaco, Vincenzo},
  year = {2016},
  pages = {2509--2514},
  doi = {10/gnq33p},
  url = {https://ieeexplore.ieee.org/document/7900013},
  abstract = {Recent works demonstrated the usefulness of temporal coherence to regularize supervised training or to learn invariant features with deep architectures. In particular, enforcing a smooth output change while presenting temporally-closed frames from video sequences, proved to be an effective strategy. In this paper we prove the efficacy of temporal coherence for semi-supervised incremental tuning. We show that a deep architecture, just mildly trained in a supervised manner, can progressively improve its classification accuracy, if exposed to video sequences of unlabeled data. The extent to which, in some cases, a semi-supervised tuning allows to improve classification accuracy (approaching the supervised one) is somewhat surprising. A number of control experiments pointed out the fundamental role of temporal coherence.},
  keywords = {/unread,\#nosource,Cameras,Coherence,deep learning,Feature extraction,incremental tuning,Optimization,self-training,temporal coherence,Training,Tuning,Video sequences}
}

@article{ororbia2021,
  title = {Continual {{Competitive Memory}}: {{A Neural System}} for {{Online Task-Free Lifelong Learning}}},
  shorttitle = {Continual {{Competitive Memory}}},
  author = {Ororbia, Alexander G.},
  year = {2021},
  journal = {arXiv},
  eprint = {2106.13300},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2106.13300},
  urldate = {2021-06-30},
  abstract = {In this article, we propose a novel form of unsupervised learning, continual competitive memory (CCM), as well as a computational framework to unify related neural models that operate under the principles of competition. The resulting neural system is shown to offer an effective approach for combating catastrophic forgetting in online continual classification problems. We demonstrate that the proposed CCM system not only outperforms other competitive learning neural models but also yields performance that is competitive with several modern, state-of-the-art lifelong learning approaches on benchmarks such as Split MNIST and Split NotMNIST. CCM yields a promising path forward for acquiring representations that are robust to interference from data streams, especially when the task is unknown to the model and must be inferred without external guidance.},
  archiveprefix = {arXiv},
  keywords = {/unread,\#nosource,⛔ No DOI found,Computer Science - Machine Learning}
}

@article{parisi2020,
  title = {Online {{Continual Learning}} on {{Sequences}}},
  author = {Parisi, German I and Lomonaco, Vincenzo},
  year = {2020},
  journal = {Studies in Computational Intelligence},
  doi = {10/gnq33q},
  url = {http://arxiv.org/abs/2003.09114},
  abstract = {Online continual learning (OCL) refers to the ability of a system to learn over time from a continuous stream of data without having to revisit previously encountered training samples. Learning continually in a single data pass is crucial for agents and robots operating in changing environments and required to acquire, fine-tune, and transfer increasingly complex representations from non-i.i.d. input distributions. Machine learning models that address OCL must alleviate \$\textbackslash backslash\$textit\{catastrophic forgetting\} in which hidden representations are disrupted or completely overwritten when learning from streams of novel input. In this chapter, we summarize and discuss recent deep learning models that address OCL on sequential input through the use (and combination) of synaptic regularization, structural plasticity, and experience replay. Different implementations of replay have been proposed that alleviate catastrophic forgetting in connectionists architectures via the re-occurrence of (latent representations of) input sequences and that functionally resemble mechanisms of hippocampal replay in the mammalian brain. Empirical evidence shows that architectures endowed with experience replay typically outperform architectures without in (online) incremental learning tasks.},
  keywords = {[framework],/unread,\#nosource,Computer Science - Computer Vision and Pattern Rec,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computi},
  note = {Comment: L. Oneto et al. (eds.), Recent Trends in Learning From Data, Studies in Computational Intelligence 896 arXiv: 2003.09114}
}

@article{smith2019,
  title = {Unsupervised {{Progressive Learning}} and the {{STAM Architecture}}},
  author = {Smith, James and Baer, Seth and Taylor, Cameron and Dovrolis, Constantine},
  year = {2019},
  journal = {arXiv},
  url = {http://arxiv.org/abs/1904.02021},
  abstract = {We first pose the Unsupervised Progressive Learning (UPL) problem: an online representation learning problem in which the learner observes a non-stationary and unlabeled data stream, and identifies a growing number of features that persist over time even though the data is not stored or replayed. To solve the UPL problem we propose the Self-Taught Associative Memory (STAM) architecture. Layered hierarchies of STAM modules learn based on a combination of online clustering, novelty detection, forgetting outliers, and storing only prototypical features rather than specific examples. We evaluate STAM representations using classification and clustering tasks. Even though there are no prior approaches that are directly applicable to the UPL problem, we evaluate the STAM architecture in comparison to some unsupervised and self-supervised deep learning approaches adapted in the UPL context.},
  keywords = {[mnist],/unread,\#nosource,⛔ No DOI found},
  annotation = {\_eprint: 1904.02021}
}

@article{sodhani2019,
  title = {Toward {{Training Recurrent Neural Networks}} for {{Lifelong Learning}}},
  author = {Sodhani, Shagun and Chandar, Sarath and Bengio, Yoshua},
  year = {2019},
  journal = {Neural Computation},
  volume = {32},
  number = {1},
  pages = {1--35},
  publisher = {{MIT Press}},
  issn = {0899-7667},
  doi = {10/ggh2mp},
  url = {https://doi.org/10.1162/neco_a_01246},
  urldate = {2021-01-06},
  abstract = {Catastrophic forgetting and capacity saturation are the central challenges of any parametric lifelong learning system. In this work, we study these challenges in the context of sequential supervised learning with an emphasis on recurrent neural networks. To evaluate the models in the lifelong learning setting, we propose a curriculum-based, simple, and intuitive benchmark where the models are trained on tasks with increasing levels of difficulty. To measure the impact of catastrophic forgetting, the model is tested on all the previous tasks as it completes any task. As a step toward developing true lifelong learning systems, we unify gradient episodic memory (a catastrophic forgetting alleviation approach) and Net2Net (a capacity expansion approach). Both models are proposed in the context of feedforward networks, and we evaluate the feasibility of using them for recurrent networks. Evaluation on the proposed benchmark shows that the unified model is more suitable than the constituent models for lifelong learning setting.},
  keywords = {[rnn],/unread,\#nosource}
}

@article{zhang2022,
  title = {Continual {{Sequence Generation}} with {{Adaptive Compositional Modules}}},
  author = {Zhang, Yanzhe and Wang, Xuezhi and Yang, Diyi},
  year = {2022},
  journal = {ACL},
  eprint = {2203.10652},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2203.10652},
  urldate = {2022-04-06},
  abstract = {Continual learning is essential for real-world deployment when there is a need to quickly adapt the model to new tasks without forgetting knowledge of old tasks. Existing work on continual sequence generation either always reuses existing parameters to learn new tasks, which is vulnerable to catastrophic forgetting on dissimilar tasks, or blindly adds new parameters for every new task, which could prevent knowledge sharing between similar tasks. To get the best of both worlds, in this work, we propose continual sequence generation with adaptive compositional modules to adaptively add modules in transformer architectures and compose both old and new modules for new tasks. We also incorporate pseudo experience replay to facilitate knowledge transfer in those shared modules. Experiment results on various sequences of generation tasks show that our framework can adaptively add modules or reuse modules based on task similarity, outperforming state-of-the-art baselines in terms of both performance and parameter efficiency. We make our code public at https://github.com/GT-SALT/Adaptive-Compositional-Modules.},
  archiveprefix = {arXiv},
  keywords = {[nlp],/unread,⛔ No DOI found,Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  annotation = {ZSCC:00006},
  note = {Comment: 15 pages, ACL 2022}
}


