
@inproceedings{luders2017a,
  title = {Continual and {{One-Shot Learning Through Neural Networks}} with {{Dynamic External Memory}}},
  booktitle = {Applications of {{Evolutionary Computation}}},
  author = {L{\"u}ders, Benno and Schl{\"a}ger, Mikkel and Korach, Aleksandra and Risi, Sebastian},
  editor = {Squillero, Giovanni and Sim, Kevin},
  year = {2017},
  pages = {886--901},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10/gnq33j},
  url = {https://link.springer.com/chapter/10.1007/978-3-319-55849-3_57},
  abstract = {Training neural networks to quickly learn new skills without forgetting previously learned skills is an important open challenge in machine learning. A common problem for adaptive networks that can learn during their lifetime is that the weights encoding a particular task are often overridden when a new task is learned. This paper takes a step in overcoming this limitation by building on the recently proposed Evolving Neural Turing Machine (ENTM) approach. In the ENTM, neural networks are augmented with an external memory component that they can write to and read from, which allows them to store associations quickly and over long periods of time. The results in this paper demonstrate that the ENTM is able to perform one-shot learning in reinforcement learning tasks without catastrophic forgetting of previously stored associations. Additionally, we introduce a new ENTM default jump mechanism that makes it easier to find unused memory location and therefor facilitates the evolution of continual learning networks. Our results suggest that augmenting evolving networks with an external memory component is not only a viable mechanism for adaptive behaviors in neuroevolution but also allows these networks to perform continual and one-shot learning at the same time.},
  isbn = {978-3-319-55849-3},
  langid = {english}
}

@article{ostapenko2022,
  title = {Foundational {{Models}} for {{Continual Learning}}: {{An Empirical Study}} of {{Latent Replay}}},
  shorttitle = {Foundational {{Models}} for {{Continual Learning}}},
  author = {Ostapenko, Oleksiy and Lesort, Timothee and Rodr{\'i}guez, Pau and Arefin, Md Rifat and Douillard, Arthur and Rish, Irina and Charlin, Laurent},
  year = {2022},
  journal = {arXiv},
  eprint = {2205.00329},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2205.00329},
  urldate = {2022-05-05},
  abstract = {Rapid development of large-scale pre-training has resulted in foundation models that can act as effective feature extractors on a variety of downstream tasks and domains. Motivated by this, we study the efficacy of pre-trained vision models as a foundation for downstream continual learning (CL) scenarios. Our goal is twofold. First, we want to understand the compute-accuracy trade-off between CL in the raw-data space and in the latent space of pre-trained encoders. Second, we investigate how the characteristics of the encoder, the pre-training algorithm and data, as well as of the resulting latent space affect CL performance. For this, we compare the efficacy of various pre-trained models in large-scale benchmarking scenarios with a vanilla replay setting applied in the latent and in the raw-data space. Notably, this study shows how transfer, forgetting, task similarity and learning are dependent on the input data characteristics and not necessarily on the CL algorithms. First, we show that under some circumstances reasonable CL performance can readily be achieved with a non-parametric classifier at negligible compute. We then show how models pre-trained on broader data result in better performance for various replay sizes. We explain this with representational similarity and transfer properties of these representations. Finally, we show the effectiveness of self-supervised pre-training for downstream domains that are out-of-distribution as compared to the pre-training domain. We point out and validate several research directions that can further increase the efficacy of latent CL including representation ensembling. The diverse set of datasets used in this study can serve as a compute-efficient playground for further CL research. The codebase is available under https://github.com/oleksost/latent\_CL.},
  archiveprefix = {arXiv},
  keywords = {/unread,⛔ No DOI found,Computer Science - Artificial Intelligence,Computer Science - Machine Learning}
}

@article{rostami2019,
  title = {Complementary {{Learning}} for {{Overcoming Catastrophic Forgetting Using Experience Replay}}},
  author = {Rostami, Mohammad and Kolouri, Soheil and Pilly, Praveen K},
  year = {2019},
  journal = {arXiv},
  url = {http://arxiv.org/abs/1903.04566},
  abstract = {Despite huge success, deep networks are unable to learn effectively in sequential multitask learning settings as they forget the past learned tasks after learning new tasks. Inspired from complementary learning systems theory, we address this challenge by learning a generative model that couples the current task to the past learned tasks through a discriminative embedding space. We learn an abstract level generative distribution in the embedding that allows the generation of data points to represent the experience. We sample from this distribution and utilize experience replay to avoid forgetting and simultaneously accumulate new knowledge to the abstract distribution in order to couple the current task with past experience. We demonstrate theoretically and empirically that our framework learns a distribution in the embedding that is shared across all task and as a result tackles catastrophic forgetting.},
  keywords = {\#nosource,⛔ No DOI found},
  annotation = {\_eprint: 1903.04566}
}

@article{rostami2019a,
  title = {Complementary {{Learning}} for {{Overcoming Catastrophic Forgetting Using Experience Replay}}},
  author = {Rostami, Mohammad and Kolouri, Soheil and Pilly, Praveen K.},
  year = {2019},
  journal = {arXiv},
  doi = {10/gpt3ct},
  url = {https://arxiv.org/abs/1903.04566v2},
  urldate = {2022-03-22},
  abstract = {Despite huge success, deep networks are unable to learn effectively in sequential multitask learning settings as they forget the past learned tasks after learning new tasks. Inspired from complementary learning systems theory, we address this challenge by learning a generative model that couples the current task to the past learned tasks through a discriminative embedding space. We learn an abstract level generative distribution in the embedding that allows the generation of data points to represent the experience. We sample from this distribution and utilize experience replay to avoid forgetting and simultaneously accumulate new knowledge to the abstract distribution in order to couple the current task with past experience. We demonstrate theoretically and empirically that our framework learns a distribution in the embedding that is shared across all task and as a result tackles catastrophic forgetting.},
  langid = {english}
}

@inproceedings{shin2017,
  title = {Continual {{Learning}} with {{Deep Generative Replay}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 30},
  author = {Shin, Hanul and Lee, Jung Kwon and Kim, Jaehong and Kim, Jiwon},
  editor = {Guyon, I and Luxburg, U V and Bengio, S and Wallach, H and Fergus, R and Vishwanathan, S and Garnett, R},
  year = {2017},
  pages = {2990--2999},
  publisher = {{Curran Associates, Inc.}},
  url = {http://papers.nips.cc/paper/6892-continual-learning-with-deep-generative-replay.pdf},
  keywords = {[mnist],\#nosource,⛔ No DOI found}
}

@article{vandeven2018a,
  title = {Generative Replay with Feedback Connections as a General Strategy for Continual Learning},
  author = {{van de Ven}, Gido M. and Tolias, Andreas S.},
  year = {2018},
  journal = {arXiv},
  url = {https://arxiv.org/abs/1809.10635},
  abstract = {A major obstacle to developing artificial intelligence applications capable of true lifelong learning is that artificial neural networks quickly or catastrophically forget previously learned tasks when trained on a new one. Numerous methods for alleviating catastrophic forgetting are currently being proposed, but differences in evaluation protocols make it difficult to directly compare their performance. To enable more meaningful comparisons, here we identified three distinct scenarios for continual learning based on whether task identity is known and, if it is not, whether it needs to be inferred. Performing the split and permuted MNIST task protocols according to each of these scenarios, we found that regularization-based approaches (e.g., elastic weight consolidation) failed when task identity needed to be inferred. In contrast, generative replay combined with distillation (i.e., using class probabilities as "soft targets") achieved superior performance in all three scenarios. Addressing the issue of efficiency, we reduced the computational cost of generative replay by integrating the generative model into the main model by equipping it with generative feedback or backward connections. This Replay-through-Feedback approach substantially shortened training time with no or negligible loss in performance. We believe this to be an important first step towards making the powerful technique of generative replay scalable to real-world continual learning applications.},
  keywords = {[framework],[generative],[mnist],\#nosource,⛔ No DOI found},
  annotation = {\_eprint: 1809.10635}
}

@article{vandeven2020a,
  title = {Brain-Inspired Replay for Continual Learning with Artificial Neural Networks},
  author = {{van de Ven}, Gido M. and Siegelmann, Hava T. and Tolias, Andreas S.},
  year = {2020},
  journal = {Nature Communications},
  volume = {11},
  doi = {10/gg8xst},
  url = {https://www.nature.com/articles/s41467-020-17866-2},
  abstract = {Artificial neural networks suffer from catastrophic forgetting. Unlike humans, when these networks are trained on something new, they rapidly forget what was learned before. In the brain, a mechanism thought to be important for protecting memories is the reactivation of neuronal activity patterns representing those memories. In artificial neural networks, such memory replay can be implemented as `generative replay', which can successfully \textendash{} and surprisingly efficiently \textendash{} prevent catastrophic forgetting on toy examples even in a class-incremental learning scenario. However, scaling up generative replay to complicated problems with many tasks or complex inputs is challenging. We propose a new, brain-inspired variant of replay in which internal or hidden representations are replayed that are generated by the network's own, context-modulated feedback connections. Our method achieves state-of-the-art performance on challenging continual learning benchmarks (e.g., class-incremental learning on CIFAR-100) without storing data, and it provides a novel model for replay in the brain.},
  keywords = {[cifar],[framework],[generative],[mnist],\#nosource},
  note = {The paper shows a generative form of replay in which a VAE, conditioned on the current task, is able to generate pseudosamples and, when used as a classifier, to address new tasks. The idea is that the generative model is inspired by the hyppocampus, which sits hierarchically on top of the cortex (often thought as the classifier). In this way, replay is fed-back by the same model used to predict the class. Forgetting is prevented both on VAE and on the classification component through replay. It also shows that regularization approaches fail in class-incremental setting.}
}

@article{wang2019,
  title = {Continual {{Learning}} of {{New Sound Classes}} Using {{Generative Replay}}},
  author = {Wang, Zhepei and Subakan, Cem and Tzinis, Efthymios and Smaragdis, Paris and Charlin, Laurent},
  year = {2019},
  journal = {arXiv},
  url = {http://arxiv.org/abs/1906.00654},
  abstract = {Continual learning consists in incrementally training a model on a sequence of datasets and testing on the union of all datasets. In this paper, we examine continual learning for the problem of sound classification, in which we wish to refine already trained models to learn new sound classes. In practice one does not want to maintain all past training data and retrain from scratch, but naively updating a model with new data(sets) results in a degradation of already learned tasks, which is referred to as "catastrophic forgetting." We develop a generative replay procedure for generating training audio spectrogram data, in place of keeping older training datasets. We show that by incrementally refining a classifier with generative replay a generator that is 4\% of the size of all previous training data matches the performance of refining the classifier keeping 20\% of all previous training data. We thus conclude that we can extend a trained sound classifier to learn new classes without having to keep previously used datasets.},
  keywords = {[audio],\#nosource,⛔ No DOI found,audio,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio,sequence,sequences,Statistics - Machine Learning,time series},
  note = {arXiv: 1906.00654
\par
arXiv: 1906.00654}
}


