
@article{du2019,
  title = {Single-{{Net Continual Learning}} with {{Progressive Segmented Training}} ({{PST}})},
  author = {Du, Xiaocong and Charan, Gouranga and Liu, Frank and Cao, Yu},
  year = {2019},
  pages = {1629--1636},
  doi = {10.1109/ICMLA.2019.00267},
  url = {http://arxiv.org/abs/1905.11550},
  abstract = {There is an increasing need of continual learning in dynamic systems, such as the self-driving vehicle, the surveillance drone, and the robotic system. Such a system requires learning from the data stream, training the model to preserve previous information and adapt to a new task, and generating a single-headed vector for future inference. Different from previous approaches with dynamic structures, this work focuses on a single network and model segmentation to prevent catastrophic forgetting. Leveraging the redundant capacity of a single network, model parameters for each task are separated into two groups: one important group which is frozen to preserve current knowledge, and secondary group to be saved (not pruned) for a future learning. A fixed-size memory containing a small amount of previously seen data is further adopted to assist the training. Without additional regularization, the simple yet effective approach of PST successfully incorporates multiple tasks and achieves the state-of-the-art accuracy in the single-head evaluation on CIFAR-10 and CIFAR-100 datasets. Moreover, the segmented training significantly improves computation efficiency in continual learning.},
  annotation = {\_eprint: 1905.11550},
  isbn = {9781728145501},
  journal = {arXiv},
  keywords = {[cifar]},
  number = {2}
}

@inproceedings{hou2018,
  title = {Lifelong {{Learning}} via {{Progressive Distillation}} and {{Retrospection}}},
  booktitle = {{{ECCV}}},
  author = {Hou, Saihui and Pan, Xinyu and Loy, Chen Change and Wang, Zilei and Lin, Dahua},
  year = {2018},
  issn = {16113349},
  doi = {10.1007/978-3-030-01219-9_27},
  url = {http://link.springer.com/10.1007/978-3-030-01219-9_27},
  abstract = {Lifelong learning aims at adapting a learned model to new tasks while retaining the knowledge gained earlier. A key challenge for lifelong learning is how to strike a balance between the preservation on old tasks and the adaptation to a new one within a given model. Approaches that combine both objectives in training have been explored in previous works. Yet the performance still suffers from considerable degradation in a long sequence of tasks. In this work, we propose a novel approach to lifelong learning, which tries to seek a better balance between preservation and adaptation via two techniques: Distillation and Retrospection. Specifically, the target model adapts to the new task by knowledge distillation from an intermediate expert, while the previous knowledge is more effectively preserved by caching a small subset of data for old tasks. The combination of Distillation and Retrospection leads to a more gentle learning curve for the target model, and extensive experiments demonstrate that our approach can bring consistent improvements on both old and new tasks.},
  isbn = {978-3-030-01218-2},
  keywords = {[imagenet],[vision],Knowledge distillation,Lifelong learning,Retrospection}
}

@inproceedings{lomonaco2020a,
  title = {Rehearsal-{{Free Continual Learning}} over {{Small Non}}-{{I}}.{{I}}.{{D}}. {{Batches}}},
  booktitle = {{{CVPR Workshop}} on {{Continual Learning}} for {{Computer Vision}}},
  author = {Lomonaco, Vincenzo and Maltoni, Davide and Pellegrini, Lorenzo},
  year = {2020},
  pages = {246--247},
  url = {https://openaccess.thecvf.com/content_CVPRW_2020/html/w15/Lomonaco_Rehearsal-Free_Continual_Learning_Over_Small_Non-I.I.D._Batches_CVPRW_2020_paper.html},
  abstract = {Robotic vision is a field where continual learning can play a significant role. An embodied agent operating in a complex environment subject to frequent and unpredictable changes is required to learn and adapt continuously. In the context of object recognition, for example, a robot should be able to learn (without forgetting) objects of never before seen classes as well as improving its recognition capabilities as new instances of already known classes are discovered. Ideally, continual learning should be triggered by the availability of short videos of single objects and performed on-line on on-board hardware with fine-grained updates. In this paper, we introduce a novel continual learning protocol based on the CORe50 benchmark and propose two rehearsal-free continual learning techniques, CWR* and AR1*, that can learn effectively even in the challenging case of nearly 400 small non-i.i.d. incremental batches. In particular, our experiments show that AR1* can outperform other state-of-the-art rehearsal-free techniques by more than 15\% accuracy in some cases, with a very light and constant computational and memory overhead across training batches.},
  keywords = {[core50]}
}

@article{maltoni2019,
  title = {Continuous {{Learning}} in {{Single}}-{{Incremental}}-{{Task Scenarios}}},
  author = {Maltoni, Davide and Lomonaco, Vincenzo},
  year = {2019},
  volume = {116},
  pages = {56--73},
  url = {http://arxiv.org/abs/1806.08568},
  abstract = {It was recently shown that architectural, regularization and rehearsal strategies can be used to train deep models sequentially on a number of disjoint tasks without forgetting previously acquired knowledge. However, these strategies are still unsatisfactory if the tasks are not disjoint but constitute a single incremental task (e.g., class-incremental learning). In this paper we point out the differences between multi-task and single-incremental-task scenarios and show that well-known approaches such as LWF, EWC and SI are not ideal for incremental task scenarios. A new approach, denoted as AR1, combining architectural and regularization strategies is then specifically proposed. AR1 overhead (in terms of memory and computation) is very small thus making it suitable for online learning. When tested on CORe50 and iCIFAR-100, AR1 outperformed existing regularization strategies by a good margin.},
  journal = {Neural Networks},
  keywords = {[core50],[framework],Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Rec,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computi,Continuous learning,Deep learning,ewc,Incremental class learning,incremental task,Lifelong learning,Object recognition,review,Single-incremental-task,Statistics - Machine Learning},
  language = {en},
  note = {Comment: 26 pages, 13 figures; v3: major revision (e.g. added Sec. 4.4), several typos and minor mistakes corrected arXiv: 1806.08568}
}

@article{mirzadeh2020,
  title = {Linear {{Mode Connectivity}} in {{Multitask}} and {{Continual Learning}}},
  author = {Mirzadeh, Seyed Iman and Farajtabar, Mehrdad and Gorur, Dilan and Pascanu, Razvan and Ghasemzadeh, Hassan},
  year = {2020},
  url = {https://arxiv.org/abs/2010.04495},
  abstract = {Continual (sequential) training and multitask (simultaneous) training are often attempting to solve the same overall objective: to find a solution that performs well on all considered tasks. The main difference is in the training regimes, where continual learning can only have access to one task at a time, which for neural networks typically leads to catastrophic forgetting. That is, the solution found for a subsequent task does not perform well on the previous ones anymore. However, the relationship between the different minima that the two training regimes arrive at is not well understood. What sets them apart? Is there a local structure that could explain the difference in performance achieved by the two different schemes? Motivated by recent work showing that different minima of the same task are typically connected by very simple curves of low error, we investigate whether multitask and continual solutions are similarly connected. We empirically find that indeed such connectivity can be reliably achieved and, more interestingly, it can be done by a linear path, conditioned on having the same initialization for both. We thoroughly analyze this observation and discuss its significance for the continual learning process. Furthermore, we exploit this finding to propose an effective algorithm that constrains the sequentially learned minima to behave as the multitask solution. We show that our method outperforms several state of the art continual learning algorithms on various vision benchmarks.},
  annotation = {\_eprint: 2010.04495},
  journal = {arXiv},
  keywords = {[cifar],[experimental],[mnist]},
  note = {The authors observe how minima of CL and Multitask algorithms lie in a linear subspace (when sharing initialization). They use this argumento to build a CL strategy which forces minima to stay on the same subspace, using also small replay memories.
\par
The authors observe how minima of CL and Multitask algorithms lie in a linear subspace (when sharing initialization). They use this argumento to build a CL strategy which forces minima to stay on the same subspace, using also small replay memories.}
}

@inproceedings{schwarz2018,
  title = {Progress \& {{Compress}}: {{A}} Scalable Framework for Continual Learning},
  shorttitle = {Progress \& {{Compress}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Schwarz, Jonathan and Czarnecki, Wojciech and Luketina, Jelena and {Grabska-Barwinska}, Agnieszka and Teh, Yee Whye and Pascanu, Razvan and Hadsell, Raia},
  year = {2018},
  pages = {4528--4537},
  url = {http://proceedings.mlr.press/v80/schwarz18a.html},
  abstract = {We introduce a conceptually simple and scalable framework for continual learning domains where tasks are learned sequentially. Our method is constant in the number of parameters and is designed to ...},
  keywords = {[vision],ewc,normalized ewc,online ewc},
  language = {en}
}

@article{sodhani2019,
  title = {Toward {{Training Recurrent Neural Networks}} for {{Lifelong Learning}}},
  author = {Sodhani, Shagun and Chandar, Sarath and Bengio, Yoshua},
  year = {2019},
  volume = {32},
  pages = {1--35},
  publisher = {{MIT Press}},
  issn = {0899-7667},
  doi = {10.1162/neco_a_01246},
  url = {https://doi.org/10.1162/neco_a_01246},
  urldate = {2021-01-06},
  abstract = {Catastrophic forgetting and capacity saturation are the central challenges of any parametric lifelong learning system. In this work, we study these challenges in the context of sequential supervised learning with an emphasis on recurrent neural networks. To evaluate the models in the lifelong learning setting, we propose a curriculum-based, simple, and intuitive benchmark where the models are trained on tasks with increasing levels of difficulty. To measure the impact of catastrophic forgetting, the model is tested on all the previous tasks as it completes any task. As a step toward developing true lifelong learning systems, we unify gradient episodic memory (a catastrophic forgetting alleviation approach) and Net2Net (a capacity expansion approach). Both models are proposed in the context of feedforward networks, and we evaluate the feasibility of using them for recurrent networks. Evaluation on the proposed benchmark shows that the unified model is more suitable than the constituent models for lifelong learning setting.},
  journal = {Neural Computation},
  keywords = {[rnn]},
  number = {1}
}

@article{wang2019,
  title = {Continual {{Learning}} of {{New Sound Classes}} Using {{Generative Replay}}},
  author = {Wang, Zhepei and Subakan, Cem and Tzinis, Efthymios and Smaragdis, Paris and Charlin, Laurent},
  year = {2019},
  url = {http://arxiv.org/abs/1906.00654},
  abstract = {Continual learning consists in incrementally training a model on a sequence of datasets and testing on the union of all datasets. In this paper, we examine continual learning for the problem of sound classification, in which we wish to refine already trained models to learn new sound classes. In practice one does not want to maintain all past training data and retrain from scratch, but naively updating a model with new data(sets) results in a degradation of already learned tasks, which is referred to as "catastrophic forgetting." We develop a generative replay procedure for generating training audio spectrogram data, in place of keeping older training datasets. We show that by incrementally refining a classifier with generative replay a generator that is 4\% of the size of all previous training data matches the performance of refining the classifier keeping 20\% of all previous training data. We thus conclude that we can extend a trained sound classifier to learn new classes without having to keep previously used datasets.},
  journal = {arXiv},
  keywords = {[audio],audio,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio,sequence,sequences,Statistics - Machine Learning,time series},
  note = {arXiv: 1906.00654
\par
arXiv: 1906.00654}
}


