
@inproceedings{buzzega2020,
  title = {Dark {{Experience}} for {{General Continual Learning}}: A {{Strong}}, {{Simple Baseline}}},
  shorttitle = {Dark {{Experience}} for {{General Continual Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Buzzega, Pietro and Boschini, Matteo and Porrello, Angelo and Abati, Davide and CALDERARA, SIMONE},
  year = {2020},
  volume = {33},
  pages = {15920--15930},
  publisher = {{Curran Associates, Inc.}},
  url = {https://papers.nips.cc/paper/2020/hash/b704ea2c39778f07c617f6b7ce480e9e-Abstract.html},
  urldate = {2022-07-14},
  abstract = {Continual Learning has inspired a plethora of approaches and evaluation settings; however, the majority of them overlooks the properties of a practical scenario, where the data stream cannot be shaped as a sequence of tasks and offline training is not viable. We work towards General Continual Learning (GCL), where task boundaries blur and the domain and class distributions shift either gradually or suddenly. We address it through mixing rehearsal with knowledge distillation and regularization; our simple baseline, Dark Experience Replay, matches the network's logits sampled throughout the optimization trajectory, thus promoting consistency with its past. By conducting an extensive analysis on both standard benchmarks and a novel GCL evaluation setting (MNIST-360), we show that such a seemingly simple baseline outperforms consolidated approaches and leverages limited resources. We further explore the generalization capabilities of our objective, showing its regularization being beneficial beyond mere performance.},
  keywords = {/unread,⛔ No DOI found},
  note = {A hybrid strategy combining replay and regularization (distillation).
\par
A buffer stores input-logits pairs from previous experiences and at each iteration the strategy samples from the buffer and minimizes the squared distance between the logits sampled from the buffer and the logits predicted by the current model.}
}

@inproceedings{chaudhry2019a,
  title = {Efficient {{Lifelong Learning}} with {{A-GEM}}},
  booktitle = {{{ICLR}}},
  author = {Chaudhry, Arslan and Ranzato, Marc'Aurelio and Rohrbach, Marcus and Elhoseiny, Mohamed},
  year = {2019},
  url = {http://arxiv.org/abs/1812.00420},
  abstract = {In lifelong learning, the learner is presented with a sequence of tasks, incrementally building a data-driven prior which may be leveraged to speed up learning of a new task. In this work, we investigate the efficiency of current lifelong approaches, in terms of sample complexity, computational and memory cost. Towards this end, we first introduce a new and a more realistic evaluation protocol, whereby learners observe each example only once and hyper-parameter selection is done on a small and disjoint set of tasks, which is not used for the actual learning experience and evaluation. Second, we introduce a new metric measuring how quickly a learner acquires a new skill. Third, we propose an improved version of GEM (Lopez-Paz \& Ranzato, 2017), dubbed Averaged GEM (A-GEM), which enjoys the same or even better performance as GEM, while being almost as computationally and memory efficient as EWC (Kirkpatrick et al., 2016) and other regularization-based methods. Finally, we show that all algorithms including A-GEM can learn even more quickly if they are provided with task descriptors specifying the classification tasks under consideration. Our experiments on several standard lifelong learning benchmarks demonstrate that A-GEM has the best trade-off between accuracy and efficiency.},
  langid = {english},
  keywords = {[cifar],[mnist],/unread,\#nosource,⛔ No DOI found,Computer Science - Machine Learning,Statistics - Machine Learning},
  note = {Comment: Published as a conference paper at ICLR 2019 arXiv: 1812.00420}
}

@article{du2019,
  title = {Single-{{Net Continual Learning}} with {{Progressive Segmented Training}} ({{PST}})},
  author = {Du, Xiaocong and Charan, Gouranga and Liu, Frank and Cao, Yu},
  year = {2019},
  journal = {arXiv},
  number = {2},
  pages = {1629--1636},
  doi = {10/gnq328},
  url = {http://arxiv.org/abs/1905.11550},
  abstract = {There is an increasing need of continual learning in dynamic systems, such as the self-driving vehicle, the surveillance drone, and the robotic system. Such a system requires learning from the data stream, training the model to preserve previous information and adapt to a new task, and generating a single-headed vector for future inference. Different from previous approaches with dynamic structures, this work focuses on a single network and model segmentation to prevent catastrophic forgetting. Leveraging the redundant capacity of a single network, model parameters for each task are separated into two groups: one important group which is frozen to preserve current knowledge, and secondary group to be saved (not pruned) for a future learning. A fixed-size memory containing a small amount of previously seen data is further adopted to assist the training. Without additional regularization, the simple yet effective approach of PST successfully incorporates multiple tasks and achieves the state-of-the-art accuracy in the single-head evaluation on CIFAR-10 and CIFAR-100 datasets. Moreover, the segmented training significantly improves computation efficiency in continual learning.},
  isbn = {9781728145501},
  keywords = {[cifar],/unread,\#nosource},
  annotation = {\_eprint: 1905.11550}
}

@inproceedings{hou2018,
  title = {Lifelong {{Learning}} via {{Progressive Distillation}} and {{Retrospection}}},
  booktitle = {{{ECCV}}},
  author = {Hou, Saihui and Pan, Xinyu and Loy, Chen Change and Wang, Zilei and Lin, Dahua},
  year = {2018},
  issn = {16113349},
  doi = {10/gnq329},
  url = {http://link.springer.com/10.1007/978-3-030-01219-9_27},
  abstract = {Lifelong learning aims at adapting a learned model to new tasks while retaining the knowledge gained earlier. A key challenge for lifelong learning is how to strike a balance between the preservation on old tasks and the adaptation to a new one within a given model. Approaches that combine both objectives in training have been explored in previous works. Yet the performance still suffers from considerable degradation in a long sequence of tasks. In this work, we propose a novel approach to lifelong learning, which tries to seek a better balance between preservation and adaptation via two techniques: Distillation and Retrospection. Specifically, the target model adapts to the new task by knowledge distillation from an intermediate expert, while the previous knowledge is more effectively preserved by caching a small subset of data for old tasks. The combination of Distillation and Retrospection leads to a more gentle learning curve for the target model, and extensive experiments demonstrate that our approach can bring consistent improvements on both old and new tasks.},
  isbn = {978-3-030-01218-2},
  keywords = {[imagenet],[vision],/unread,\#nosource,Knowledge distillation,Lifelong learning,Retrospection}
}

@inproceedings{lomonaco2020,
  title = {Rehearsal-{{Free Continual Learning}} over {{Small Non-I}}.{{I}}.{{D}}. {{Batches}}},
  booktitle = {{{CVPR Workshop}} on {{Continual Learning}} for {{Computer Vision}}},
  author = {Lomonaco, Vincenzo and Maltoni, Davide and Pellegrini, Lorenzo},
  year = {2020},
  pages = {246--247},
  url = {https://openaccess.thecvf.com/content_CVPRW_2020/html/w15/Lomonaco_Rehearsal-Free_Continual_Learning_Over_Small_Non-I.I.D._Batches_CVPRW_2020_paper.html},
  abstract = {Robotic vision is a field where continual learning can play a significant role. An embodied agent operating in a complex environment subject to frequent and unpredictable changes is required to learn and adapt continuously. In the context of object recognition, for example, a robot should be able to learn (without forgetting) objects of never before seen classes as well as improving its recognition capabilities as new instances of already known classes are discovered. Ideally, continual learning should be triggered by the availability of short videos of single objects and performed on-line on on-board hardware with fine-grained updates. In this paper, we introduce a novel continual learning protocol based on the CORe50 benchmark and propose two rehearsal-free continual learning techniques, CWR* and AR1*, that can learn effectively even in the challenging case of nearly 400 small non-i.i.d. incremental batches. In particular, our experiments show that AR1* can outperform other state-of-the-art rehearsal-free techniques by more than 15\% accuracy in some cases, with a very light and constant computational and memory overhead across training batches.},
  keywords = {[core50],/unread,\#nosource,⛔ No DOI found}
}

@inproceedings{lopez-paz2017,
  title = {Gradient {{Episodic Memory}} for {{Continual Learning}}},
  booktitle = {{{NIPS}}},
  author = {{Lopez-Paz}, David and Ranzato, Marc'Aurelio},
  year = {2017},
  url = {https://arxiv.org/abs/1706.08840},
  abstract = {One major obstacle towards AI is the poor ability of models to solve new problems quicker, and without forgetting previously acquired knowledge. To better understand this issue, we study the problem of continual learning, where the model observes, once and one by one, examples concerning a sequence of tasks. First, we propose a set of metrics to evaluate models learning over a continuum of data. These metrics characterize models not only by their test accuracy, but also in terms of their ability to transfer knowledge across tasks. Second, we propose a model for continual learning, called Gradient Episodic Memory (GEM) that alleviates forgetting, while allowing beneficial transfer of knowledge to previous tasks. Our experiments on variants of the MNIST and CIFAR-100 datasets demonstrate the strong performance of GEM when compared to the state-of-the-art.},
  keywords = {[cifar],[mnist],/unread,\#nosource,⛔ No DOI found,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,gem},
  note = {Comment: Published at NIPS 2017 arXiv: 1706.08840}
}

@article{maltoni2019,
  title = {Continuous {{Learning}} in {{Single-Incremental-Task Scenarios}}},
  author = {Maltoni, Davide and Lomonaco, Vincenzo},
  year = {2019},
  journal = {Neural Networks},
  volume = {116},
  pages = {56--73},
  doi = {10/gk79q3},
  url = {http://arxiv.org/abs/1806.08568},
  abstract = {It was recently shown that architectural, regularization and rehearsal strategies can be used to train deep models sequentially on a number of disjoint tasks without forgetting previously acquired knowledge. However, these strategies are still unsatisfactory if the tasks are not disjoint but constitute a single incremental task (e.g., class-incremental learning). In this paper we point out the differences between multi-task and single-incremental-task scenarios and show that well-known approaches such as LWF, EWC and SI are not ideal for incremental task scenarios. A new approach, denoted as AR1, combining architectural and regularization strategies is then specifically proposed. AR1 overhead (in terms of memory and computation) is very small thus making it suitable for online learning. When tested on CORe50 and iCIFAR-100, AR1 outperformed existing regularization strategies by a good margin.},
  langid = {english},
  keywords = {[core50],[framework],/unread,\#nosource,Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Rec,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computi,Continuous learning,Deep learning,ewc,Incremental class learning,incremental task,Lifelong learning,Object recognition,review,Single-incremental-task,Statistics - Machine Learning},
  note = {Comment: 26 pages, 13 figures; v3: major revision (e.g. added Sec. 4.4), several typos and minor mistakes corrected arXiv: 1806.08568}
}

@article{mirzadeh2020a,
  title = {Linear {{Mode Connectivity}} in {{Multitask}} and {{Continual Learning}}},
  author = {Mirzadeh, Seyed Iman and Farajtabar, Mehrdad and Gorur, Dilan and Pascanu, Razvan and Ghasemzadeh, Hassan},
  year = {2020},
  journal = {arXiv},
  url = {https://arxiv.org/abs/2010.04495},
  abstract = {Continual (sequential) training and multitask (simultaneous) training are often attempting to solve the same overall objective: to find a solution that performs well on all considered tasks. The main difference is in the training regimes, where continual learning can only have access to one task at a time, which for neural networks typically leads to catastrophic forgetting. That is, the solution found for a subsequent task does not perform well on the previous ones anymore. However, the relationship between the different minima that the two training regimes arrive at is not well understood. What sets them apart? Is there a local structure that could explain the difference in performance achieved by the two different schemes? Motivated by recent work showing that different minima of the same task are typically connected by very simple curves of low error, we investigate whether multitask and continual solutions are similarly connected. We empirically find that indeed such connectivity can be reliably achieved and, more interestingly, it can be done by a linear path, conditioned on having the same initialization for both. We thoroughly analyze this observation and discuss its significance for the continual learning process. Furthermore, we exploit this finding to propose an effective algorithm that constrains the sequentially learned minima to behave as the multitask solution. We show that our method outperforms several state of the art continual learning algorithms on various vision benchmarks.},
  keywords = {[cifar],[experimental],[mnist],/unread,\#nosource,⛔ No DOI found},
  annotation = {\_eprint: 2010.04495},
  note = {The authors observe how minima of CL and Multitask algorithms lie in a linear subspace (when sharing initialization). They use this argumento to build a CL strategy which forces minima to stay on the same subspace, using also small replay memories.
\par
The authors observe how minima of CL and Multitask algorithms lie in a linear subspace (when sharing initialization). They use this argumento to build a CL strategy which forces minima to stay on the same subspace, using also small replay memories.}
}

@article{pomponi2020a,
  title = {Efficient Continual Learning in Neural Networks with Embedding Regularization},
  author = {Pomponi, Jary and Scardapane, Simone and Lomonaco, Vincenzo and Uncini, Aurelio},
  year = {2020},
  journal = {Neurocomputing},
  volume = {397},
  pages = {139--148},
  issn = {09252312},
  doi = {10/gnq325},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S092523122030151X},
  urldate = {2022-01-26},
  abstract = {Continual learning of deep neural networks is a key requirement for scaling them up to more complex applicative scenarios and for achieving real lifelong learning of these architectures. Previous approaches to the problem have considered either the progressive increase in the size of the networks, or have tried to regularize the network behavior to equalize it with respect to previously observed tasks. In the latter case, it is essential to understand what type of information best represents this past behavior. Common techniques include regularizing the past outputs, gradients, or individual weights. In this work, we propose a new, relatively simple and efficient method to perform continual learning by regularizing instead the network internal embeddings. To make the approach scalable, we also propose a dynamic sampling strategy to reduce the memory footprint of the required external storage. We show that our method performs favorably with respect to state-of-the-art approaches in the literature, while requiring significantly less space in memory and computational time. In addition, inspired by to recent works, we evaluate the impact of selecting a more flexible model for the activation functions inside the network, evaluating the impact of catastrophic forgetting on the activation functions themselves.},
  langid = {english},
  keywords = {/unread},
  annotation = {ZSCC: 0000022}
}

@inproceedings{schwarz2018,
  title = {Progress \& {{Compress}}: {{A}} Scalable Framework for Continual Learning},
  shorttitle = {Progress \& {{Compress}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Schwarz, Jonathan and Czarnecki, Wojciech and Luketina, Jelena and {Grabska-Barwinska}, Agnieszka and Teh, Yee Whye and Pascanu, Razvan and Hadsell, Raia},
  year = {2018},
  pages = {4528--4537},
  url = {http://proceedings.mlr.press/v80/schwarz18a.html},
  abstract = {We introduce a conceptually simple and scalable framework for continual learning domains where tasks are learned sequentially. Our method is constant in the number of parameters and is designed to ...},
  langid = {english},
  keywords = {[vision],/unread,\#nosource,ewc,normalized ewc,online ewc}
}

@article{sodhani2019,
  title = {Toward {{Training Recurrent Neural Networks}} for {{Lifelong Learning}}},
  author = {Sodhani, Shagun and Chandar, Sarath and Bengio, Yoshua},
  year = {2019},
  journal = {Neural Computation},
  volume = {32},
  number = {1},
  pages = {1--35},
  publisher = {{MIT Press}},
  issn = {0899-7667},
  doi = {10/ggh2mp},
  url = {https://doi.org/10.1162/neco_a_01246},
  urldate = {2021-01-06},
  abstract = {Catastrophic forgetting and capacity saturation are the central challenges of any parametric lifelong learning system. In this work, we study these challenges in the context of sequential supervised learning with an emphasis on recurrent neural networks. To evaluate the models in the lifelong learning setting, we propose a curriculum-based, simple, and intuitive benchmark where the models are trained on tasks with increasing levels of difficulty. To measure the impact of catastrophic forgetting, the model is tested on all the previous tasks as it completes any task. As a step toward developing true lifelong learning systems, we unify gradient episodic memory (a catastrophic forgetting alleviation approach) and Net2Net (a capacity expansion approach). Both models are proposed in the context of feedforward networks, and we evaluate the feasibility of using them for recurrent networks. Evaluation on the proposed benchmark shows that the unified model is more suitable than the constituent models for lifelong learning setting.},
  keywords = {[rnn],/unread,\#nosource}
}

@article{wang2019,
  title = {Continual {{Learning}} of {{New Sound Classes}} Using {{Generative Replay}}},
  author = {Wang, Zhepei and Subakan, Cem and Tzinis, Efthymios and Smaragdis, Paris and Charlin, Laurent},
  year = {2019},
  journal = {arXiv},
  url = {http://arxiv.org/abs/1906.00654},
  abstract = {Continual learning consists in incrementally training a model on a sequence of datasets and testing on the union of all datasets. In this paper, we examine continual learning for the problem of sound classification, in which we wish to refine already trained models to learn new sound classes. In practice one does not want to maintain all past training data and retrain from scratch, but naively updating a model with new data(sets) results in a degradation of already learned tasks, which is referred to as "catastrophic forgetting." We develop a generative replay procedure for generating training audio spectrogram data, in place of keeping older training datasets. We show that by incrementally refining a classifier with generative replay a generator that is 4\% of the size of all previous training data matches the performance of refining the classifier keeping 20\% of all previous training data. We thus conclude that we can extend a trained sound classifier to learn new classes without having to keep previously used datasets.},
  keywords = {[audio],/unread,\#nosource,⛔ No DOI found,audio,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio,sequence,sequences,Statistics - Machine Learning,time series},
  note = {arXiv: 1906.00654
\par
arXiv: 1906.00654}
}


