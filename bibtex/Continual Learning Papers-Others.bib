
@article{achille2018,
  title = {Life-{{Long Disentangled Representation Learning}} with {{Cross}}-{{Domain Latent Homologies}}},
  author = {Achille, Alessandro and Eccles, Tom and Matthey, Loic and Burgess, Christopher P. and Watters, Nick and Lerchner, Alexander and Higgins, Irina},
  year = {2018},
  url = {http://arxiv.org/abs/1808.06508},
  abstract = {Intelligent behaviour in the real-world requires the ability to acquire new knowledge from an ongoing sequence of experiences while preserving and reusing past knowledge. We propose a novel algorithm for unsupervised representation learning from piece-wise stationary visual data: Variational Autoencoder with Shared Embeddings (VASE). Based on the Minimum Description Length principle, VASE automatically detects shifts in the data distribution and allocates spare representational capacity to new knowledge, while simultaneously protecting previously learnt representations from catastrophic forgetting. Our approach encourages the learnt representations to be disentangled, which imparts a number of desirable properties: VASE can deal sensibly with ambiguous inputs, it can enhance its own representations through imagination-based exploration, and most importantly, it exhibits semantically meaningful sharing of latents between different datasets. Compared to baselines with entangled representations, our approach is able to reason beyond surface-level statistics and perform semantically meaningful cross-domain inference.},
  annotation = {\_eprint: 1808.06508},
  journal = {Neural Information Processing Systems (NeurIPS)}
}

@inproceedings{chen2020,
  title = {Long {{Live}} the {{Lottery}}: {{The Existence}} of {{Winning Tickets}} in {{Lifelong Learning}}},
  shorttitle = {Long {{Live}} the {{Lottery}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Chen, Tianlong and Zhang, Zhenyu and Liu, Sijia and Chang, Shiyu and Wang, Zhangyang},
  year = {2020},
  url = {https://openreview.net/forum?id=LXMSvPmsm0g},
  urldate = {2021-01-17},
  abstract = {The lottery ticket hypothesis demonstrates that a highly sparsified sub-network can be trained in isolation, given the appropriate weight initialization. This paper extends that hypothesis from...},
  language = {en}
}

@article{cheung2019,
  title = {Superposition of Many Models into One},
  author = {Cheung, Brian and Terekhov, Alex and Chen, Yubei and Agrawal, Pulkit and Olshausen, Bruno},
  year = {2019},
  url = {http://arxiv.org/abs/1902.05522},
  abstract = {We present a method for storing multiple models within a single set of parameters. Models can coexist in superposition and still be retrieved individually. In experiments with neural networks, we show that a surprisingly large number of models can be effectively stored within a single parameter instance. Furthermore, each of these models can undergo thousands of training steps without significantly interfering with other models within the superposition. This approach may be viewed as the online complement of compression: rather than reducing the size of a network after training, we make use of the unrealized capacity of a network during training.},
  annotation = {\_eprint: 1902.05522},
  journal = {arXiv},
  keywords = {[cifar],[mnist]}
}

@article{diethe2019,
  title = {Continual {{Learning}} in {{Practice}}},
  author = {Diethe, Tom and Borchert, Tom and Thereska, Eno and Balle, Borja and Lawrence, Neil},
  year = {2019},
  url = {http://arxiv.org/abs/1903.05202},
  abstract = {This paper describes a reference architecture for self-maintaining systems that can learn continually, as data arrives. In environments where data evolves, we need architectures that manage Machine Learning (ML) models in production, adapt to shifting data distributions, cope with outliers, retrain when necessary, and adapt to new tasks. This represents continual AutoML or Automatically Adaptive Machine Learning. We describe the challenges and proposes a reference architecture.},
  annotation = {\_eprint: 1903.05202},
  journal = {arXiv}
}

@article{douillard2021,
  title = {Continuum: {{Simple Management}} of {{Complex Continual Learning Scenarios}}},
  author = {Douillard, Arthur and Lesort, Timoth{\'e}e},
  year = {2021},
  url = {https://arxiv.org/abs/2102.06253},
  abstract = {Continual learning is a machine learning sub-field specialized in settings with non-iid data. Hence, the training data distribution is not static and drifts through time. Those drifts might cause interferences in the trained model and knowledge learned on previous states of the data distribution might be forgotten. Continual learning's challenge is to create algorithms able to learn an ever-growing amount of knowledge while dealing with data distribution drifts. One implementation difficulty in these field is to create data loaders that simulate non-iid scenarios. Indeed, data loaders are a key component for continual algorithms. They should be carefully designed and reproducible. Small errors in data loaders have a critical impact on algorithm results, e.g. with bad preprocessing, wrong order of data or bad test set. Continuum is a simple and efficient framework with numerous data loaders that avoid researcher to spend time on designing data loader and eliminate time-consuming errors. Using our proposed framework, it is possible to directly focus on the model design by using the multiple scenarios and evaluation metrics implemented. Furthermore the framework is easily extendable to add novel settings for specific needs.},
  journal = {arXiv}
}

@inproceedings{farquhar2018,
  title = {A {{Unifying Bayesian View}} of {{Continual Learning}}},
  booktitle = {{{NeurIPS Bayesian Deep Learning Workshop}}},
  author = {Farquhar, Sebastian and Gal, Yarin},
  year = {2018},
  url = {http://bayesiandeeplearning.org/2018/papers/74.pdf},
  keywords = {[bayes],[cifar],[mnist]},
  note = {The authors distinguish between prior-focused and likelihood-focused bayesian methods and then combine the two. They carefully analyze the difference between multi-head and single-head approach.}
}

@incollection{french2019,
  title = {Dynamically Constraining Connectionist Networks to Produce Distributed, Orthogonal Representations to Reduce Catastrophic Interference},
  booktitle = {Proceedings of the {{Sixteenth Annual Conference}} of the {{Cognitive Science Society}}},
  author = {French, Robert},
  editor = {Ram, Ashwin and Eiselt, Kurt},
  year = {2019},
  edition = {First},
  pages = {335--340},
  publisher = {{Routledge}},
  doi = {10.4324/9781315789354-58},
  url = {https://www.taylorfrancis.com/books/9781317729266/chapters/10.4324/9781315789354-58},
  abstract = {It is well known that when a connectionist network is trained on one set of patterns and then attempts to add new patterns to its repertoire, catastrophic interference may result. The use of sparse, orthogonal hidden-layer representations has been shown to reduce catastrophic interference. The author demonstrates that the use of sparse representations may, in certain cases, actually result in worse performance on catastrophic interference. This paper argues for the necessity of maintaining hidden-layer representations that are both as highly distributed and as highly orthogonal as possible. The author presents a learning algorithm, called context-biasing, that dynamically solves the problem of constraining hiddenlayer representations to simultaneously produce good orthogonality and distributedness. On the data tested for this study, context-biasing is shown to reduce catastrophic interference by more than 50\% compared to standard backpropagation. In particular, this technique succeeds in reducing catastrophic interference on data where sparse, orthogonal distributions failed to produce any improvement.},
  isbn = {978-1-315-78935-4},
  keywords = {sparsity},
  language = {en}
}

@article{golkar2019,
  title = {Continual {{Learning}} via {{Neural Pruning}}},
  author = {Golkar, Siavash and Kagan, Michael and Cho, Kyunghyun},
  year = {2019},
  url = {http://arxiv.org/abs/1903.04476},
  abstract = {We introduce Continual Learning via Neural Pruning (CLNP), a new method aimed at lifelong learning in fixed capacity models based on neuronal model sparsification. In this method, subsequent tasks are trained using the inactive neurons and filters of the sparsified network and cause zero deterioration to the performance of previous tasks. In order to deal with the possible compromise between model sparsity and performance, we formalize and incorporate the concept of graceful forgetting: the idea that it is preferable to suffer a small amount of forgetting in a controlled manner if it helps regain network capacity and prevents uncontrolled loss of performance during the training of future tasks. CLNP also provides simple continual learning diagnostic tools in terms of the number of free neurons left for the training of future tasks as well as the number of neurons that are being reused. In particular, we see in experiments that CLNP verifies and automatically takes advantage of the fact that the features of earlier layers are more transferable. We show empirically that CLNP leads to significantly improved results over current weight elasticity based methods.},
  journal = {arXiv},
  keywords = {[cifar],[mnist],[sparsity],Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computi,Quantitative Biology - Neurons and Cognition,Statistics - Machine Learning},
  note = {Comment: 12 pages, 5 figures, 3 tables arXiv: 1903.04476}
}

@inproceedings{kading2016,
  title = {Fine-{{Tuning Deep Neural Networks}} in {{Continuous Learning Scenarios}}},
  booktitle = {{{ACCV Workshop}}},
  author = {K{\"a}ding, Christoph and Rodner, Erik and Freytag, Alexander and Denzler, Joachim},
  year = {2016},
  issn = {16113349},
  doi = {10.1007/978-3-319-54526-4_43},
  url = {http://link.springer.com/10.1007/978-3-319-54526-4_43},
  abstract = {The revival of deep neural networks and the availability of ImageNet laid the foundation for recent success in highly complex recognition tasks. However, ImageNet does not cover all visual concepts of all possible application scenarios. Hence, application experts still record new data constantly and expect the data to be used upon its availability. In this paper, we follow this observation and apply the classical concept of fine-tuning deep neural networks to scenarios where data from known or completely new classes is continuously added. Besides a straightforward realization of continuous fine-tuning, we empirically analyze how computational burdens of training can be further reduced. Finally, we visualize how the network's attention maps evolve over time which allows for visually investigating what the network learned during continuous fine-tuning.},
  isbn = {978-3-319-54525-7},
  keywords = {[imagenet]}
}

@article{kuzina2019,
  title = {{{BooVAE}}: {{A}} Scalable Framework for Continual {{VAE}} Learning under Boosting Approach},
  author = {Kuzina, Anna and Egorov, Evgenii and Burnaev, Evgeny},
  year = {2019},
  url = {http://arxiv.org/abs/1908.11853},
  abstract = {Variational Auto Encoders (VAE) are capable of generating realistic images, sounds and video sequences. From practitioners point of view, we are usually interested in solving problems where tasks are learned sequentially, in a way that avoids revisiting all previous data at each stage. We address this problem by introducing a conceptually simple and scalable end-to-end approach of incorporating past knowledge by learning prior directly from the data. We consider scalable boosting-like approximation for intractable theoretical optimal prior. We provide empirical studies on two commonly used benchmarks, namely MNIST and Fashion MNIST on disjoint sequential image generation tasks. For each dataset proposed method delivers the best results or comparable to SOTA, avoiding catastrophic forgetting in a fully automatic way.},
  annotation = {\_eprint: 1908.11853},
  journal = {arXiv},
  keywords = {[bayes],[fashion],[mnist]}
}

@inproceedings{lee2019,
  title = {Overcoming Catastrophic Forgetting with Unlabeled Data in the Wild},
  booktitle = {Proceedings of the {{IEEE International Conference}} on {{Computer Vision}}},
  author = {Lee, Kibok and Lee, Kimin and Shin, Jinwoo and Lee, Honglak},
  year = {2019},
  volume = {2019-Octob},
  pages = {312--321},
  publisher = {{Institute of Electrical and Electronics Engineers Inc.}},
  issn = {15505499},
  doi = {10.1109/ICCV.2019.00040},
  url = {http://arxiv.org/abs/1903.12648},
  abstract = {Lifelong learning with deep neural networks is well-known to suffer from catastrophic forgetting: The performance on previous tasks drastically degrades when learning a new task. To alleviate this effect, we propose to leverage a large stream of unlabeled data easily obtainable in the wild. In particular, we design a novel class-incremental learning scheme with (a) a new distillation loss, termed global distillation, (b) a learning strategy to avoid overfitting to the most recent task, and (c) a confidence-based sampling method to effectively leverage unlabeled external data. Our experimental results on various datasets, including CIFAR and ImageNet, demonstrate the superiority of the proposed methods over prior methods, particularly when a stream of unlabeled data is accessible: Our method shows up to 15.8\% higher accuracy and 46.5\% less forgetting compared to the state-of-the-art method. The code is available at https://github.com/kibok90/iccv2019-inc.},
  annotation = {\_eprint: 1903.12648},
  isbn = {978-1-72814-803-8}
}

@article{li2019,
  title = {Continual {{Learning Using Bayesian Neural Networks}}},
  author = {Li, HongLin and Barnaghi, Payam and Enshaeifar, Shirin and Ganz, Frieder},
  year = {2019},
  url = {http://arxiv.org/abs/1910.04112},
  abstract = {Continual learning models allow to learn and adapt to new changes and tasks over time. However, in continual and sequential learning scenarios in which the models are trained using different data with various distributions, neural networks tend to forget the previously learned knowledge. This phenomenon is often referred to as catastrophic forgetting. The catastrophic forgetting is an inevitable problem in continual learning models for dynamic environments. To address this issue, we propose a method, called Continual Bayesian Learning Networks (CBLN), which enables the networks to allocate additional resources to adapt to new tasks without forgetting the previously learned tasks. Using a Bayesian Neural Network, CBLN maintains a mixture of Gaussian posterior distributions that are associated with different tasks. The proposed method tries to optimise the number of resources that are needed to learn each task and avoids an exponential increase in the number of resources that are involved in learning multiple tasks. The proposed method does not need to access the past training data and can choose suitable weights to classify the data points during the test time automatically based on an uncertainty criterion. We have evaluated our method on the MNIST and UCR time-series datasets. The evaluation results show that our method can address the catastrophic forgetting problem at a promising rate compared to the state-of-the-art models.},
  annotation = {\_eprint: 1910.04112},
  journal = {arXiv},
  keywords = {[bayes],[mnist],Bayesian neural networks,continual learning,in-cremental learning,Index Terms-Catastrophic forgetting,uncertainty}
}

@article{li2020,
  title = {Continual {{Learning Using Task Conditional Neural Networks}}},
  author = {Li, Honglin and Barnaghi, Payam and Enshaeifar, Shirin and Ganz, Frieder},
  year = {2020},
  url = {http://arxiv.org/abs/2005.05080},
  abstract = {Conventional deep learning models have limited capacity in learning multiple tasks sequentially. The issue of forgetting the previously learned tasks in continual learning is known as catastrophic forgetting or interference. When the input data or the goal of learning change, a continual model will learn and adapt to the new status. However, the model will not remember or recognise any revisits to the previous states. This causes performance reduction and re-training curves in dealing with periodic or irregularly reoccurring changes in the data or goals. The changes in goals or data are referred to as new tasks in a continual learning model. Most of the continual learning methods have a task-known setup in which the task identities are known in advance to the learning model. We propose Task Conditional Neural Networks (TCNN) that does not require to known the reoccurring tasks in advance. We evaluate our model on standard datasets using MNIST and CIFAR10, and also a real-world dataset that we have collected in a remote healthcare monitoring study (i.e. TIHM dataset). The proposed model outperforms the state-of-the-art solutions in continual learning and adapting to new tasks that are not defined in advance.},
  annotation = {\_eprint: 2005.05080},
  journal = {arXiv},
  keywords = {[cifar],[mnist]}
}

@article{li2020a,
  title = {Energy-{{Based Models}} for {{Continual Learning}}},
  author = {Li, Shuang and Du, Yilun and {van de Ven}, Gido M. and Torralba, Antonio and Mordatch, Igor},
  year = {2020},
  url = {http://arxiv.org/abs/2011.12216},
  abstract = {We motivate Energy-Based Models (EBMs) as a promising model class for continual learning problems. Instead of tackling continual learning via the use of external memory, growing models, or regularization, EBMs have a natural way to support a dynamically-growing number of tasks or classes that causes less interference with previously learned information. We find that EBMs outperform the baseline methods by a large margin on several continual learning benchmarks. We also show that EBMs are adaptable to a more general continual learning setting where the data distribution changes without the notion of explicitly delineated tasks. These observations point towards EBMs as a class of models naturally inclined towards the continual learning regime.},
  annotation = {\_eprint: 2011.12216},
  journal = {arXiv},
  keywords = {[cifar],[experimental],[mnist]},
  note = {The paper introduces Energy-Based models for classification in single incremental task + new classes (i.e. class incremental) scenarios. The model does not require task labels at test time, nor task boundaries at training time. It does not make use of replay.}
}

@article{liu2020,
  title = {Continual {{Universal Object Detection}}},
  author = {Liu, Xialei and Yang, Hao and Ravichandran, Avinash and Bhotika, Rahul and Soatto, Stefano},
  year = {2020},
  url = {http://arxiv.org/abs/2002.05347},
  abstract = {Object detection has improved significantly in recent years on multiple challenging benchmarks. However, most existing detectors are still domain-specific, where the models are trained and tested on a single domain. When adapting these detectors to new domains, they often suffer from catastrophic forgetting of previous knowledge. In this paper, we propose a continual object detector that can learn sequentially from different domains without forgetting. First, we explore learning the object detector continually in different scenarios across various domains and categories. Learning from the analysis, we propose attentive feature distillation leveraging both bottom-up and top-down attentions to mitigate forgetting. It takes advantage of attention to ignore the noisy background information and feature distillation to provide strong supervision. Finally, for the most challenging scenarios, we propose an adaptive exemplar sampling method to leverage exemplars from previous tasks for less forgetting effectively. The experimental results show the excellent performance of our proposed method in three different scenarios across seven different object detection datasets.},
  annotation = {\_eprint: 2002.05347},
  journal = {arXiv}
}

@article{liu2020a,
  title = {Mnemonics {{Training}}: {{Multi}}-{{Class Incremental Learning}} without {{Forgetting}}},
  author = {Liu, Yaoyao and Liu, An-An and Su, Yuting and Schiele, Bernt and Sun, Qianru},
  year = {2020},
  url = {http://arxiv.org/abs/2002.10211},
  abstract = {Multi-Class Incremental Learning (MCIL) aims to learn new concepts by incrementally updating a model trained on previous concepts. However, there is an inherent trade-off to effectively learning new concepts without catastrophic forgetting of previous ones. To alleviate this issue, it has been proposed to keep around a few examples of the previous concepts but the effectiveness of this approach heavily depends on the representativeness of these examples. This paper proposes a novel and automatic framework we call mnemonics, where we parameterize exemplars and make them optimizable in an end-to-end manner. We train the framework through bilevel optimizations, i.e., model-level and exemplar-level. We conduct extensive experiments on three MCIL benchmarks, CIFAR-100, ImageNet-Subset and ImageNet, and show that using mnemonics exemplars can surpass the state-of-the-art by a large margin. Interestingly and quite intriguingly, the mnemonics exemplars tend to be on the boundaries between different classes.},
  annotation = {\_eprint: 2002.10211},
  journal = {arXiv},
  keywords = {[cifar],[imagenet]}
}

@inproceedings{mallya2018,
  title = {Piggyback: {{Adapting}} a {{Single Network}} to {{Multiple Tasks}} by {{Learning}} to {{Mask Weights}}},
  booktitle = {{{ECCV}}},
  author = {Mallya, Arun and Davis, Dillon and Lazebnik, Svetlana},
  year = {2018},
  pages = {72--88},
  publisher = {{Springer Verlag}},
  issn = {16113349},
  doi = {10.1007/978-3-030-01225-0_5},
  url = {https://doi.org/10.1007/978-3-030-01225-0_5},
  abstract = {This work presents a method for adapting a single, fixed deep neural network to multiple tasks without affecting performance on already learned tasks. By building upon ideas from network quantization and pruning, we learn binary masks that ``piggyback'' on an existing network, or are applied to unmodified weights of that network to provide good performance on a new task. These masks are learned in an end-to-end differentiable fashion, and incur a low overhead of 1 bit per network parameter, per task. Even though the underlying network is fixed, the ability to mask individual weights allows for the learning of a large number of filters. We show performance comparable to dedicated fine-tuned networks for a variety of classification tasks, including those with large domain shifts from the initial task (ImageNet), and a variety of network architectures. Our performance is agnostic to task ordering and we do not suffer from catastrophic forgetting or competition between tasks.},
  annotation = {\_eprint: 1801.06519},
  isbn = {978-3-030-01224-3},
  keywords = {[imagenet],Binary networks,Incremental learning}
}

@article{mancini2018,
  title = {Adding {{New Tasks}} to a {{Single Network}} with {{Weight Transformations}} Using {{Binary Masks}}},
  author = {Mancini, Massimiliano and Ricci, Elisa and Caputo, Barbara and Bul{\`o}, Samuel Rota},
  year = {2018},
  volume = {11130 LNCS},
  pages = {180--189},
  issn = {16113349},
  doi = {10.1007/978-3-030-11012-3_14},
  url = {http://arxiv.org/abs/1805.11119},
  abstract = {Visual recognition algorithms are required today to exhibit adaptive abilities. Given a deep model trained on a specific, given task, it would be highly desirable to be able to adapt incrementally to new tasks, preserving scalability as the number of new tasks increases, while at the same time avoiding catastrophic forgetting issues. Recent work has shown that masking the internal weights of a given original conv-net through learned binary variables is a promising strategy. We build upon this intuition and take into account more elaborated affine transformations of the convolutional weights that include learned binary masks. We show that with our generalization it is possible to achieve significantly higher levels of adaptation to new tasks, enabling the approach to compete with fine tuning strategies by requiring slightly more than 1 bit per network parameter per additional task. Experiments on two popular benchmarks showcase the power of our approach, that achieves the new state of the art on the Visual Decathlon Challenge.},
  annotation = {\_eprint: 1805.11119},
  isbn = {9783030110116},
  journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  keywords = {[sparsity],[vision],Incremental learning,Multi-task learning}
}

@article{mundt2019,
  title = {Unified {{Probabilistic Deep Continual Learning}} through {{Generative Replay}} and {{Open Set Recognition}}},
  author = {Mundt, Martin and Majumder, Sagnik and Pliushch, Iuliia and Hong, Yong Won and Ramesh, Visvanathan},
  year = {2019},
  url = {http://arxiv.org/abs/1905.12019},
  abstract = {We introduce a probabilistic approach to unify open set recognition with the prevention of catastrophic forgetting in deep continual learning, based on variational Bayesian inference. Our single model combines a joint probabilistic encoder with a generative model and a linear classifier that get shared across sequentially arriving tasks. In order to successfully distinguish unseen unknown data from trained known tasks, we propose to bound the class specific approximate posterior by fitting regions of high density on the basis of correctly classified data points. These bounds are further used to significantly alleviate catastrophic forgetting by avoiding samples from low density areas in generative replay. Our approach requires neither storing of old, nor upfront knowledge of future data, and is empirically validated on visual and audio tasks in class incremental, as well as cross-dataset scenarios across modalities.},
  annotation = {\_eprint: 1905.12019},
  journal = {arXiv},
  keywords = {[audio],[bayes],[fashion],[framework],[generative],[mnist],[vision]}
}

@inproceedings{nguyen2018,
  title = {Variational {{Continual Learning}}},
  booktitle = {{{ICLR}}},
  author = {Nguyen, Cuong V and Li, Yingzhen and Bui, Thang D and Turner, Richard E},
  year = {2018},
  url = {https://openreview.net/forum?id=BkQqq0gRb},
  abstract = {This paper develops variational continual learning (VCL), a simple but general framework for continual learning that fuses online variational inference (VI) and recent advances in Monte Carlo VI for neural networks. The framework can successfully train both deep discriminative models and deep generative models in complex continual learning settings where existing tasks evolve over time and entirely new tasks emerge. Experimental results show that VCL outperforms state-of-the-art continual learning methods on a variety of tasks, avoiding catastrophic forgetting in a fully automatic way.},
  keywords = {[bayes]}
}

@inproceedings{nguyen2019,
  title = {Continual {{Rare}}-{{Class Recognition}} with {{Emerging Novel Subclasses}}},
  booktitle = {{{ECML}}},
  author = {Nguyen, Hung and Wang, Xuejian and Akoglu, Leman},
  year = {2019},
  url = {http://arxiv.org/abs/1906.12218},
  abstract = {Given a labeled dataset that contains a rare (or minority) class of of-interest instances, as well as a large class of instances that are not of interest, how can we learn to recognize future of-interest instances over a continuous stream? We introduce RaRecognize, which (i) estimates a general decision boundary between the rare and the majority class, (ii) learns to recognize individual rare subclasses that exist within the training data, as well as (iii) flags instances from previously unseen rare subclasses as newly emerging. The learner in (i) is general in the sense that by construction it is dissimilar to the specialized learners in (ii), thus distinguishes minority from the majority without overly tuning to what is seen in the training data. Thanks to this generality, RaRecognize ignores all future instances that it labels as majority and recognizes the recurrent as well as emerging rare subclasses only. This saves effort at test time as well as ensures that the model size grows moderately over time as it only maintains specialized minority learners. Through extensive experiments, we show that RaRecognize outperforms state-of-the art baselines on three real-world datasets that contain corporate-risk and disaster documents as rare classes.},
  annotation = {\_eprint: 1906.12218},
  keywords = {[nlp]}
}

@inproceedings{rajasegaran2019,
  title = {Random {{Path Selection}} for {{Incremental Learning}}},
  booktitle = {{{NeurIPS}}},
  author = {Rajasegaran, Jathushan and Hayat, Munawar and Fahad, Salman Khan and Khan, Shahbaz and Shao, Ling},
  year = {2019},
  pages = {12669--12679},
  url = {http://papers.nips.cc/paper/9429-random-path-selection-for-continual-learning.pdf},
  abstract = {Incremental lifelong learning is a main challenge towards the long-standing goal of Artificial General Intelligence. In real-life settings, learning tasks arrive in a sequence and machine learning models must continually learn to increment already acquired knowledge. Existing incremental learning approaches, fall well below the state-of-the-art cumulative models that use all training classes at once. In this paper, we propose a random path selection algorithm, called RPS-Net, that progressively chooses optimal paths for the new tasks while encouraging parameter sharing. Since the reuse of previous paths enables forward knowledge transfer, our approach requires a considerably lower computational overhead. As an added novelty, the proposed model integrates knowledge distillation and retrospection along with the path selection strategy to overcome catastrophic forgetting. In order to maintain an equilibrium between previous and newly acquired knowledge, we propose a simple controller to dynamically balance the model plasticity. Through extensive experiments, we demonstrate that the proposed method surpasses the state-of-the-art performance on incremental learning and by utilizing parallel computation this method can run in constant time with nearly the same efficiency as a conventional deep convolutional neural network.},
  keywords = {[cifar],[imagenet],[mnist]}
}

@article{saha2020,
  title = {Structured {{Compression}} and {{Sharing}} of {{Representational Space}} for {{Continual Learning}}},
  author = {Saha, Gobinda and Garg, Isha and Ankit, Aayush and Roy, Kaushik},
  year = {2020},
  url = {http://arxiv.org/abs/2001.08650},
  abstract = {Humans are skilled at learning adaptively and efficiently throughout their lives, but learning tasks incrementally causes artificial neural networks to overwrite relevant information learned about older tasks, resulting in 'Catastrophic Forgetting'. Efforts to overcome this phenomenon suffer from poor utilization of resources in many ways, such as through the need to save older data or parametric importance scores, or to grow the network architecture. We propose an algorithm that enables a network to learn continually and efficiently by partitioning the representational space into a Core space, that contains the condensed information from previously learned tasks, and a Residual space, which is akin to a scratch space for learning the current task. The information in the Residual space is then compressed using Principal Component Analysis and added to the Core space, freeing up parameters for the next task. We evaluate our algorithm on P-MNIST, CIFAR-10 and CIFAR-100 datasets. We achieve comparable accuracy to state-of-the-art methods while overcoming the problem of catastrophic forgetting completely. Additionally, we get up to 4.5x improvement in energy efficiency during inference due to the structured nature of the resulting architecture.},
  annotation = {\_eprint: 2001.08650},
  journal = {arXiv},
  keywords = {[cifar],[mnist]}
}

@inproceedings{saha2020a,
  title = {Gradient {{Projection Memory}} for {{Continual Learning}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Saha, Gobinda and Roy, Kaushik},
  year = {2020},
  url = {https://openreview.net/forum?id=3AOj0RCNC2},
  urldate = {2021-01-17},
  abstract = {The ability to learn continually without forgetting the past tasks is a desired attribute for artificial learning systems. Existing approaches to enable such learning in artificial neural networks...},
  language = {en}
}

@article{swaroop2019,
  title = {Improving and {{Understanding Variational Continual Learning}}},
  author = {Swaroop, Siddharth and Nguyen, Cuong V and Bui, Thang D and Turner, Richard E},
  year = {2019},
  pages = {1--17},
  url = {http://arxiv.org/abs/1905.02099},
  abstract = {In the continual learning setting, tasks are encountered sequentially. The goal is to learn whilst i) avoiding catastrophic forgetting, ii) efficiently using model capacity, and iii) employing forward and backward transfer learning. In this paper, we explore how the Variational Continual Learning (VCL) framework achieves these desiderata on two benchmarks in continual learning: split MNIST and permuted MNIST. We first report significantly improved results on what was already a competitive approach. The improvements are achieved by establishing a new best practice approach to mean-field variational Bayesian neural networks. We then look at the solutions in detail. This allows us to obtain an understanding of why VCL performs as it does, and we compare the solution to what an `ideal' continual learning solution might be.},
  annotation = {\_eprint: 1905.02099},
  journal = {Continual Learning Workshop NeurIPS},
  keywords = {[bayes],[mnist]}
}

@article{teng2019,
  title = {Continual {{Learning}} via {{Online Leverage Score Sampling}}},
  author = {Teng, Dan and Dasgupta, Sakyasingha},
  year = {2019},
  url = {http://arxiv.org/abs/1908.00355},
  abstract = {In order to mimic the human ability of continual acquisition and transfer of knowledge across various tasks, a learning system needs the capability for continual learning, effectively utilizing the previously acquired skills. As such, the key challenge is to transfer and generalize the knowledge learned from one task to other tasks, avoiding forgetting and interference of previous knowledge and improving the overall performance. In this paper, within the continual learning paradigm, we introduce a method that effectively forgets the less useful data samples continuously and allows beneficial information to be kept for training of the subsequent tasks, in an online manner. The method uses statistical leverage score information to measure the importance of the data samples in every task and adopts frequent directions approach to enable a continual or life-long learning property. This effectively maintains a constant training size across all tasks. We first provide mathematical intuition for the method and then demonstrate its effectiveness in avoiding catastrophic forgetting and computational efficiency on continual learning of classification tasks when compared with the existing state-of-the-art techniques.},
  annotation = {\_eprint: 1908.00355},
  journal = {arXiv},
  keywords = {[cifar],[mnist]}
}

@article{triki2017,
  title = {Encoder {{Based Lifelong Learning}}},
  author = {Triki, Amal Rannen and Aljundi, Rahaf and Blaschko, Mathew B. and Tuytelaars, Tinne},
  year = {2017},
  volume = {2017-Octob},
  pages = {1329--1337},
  publisher = {{Institute of Electrical and Electronics Engineers Inc.}},
  issn = {15505499},
  doi = {10.1109/ICCV.2017.148},
  url = {http://arxiv.org/abs/1704.01920 http://dx.doi.org/10.1109/ICCV.2017.148},
  abstract = {This paper introduces a new lifelong learning solution where a single model is trained for a sequence of tasks. The main challenge that vision systems face in this context is catastrophic forgetting: as they tend to adapt to the most recently seen task, they lose performance on the tasks that were learned previously. Our method aims at preserving the knowledge of the previous tasks while learning a new one by using autoencoders. For each task, an under-complete autoencoder is learned, capturing the features that are crucial for its achievement. When a new task is presented to the system, we prevent the reconstructions of the features with these autoencoders from changing, which has the effect of preserving the information on which the previous tasks are mainly relying. At the same time, the features are given space to adjust to the most recent environment as only their projection into a low dimension submanifold is controlled. The proposed system is evaluated on image classification tasks and shows a reduction of forgetting over the state-of-the-art},
  annotation = {\_eprint: 1704.01920},
  isbn = {9781538610329},
  journal = {Proceedings of the IEEE International Conference on Computer Vision},
  keywords = {[imagenet],[vision]}
}

@article{veness2020,
  title = {Gated {{Linear Networks}}},
  author = {Veness, Joel and Lattimore, Tor and Budden, David and Bhoopchand, Avishkar and Mattern, Christopher and {Grabska-Barwinska}, Agnieszka and Sezener, Eren and Wang, Jianan and Toth, Peter and Schmitt, Simon and Hutter, Marcus},
  year = {2020},
  url = {http://arxiv.org/abs/1910.01526},
  urldate = {2021-04-20},
  abstract = {This paper presents a new family of backpropagation-free neural architectures, Gated Linear Networks (GLNs). What distinguishes GLNs from contemporary neural networks is the distributed and local nature of their credit assignment mechanism; each neuron directly predicts the target, forgoing the ability to learn feature representations in favor of rapid online learning. Individual neurons can model nonlinear functions via the use of data-dependent gating in conjunction with online convex optimization. We show that this architecture gives rise to universal learning capabilities in the limit, with effective model capacity increasing as a function of network size in a manner comparable with deep ReLU networks. Furthermore, we demonstrate that the GLN learning mechanism possesses extraordinary resilience to catastrophic forgetting, performing comparably to a MLP with dropout and Elastic Weight Consolidation on standard benchmarks. These desirable theoretical and empirical properties position GLNs as a complementary technique to contemporary offline deep learning methods.},
  archiveprefix = {arXiv},
  eprint = {1910.01526},
  eprinttype = {arxiv},
  journal = {arXiv},
  keywords = {Computer Science - Information Theory,Computer Science - Machine Learning,Statistics - Machine Learning},
  note = {Comment: arXiv admin note: substantial text overlap with arXiv:1712.01897}
}

@article{wang2020,
  title = {Lifelong {{Graph Learning}}},
  author = {Wang, Chen and Qiu, Yuheng and Scherer, Sebastian},
  year = {2020},
  url = {http://arxiv.org/abs/2009.00647},
  abstract = {Graph neural networks are powerful models for many graph-structured tasks. In this paper, we aim to solve the problem of lifelong learning for graph neural networks. One of the main challenges is the effect of "catastrophic forgetting" for continuously learning a sequence of tasks, as the nodes can only be present to the model once. Moreover, the number of nodes changes dynamically in lifelong learning and this makes many graph models and sampling strategies inapplicable. To solve these problems, we construct a new graph topology, called the feature graph. It takes features as new nodes and turns nodes into independent graphs. This successfully converts the original problem of node classification to graph classification. In this way, the increasing nodes in lifelong learning can be regarded as increasing training samples, which makes lifelong learning easier. We demonstrate that the feature graph achieves much higher accuracy than the state-of-the-art methods in both data-incremental and class-incremental tasks. We expect that the feature graph will have broad potential applications for graph-structured tasks in lifelong learning.},
  annotation = {\_eprint: 2009.00647},
  journal = {arXiv},
  keywords = {[graph]}
}

@article{ye2019,
  title = {Class-{{Incremental Learning Based}} on {{Feature Extraction}} of {{CNN With Optimized Softmax}} and {{One}}-{{Class Classifiers}}},
  author = {Ye, Xin and Zhu, Qiuyu},
  year = {2019},
  volume = {7},
  pages = {42024--42031},
  publisher = {{Institute of Electrical and Electronics Engineers Inc.}},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2019.2904614},
  url = {https://ieeexplore.ieee.org/document/8666123/},
  abstract = {With the development of deep convolutional neural networks in recent years, the network structure has become more and more complicated and varied, and there are very good results in pattern recognition, image classification, scene classification, and target tracking. This end-to-end learning model relies on the initial large dataset. However, many data are gradually obtained in practical situations, which contradict the deep learning of one-time batch learning. There is an urgent need for an incremental learning approach that can continuously learn new knowledge from new data while retaining what has already been learned. This paper proposes an incremental learning algorithm based on convolutional neural network and support vector data description. CNN and AM-Softmax loss function are used to represent and continuously learn image features. Support vector data description is used to construct multiple hyperspheres for new and old classes of images. Class-incremental learning is achieved by the increment of hyperspheres. The experimental results show that the incremental learning method proposed in this paper can effectively extract the latent features of the image and adapt it to the learning situation of the class-increment. The recognition accuracy is close to batch learning.},
  journal = {IEEE Access},
  keywords = {[cifar],[mnist],feature extraction,incremental learning,loss function,One-class classifier}
}

@inproceedings{zeno2018,
  title = {Task {{Agnostic Continual Learning Using Online Variational Bayes}}},
  booktitle = {{{NeurIPS Bayesian Deep Learning Workshop}}},
  author = {Zeno, Chen and Golan, Itay and Hoffer, Elad and Soudry, Daniel},
  year = {2018},
  url = {http://bayesiandeeplearning.org/2018/papers/58.pdf},
  abstract = {Catastrophic forgetting is the notorious vulnerability of neural networks to the change of the data distribution while learning. This phenomena has long been considered a major obstacle for allowing the use of learning agents in realistic continual learning settings. Although this vulnerability of neural networks is widely investigated, it is currently only mitigated by explicitly reacting to the change of task. We suggest a novel approach for overcoming catastrophic forgetting in neural networks, using an online version of the variational Bayes method. Having a confidence measure of the weights alleviates catastrophic forgetting and, for the first time, succeeds in this even without the knowledge of when the tasks are being switched. 2},
  keywords = {[bayes],[cifar],[mnist]},
  note = {Bayesian Gradient Descent allows for Task Agnostic Continual Learning. The posterior on the parameters is updated at every batch, without needing task boundaries or task labels at training time. Experiments are performed with multi-head for Split scenarios.}
}


