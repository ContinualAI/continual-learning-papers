
@article{abbott2000,
  title = {Synaptic Plasticity: Taming the Beast},
  shorttitle = {Synaptic Plasticity},
  author = {Abbott, L F and Nelson, Sacha B},
  year = {2000},
  journal = {Nature Neuroscience},
  volume = {3},
  number = {11},
  pages = {1178--1183},
  issn = {1546-1726},
  doi = {10/fvn296},
  url = {https://www.nature.com/articles/nn1100_1178},
  abstract = {Synaptic plasticity provides the basis for most models of learning, memory and development in neural circuits. To generate realistic results, synapse-specific Hebbian forms of plasticity, such as long-term potentiation and depression, must be augmented by global processes that regulate overall levels of neuronal and network activity. Regulatory processes are often as important as the more intensively studied Hebbian processes in determining the consequences of synaptic plasticity for network function. Recent experimental results suggest several novel mechanisms for regulating levels of activity in conjunction with Hebbian synaptic modification. We review three of them\textemdash synaptic scaling, spike-timing dependent plasticity and synaptic redistribution\textemdash and discuss their functional implications.},
  langid = {english},
  keywords = {[hebbian],/unread,\#nosource}
}

@article{achille2018,
  title = {Life-{{Long Disentangled Representation Learning}} with {{Cross-Domain Latent Homologies}}},
  author = {Achille, Alessandro and Eccles, Tom and Matthey, Loic and Burgess, Christopher P. and Watters, Nick and Lerchner, Alexander and Higgins, Irina},
  year = {2018},
  journal = {Neural Information Processing Systems (NeurIPS)},
  url = {http://arxiv.org/abs/1808.06508},
  abstract = {Intelligent behaviour in the real-world requires the ability to acquire new knowledge from an ongoing sequence of experiences while preserving and reusing past knowledge. We propose a novel algorithm for unsupervised representation learning from piece-wise stationary visual data: Variational Autoencoder with Shared Embeddings (VASE). Based on the Minimum Description Length principle, VASE automatically detects shifts in the data distribution and allocates spare representational capacity to new knowledge, while simultaneously protecting previously learnt representations from catastrophic forgetting. Our approach encourages the learnt representations to be disentangled, which imparts a number of desirable properties: VASE can deal sensibly with ambiguous inputs, it can enhance its own representations through imagination-based exploration, and most importantly, it exhibits semantically meaningful sharing of latents between different datasets. Compared to baselines with entangled representations, our approach is able to reason beyond surface-level statistics and perform semantically meaningful cross-domain inference.},
  keywords = {/unread,\#nosource,⛔ No DOI found},
  annotation = {\_eprint: 1808.06508}
}

@inproceedings{adel2020,
  title = {Continual {{Learning}} with {{Adaptive Weights}} ({{CLAW}})},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Adel, Tameem and Zhao, Han and Turner, Richard E},
  year = {2020},
  url = {https://openreview.net/forum?id=Hklso24Kwr},
  abstract = {Approaches to continual learning aim to successfully learn a set of related tasks that arrive in an online manner. Recently, several frameworks have been developed which enable deep learning to be deployed in this learning scenario. A key modelling decision is to what extent the architecture should be shared across tasks. On the one hand, separately modelling each task avoids catastrophic forgetting but it does not support transfer learning and leads to large models. On the other hand, rigidly specifying a shared component and a task-specific part enables task transfer and limits the model size, but it is vulnerable to catastrophic forgetting and restricts the form of task-transfer that can occur. Ideally, the network should adaptively identify which parts of the network to share in a data driven way. Here we introduce such an approach called Continual Learning with Adaptive Weights (CLAW), which is based on probabilistic modelling and variational inference. Experiments show that CLAW achieves state-of-the-art performance on six benchmarks in terms of overall continual learning performance, as measured by classification accuracy, and in terms of addressing catastrophic forgetting.},
  keywords = {[cifar],[mnist],[omniglot],/unread,\#nosource}
}

@article{aggarwal2022,
  title = {A {{Comparative Study}} of {{Calibration Methods}} for {{Imbalanced Class Incremental Learning}}},
  author = {Aggarwal, Umang and Popescu, Adrian and Belouadah, Eden and Hudelot, C{\'e}line},
  year = {2022},
  journal = {arXiv},
  volume = {abs/2202.00386},
  eprint = {2202.00386},
  eprinttype = {arxiv},
  url = {https://arxiv.org/abs/2202.00386},
  archiveprefix = {arXiv},
  keywords = {/unread,⛔ No DOI found}
}

@article{ahmad2016,
  title = {How Do Neurons Operate on Sparse Distributed Representations? {{A}} Mathematical Theory of Sparsity, Neurons and Active Dendrites},
  author = {Ahmad, Subutai and Hawkins, Jeff},
  year = {2016},
  journal = {arXiv},
  pages = {1--23},
  url = {http://arxiv.org/abs/1601.00720},
  abstract = {We propose a formal mathematical model for sparse representations and active dendrites in neocortex. Our model is inspired by recent experimental findings on active dendritic processing and NMDA spikes in pyramidal neurons. These experimental and modeling studies suggest that the basic unit of pattern memory in the neocortex is instantiated by small clusters of synapses operated on by localized non-linear dendritic processes. We derive a number of scaling laws that characterize the accuracy of such dendrites in detecting activation patterns in a neuronal population under adverse conditions. We introduce the union property which shows that synapses for multiple patterns can be randomly mixed together within a segment and still lead to highly accurate recognition. We describe simulation results that provide further insight into sparse representations as well as two primary results. First we show that pattern recognition by a neuron with active dendrites can be extremely accurate and robust with high dimensional sparse inputs even when using a tiny number of synapses to recognize large patterns. Second, equations representing recognition accuracy of a dendrite predict optimal NMDA spiking thresholds under a generous set of assumptions. The prediction tightly matches NMDA spiking thresholds measured in the literature. Our model matches many of the known properties of pyramidal neurons. As such the theory provides a mathematical framework for understanding the benefits and limits of sparse representations in cortical networks.},
  keywords = {[hebbian],[sparsity],/unread,\#nosource,⛔ No DOI found,active dendrites,neocortex,neurons,nmda spike,sparse coding},
  annotation = {\_eprint: 1601.00720}
}

@inproceedings{ahn2019,
  title = {Uncertainty-Based {{Continual Learning}} with {{Adaptive Regularization}}},
  booktitle = {{{NeurIPS}}},
  author = {Ahn, Hongjoon and Cha, Sungmin and Lee, Donggyu and Moon, Taesup},
  year = {2019},
  pages = {4392--4402},
  url = {https://papers.nips.cc/paper/8690-uncertainty-based-continual-learning-with-adaptive-regularization.pdf},
  abstract = {We introduce a new neural network-based continual learning algorithm, dubbed as Uncertainty-regularized Continual Learning (UCL), which builds on traditional Bayesian online learning framework with variational inference. We focus on two significant drawbacks of the recently proposed regularization-based methods: a) considerable additional memory cost for determining the per-weight regularization strengths and b) the absence of gracefully forgetting scheme, which can prevent performance degradation in learning new tasks. In this paper, we show UCL can solve these two problems by introducing a fresh interpretation on the Kullback-Leibler (KL) divergence term of the variational lower bound for Gaussian mean-field approximation. Based on the interpretation, we propose the notion of node-wise uncertainty, which drastically reduces the number of additional parameters for implementing per-weight regularization. Moreover, we devise two additional regularization terms that enforce stability by freezing important parameters for past tasks and allow plasticity by controlling the actively learning parameters for a new task. Through extensive experiments, we show UCL convincingly outperforms most of recent state-of-the-art baselines not only on popular supervised learning benchmarks, but also on challenging lifelong reinforcement learning tasks. The source code of our algorithm is available at https://github.com/csm9493/UCL.},
  keywords = {[bayes],[cifar],[mnist],/unread,\#nosource,⛔ No DOI found}
}

@inproceedings{aljundi2017,
  title = {Expert {{Gate}}: {{Lifelong Learning}} with a {{Network}} of {{Experts}}},
  booktitle = {{{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Aljundi, Rahaf and Chakravarty, Punarjay and Tuytelaars, Tinne},
  year = {2017},
  doi = {10/gnq33f},
  url = {http://arxiv.org/abs/1611.06194},
  abstract = {In this paper we introduce a model of lifelong learning, based on a Network of Experts. New tasks / experts are learned and added to the model sequentially, building on what was learned before. To ensure scalability of this process, data from previous tasks cannot be stored and hence is not available when learning a new task. A critical issue in such context, not addressed in the literature so far, relates to the decision of which expert to deploy at test time. We introduce a gating autoencoder that learns a representation for the task at hand, and is used at test time to automatically forward the test sample to the relevant expert. This has the added advantage of being memory efficient as only one expert network has to be loaded into memory at any given time. Further, the autoencoders inherently capture the relatedness of one task to another, based on which the most relevant prior model to be used for training a new expert with fine-tuning or learning-without-forgetting can be selected. We evaluate our system on image classification and video prediction problems.},
  keywords = {[vision],/unread,\#nosource},
  annotation = {\_eprint: 1611.06194}
}

@inproceedings{aljundi2018,
  title = {Memory {{Aware Synapses}}: {{Learning}} What (Not) to Forget},
  booktitle = {The {{European Conference}} on {{Computer Vision}} ({{ECCV}})},
  author = {Aljundi, Rahaf and Babiloni, Francesca and Elhoseiny, Mohamed and Rohrbach, Marcus and Tuytelaars, Tinne},
  year = {2018},
  url = {https://openaccess.thecvf.com/content_ECCV_2018/papers/Rahaf_Aljundi_Memory_Aware_Synapses_ECCV_2018_paper.pdf},
  keywords = {[vision],/unread,\#nosource}
}

@inproceedings{aljundi2019,
  title = {Selfless {{Sequential Learning}}},
  booktitle = {{{ICLR}}},
  author = {Aljundi, Rahaf and Rohrbach, Marcus and Tuytelaars, Tinne},
  year = {2019},
  url = {https://openreview.net/forum?id=Bkxbrn0cYX},
  abstract = {Sequential learning, also called lifelong learning, studies the problem of learning tasks in a sequence with access restricted to only the data of the current task. In this paper we look at a...},
  keywords = {[cifar],[mnist],[sparsity],/unread,\#nosource,⛔ No DOI found},
  note = {The authors combine multiple penalizations to (1) induce sparse activations through lateral inhibitions between neurons and to (2) penalize changes in most important weights in order to prevent forgetting.}
}

@inproceedings{aljundi2019a,
  title = {Online {{Continual Learning}} with {{Maximal Interfered Retrieval}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 32},
  author = {Aljundi, Rahaf and Belilovsky, Eugene and Tuytelaars, Tinne and Charlin, Laurent and Caccia, Massimo and Lin, Min and {Page-Caccia}, Lucas},
  editor = {Wallach, H and Larochelle, H and Beygelzimer, A and {d\${\textbackslash}backslash\$textquotesingle Alch{\'e}-Buc}, F and Fox, E and Garnett, R},
  year = {2019},
  pages = {11849--11860},
  publisher = {{Curran Associates, Inc.}},
  url = {http://papers.nips.cc/paper/9357-online-continual-learning-with-maximal-interfered-retrieval.pdf},
  abstract = {Continual learning, the setting where a learning agent is faced with a never ending stream of data, continues to be a great challenge for modern machine learning systems. In particular the online or "single-pass through the data" setting has gained attention recently as a natural setting that is difficult to tackle. Methods based on replay, either generative or from a stored memory, have been shown to be effective approaches for continual learning, matching or exceeding the state of the art in a number of standard benchmarks. These approaches typically rely on randomly selecting samples from the replay memory or from a generative model, which is suboptimal. In this work we consider a controlled sampling of memories for replay. We retrieve the samples which are most interfered, i.e. whose prediction will be most negatively impacted by the foreseen parameters update. We show a formulation for this sampling criterion in both the generative replay and the experience replay setting, producing consistent gains in performance and greatly reduced forgetting. We release an implementation of our method at https://github.com/optimass/Maximally\_Interfered\_Retrieval},
  keywords = {[cifar],[mnist],/unread,\#nosource,⛔ No DOI found}
}

@phdthesis{aljundi2019b,
  title = {Continual {{Learning}} in {{Neural Networks}}},
  author = {Aljundi, Rahaf},
  year = {2019},
  journal = {arXiv},
  number = {September},
  url = {https://arxiv.org/abs/1910.02718},
  abstract = {Artificial neural networks have exceeded human-level performance in accomplishing several individual tasks (e.g. voice recognition, object recognition, and video games). However, such success remains modest compared to human intelligence that can learn and perform an unlimited number of tasks. Humans' ability of learning and accumulating knowledge over their lifetime is an essential aspect of their intelligence. Continual machine learning aims at a higher level of machine intelligence through providing the artificial agents with the ability to learn online from a non-stationary and never-ending stream of data. A key component of such a never-ending learning process is to overcome the catastrophic forgetting of previously seen data, a problem that neural networks are well known to suffer from. The work described in this thesis has been dedicated to the investigation of continual learning and solutions to mitigate the forgetting phenomena in neural networks. To approach the continual learning problem, we first assume a task incremental setting where tasks are received one at a time and data from previous tasks are not stored. Since the task incremental setting can't be assumed in all continual learning scenarios, we also study the more general online continual setting. We consider an infinite stream of data drawn from a non-stationary distribution with a supervisory or self-supervisory training signal. The proposed methods in this thesis have tackled important aspects of continual learning. They were evaluated on different benchmarks and over various learning sequences. Advances in the state of the art of continual learning have been shown and challenges for bringing continual learning into application were critically identified.},
  school = {KU Leuven},
  keywords = {[cifar],[imagenet],[mnist],[vision],/unread,\#nosource}
}

@inproceedings{aljundi2019c,
  title = {Gradient Based Sample Selection for Online Continual Learning},
  booktitle = {Advances in {{Neural Information Processing Systems}} 32},
  author = {Aljundi, Rahaf and Lin, Min and Goujaud, Baptiste and Bengio, Yoshua},
  editor = {Wallach, H and Larochelle, H and Beygelzimer, A and {d\${\textbackslash}backslash\$textquotesingle Alch{\'e}-Buc}, F and Fox, E and Garnett, R},
  year = {2019},
  pages = {11816--11825},
  publisher = {{Curran Associates, Inc.}},
  url = {http://papers.nips.cc/paper/9354-gradient-based-sample-selection-for-online-continual-learning.pdf},
  keywords = {[cifar],[mnist],/unread,\#nosource,⛔ No DOI found}
}

@inproceedings{aljundi2019d,
  title = {Task-{{Free Continual Learning}}},
  booktitle = {The {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Aljundi, Rahaf and Kelchtermans, Klaas and Tuytelaars, Tinne},
  year = {2019},
  url = {https://openaccess.thecvf.com/content_CVPR_2019/papers/Aljundi_Task-Free_Continual_Learning_CVPR_2019_paper.pdf},
  keywords = {[vision],/unread,\#nosource}
}

@article{aljundi2021,
  title = {Continual {{Novelty Detection}}},
  author = {Aljundi, Rahaf and Reino, Daniel Olmeda and Chumerin, Nikolay and Turner, Richard E.},
  year = {2021},
  journal = {arXiv},
  eprint = {2106.12964},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2106.12964},
  urldate = {2021-06-25},
  abstract = {Novelty Detection methods identify samples that are not representative of a model's training set thereby flagging misleading predictions and bringing a greater flexibility and transparency at deployment time. However, research in this area has only considered Novelty Detection in the offline setting. Recently, there has been a growing realization in the computer vision community that applications demand a more flexible framework - Continual Learning - where new batches of data representing new domains, new classes or new tasks become available at different points in time. In this setting, Novelty Detection becomes more important, interesting and challenging. This work identifies the crucial link between the two problems and investigates the Novelty Detection problem under the Continual Learning setting. We formulate the Continual Novelty Detection problem and present a benchmark, where we compare several Novelty Detection methods under different Continual Learning settings. We show that Continual Learning affects the behaviour of novelty detection algorithms, while novelty detection can pinpoint insights in the behaviour of a continual learner. We further propose baselines and discuss possible research directions. We believe that the coupling of the two problems is a promising direction to bring vision models into practice.},
  archiveprefix = {arXiv},
  keywords = {/unread,\#nosource,⛔ No DOI found,Computer Science - Computer Vision and Pattern Recognition}
}

@article{allred2020,
  title = {Controlled {{Forgetting}}: {{Targeted Stimulation}} and {{Dopaminergic Plasticity Modulation}} for {{Unsupervised Lifelong Learning}} in {{Spiking Neural Networks}}},
  author = {Allred, Jason M. and Roy, Kaushik},
  year = {2020},
  journal = {Frontiers in Neuroscience},
  volume = {14},
  pages = {7},
  publisher = {{Frontiers Media S.A.}},
  issn = {1662-453X},
  doi = {10/gnm8qv},
  url = {https://www.frontiersin.org/article/10.3389/fnins.2020.00007/full},
  abstract = {Stochastic gradient descent requires that training samples be drawn from a uniformly random distribution of the data. For a deployed system that must learn online from an uncontrolled and unknown environment, the ordering of input samples often fails to meet this criterion, making lifelong learning a difficult challenge. We exploit the locality of the unsupervised Spike Timing Dependent Plasticity (STDP) learning rule to target local representations in a Spiking Neural Network (SNN) to adapt to novel information while protecting essential information in the remainder of the SNN from catastrophic forgetting. In our Controlled Forgetting Networks (CFNs), novel information triggers stimulated firing and heterogeneously modulated plasticity, inspired by biological dopamine signals, to cause rapid and isolated adaptation in the synapses of neurons associated with outlier information. This targeting controls the forgetting process in a way that reduces the degradation of accuracy for older tasks while learning new tasks. Our experimental results on the MNIST dataset validate the capability of CFNs to learn successfully over time from an unknown, changing environment, achieving 95.24\% accuracy, which we believe is the best unsupervised accuracy ever achieved by a fixed-size, single-layer SNN on a completely disjoint MNIST dataset.},
  keywords = {[spiking],/unread,\#nosource,catastrophic forgetting,continual learning,controlled forgetting,dopaminergic learning,lifelong learning,Spike Timing Dependent Plasticity,Spiking Neural Networks,stability-plasticity dilemma}
}

@inproceedings{ans2002,
  title = {Preventing {{Catastrophic Interference}} in  {{MultipleSequence Learning Using Coupled Reverberating Elman Networks}}},
  booktitle = {Proceedings of the 24th {{Annual Conference}} of the {{Cognitive Science Society}}},
  author = {Ans, Bernard and Rousset, Stephane and French, Robert M. and Musca, Serban C.},
  year = {2002},
  url = {http://leadserv.u-bourgogne.fr/IMG/pdf/CogSci2002.Rsrn.pdf},
  abstract = {Everyone agrees that real cognition requires much more than static pattern recognition. In particular, it requires the ability to learn sequences of patterns (or actions) But learning sequences really means being able to learn multiple sequences, one after the other, wi thout the most recently learned ones erasing the previously learned ones. But if catastrophic interference is a problem for the sequential learning of individual patterns, the problem is amplified many times over when multiple sequences of patterns have to be learned consecutively, because each new sequence consists of many linked patterns. In this paper we will present a connectionist architecture that would seem to solve the problem of multiple sequence learning using pseudopatterns. Introduction Building a robot that could unfailingly recognize and respond to hundreds of objects in the world \textendash{} apples, mice, telephones and paper napkins, among them \textendash{} would unquestionably constitute a major artificial intelligence tour de force. But everyone agrees that real cognition requires much more than static pattern recognition. In particular, it requires the ability to learn sequences of patterns (or actions). This was the primary reason for the development of the simple recurrent network (SRN, Elman, 1990) and the many variants of this architecture. But learning sequences means more than being able to learn a single, isolated sequence of patterns: it means being able to learn multiple sequences, one after the other, without the most recently learned ones erasing the previously learned ones. But if catastrophic interference \textendash{} the phenomenon whereby new learning completely erases old learning \textendash{} is a problem with static pattern learning (McCloskey \& Cohen, 1989; Ratcliff, 1990), the problem is amplified many times over when multiple sequences of patterns have to be learned consecutively, because each sequence consists of many new linked patterns. What hope is there for a previously learned sequence of patterns to survive after the network has learned a new sequence consisting of many individual patterns? In this paper, we will present a connectionist architecture that solves the problem of multiple sequence learning. Catastrophic interference The problem of catastrophic interference (or forgetting) has been with the connectionist community for well over a decade now (McCloskey \& Cohen, 1989; Ratcliff, 1990; for a review see Sharkey \& Sharkey, 1995). Catastrophic forgetting occurs when newly learned information suddenly and completely erases information that was previously learned by the network, a phenomenon that is not only implausible cognitively, but disastrous for most practical applications. The problem has been studied by numerous authors over the past decade (see French, 1999 for a review). The problem is that the very property \textendash{} a single set of weights to encode information \textendash{} that gives connectionist networks their remarkable abilities of generalization and graceful degradation in the presence of incomplete information are also the root cause of catastrophic interference (see, for example, French, 1992). Various authors (Ans \& Rousset, 1997, 2000; French, 1997; Robins, 1995) have developed systems that rehearse on pseudo-episodes (or pseudopatterns), rather than on the real items that were previously learned. The basic principle of this mechanism is when learning new external patterns to interleave them with internally-generated pseudopatterns. These latter patterns, self-generated by the network from random activation, reflect (but are not identical to) the previously learned information. It has now been established that this pseudopattern rehearsal method effectively eliminates catastrophic forgetting. A serious problem remains, however, and that is this: cognition involves more than being able to sequentially learn a series of "static" (non-temporal) patterns without interference. It is of equal importance to be able to serially learn many of temporal sequences of patterns. We will propose an pseudopattern-based architecture that can effectively learn multiple temporal pat terns consecutively. The key insight of this paper is this: Once an SRN has learned a particular sequence, each pseudopattern generated by that network reflects the entire sequence (or set of sequences) that has been learned .},
  keywords = {[rnn],/unread,⛔ No DOI found,Catastrophic interference,Connectionism,Dual,Elegant degradation,ENCODE,Fault tolerance,Generalization (Psychology),Greater Than,Interference (communication),Memory Disorders,Network architecture,Pattern Recognition,Pseudo brand of pseudoephedrine,Recurrent neural network,reverberating,rnn,Robin bird,Telephone}
}

@article{ans2004,
  title = {Self-Refreshing Memory in Artificial Neural Networks: Learning Temporal Sequences without Catastrophic Forgetting},
  shorttitle = {Self-Refreshing Memory in Artificial Neural Networks},
  author = {Ans, Bernard and Rousset, St{\'e}phane and French, Robert M. and Musca, Serban},
  year = {2004},
  journal = {Connection Science},
  volume = {16},
  number = {2},
  pages = {71--99},
  publisher = {{Taylor \& Francis}},
  issn = {0954-0091},
  doi = {10/bbz9jp},
  url = {https://doi.org/10.1080/09540090412331271199},
  urldate = {2021-01-08},
  abstract = {While humans forget gradually, highly distributed connectionist networks forget catastrophically: newly learned information often completely erases previously learned information. This is not just implausible cognitively, but disastrous practically. However, it is not easy in connectionist cognitive modelling to keep away from highly distributed neural networks, if only because of their ability to generalize. A realistic and effective system that solves the problem of catastrophic interference in sequential learning of `static' (i.e. non-temporally ordered) patterns has been proposed recently (Robins 1995, Connection Science, 7: 123\textendash 146, 1996, Connection Science, 8: 259\textendash 275, Ans and Rousset 1997, CR Acad\'emie des Sciences Paris, Life Sciences, 320: 989\textendash 997, French 1997, Connection Science, 9: 353\textendash 379, 1999, Trends in Cognitive Sciences, 3: 128\textendash 135, Ans and Rousset 2000, Connection Science, 12: 1\textendash 19). The basic principle is to learn new external patterns interleaved with internally generated `pseudopatterns' (generated from random activation) that reflect the previously learned information. However, to be credible, this self-refreshing mechanism for static learning has to encompass our human ability to learn serially many temporal sequences of patterns without catastrophic forgetting. Temporal sequence learning is arguably more important than static pattern learning in the real world. In this paper, we develop a dual-network architecture in which self-generated pseudopatterns reflect (non-temporally) all the sequences of temporally ordered items previously learned. Using these pseudopatterns, several self-refreshing mechanisms that eliminate catastrophic forgetting in sequence learning are described and their efficiency is demonstrated through simulations. Finally, an experiment is presented that evidences a close similarity between human and simulated behaviour.},
  keywords = {[rnn],/unread,\#nosource,artificial neural networks,catastrophic forgetting,self-refreshing memory,temporal sequence learning},
  annotation = {\_eprint: https://doi.org/10.1080/09540090412331271199}
}

@article{antoniou2020,
  title = {Defining {{Benchmarks}} for {{Continual Few-Shot Learning}}},
  author = {Antoniou, Antreas and Patacchiola, Massimiliano and Ochal, Mateusz and Storkey, Amos},
  year = {2020},
  journal = {arXiv},
  url = {http://arxiv.org/abs/2004.11967},
  abstract = {Both few-shot and continual learning have seen substantial progress in the last years due to the introduction of proper benchmarks. That being said, the field has still to frame a suite of benchmarks for the highly desirable setting of continual few-shot learning, where the learner is presented a number of few-shot tasks, one after the other, and then asked to perform well on a validation set stemming from all previously seen tasks. Continual few-shot learning has a small computational footprint and is thus an excellent setting for efficient investigation and experimentation. In this paper we first define a theoretical framework for continual few-shot learning, taking into account recent literature, then we propose a range of flexible benchmarks that unify the evaluation criteria and allows exploring the problem from multiple perspectives. As part of the benchmark, we introduce a compact variant of ImageNet, called SlimageNet64, which retains all original 1000 classes but only contains 200 instances of each one (a total of 200K data-points) downscaled to 64 x 64 pixels. We provide baselines for the proposed benchmarks using a number of popular few-shot learning algorithms, as a result, exposing previously unknown strengths and weaknesses of those algorithms in continual and data-limited settings.},
  keywords = {[imagenet],/unread,\#nosource,⛔ No DOI found},
  annotation = {\_eprint: 2004.11967}
}

@article{arumae2020,
  title = {{{CALM}}: {{Continuous Adaptive Learning}} for {{Language Modeling}}},
  author = {Arumae, Kristjan and Bhatia, Parminder},
  year = {2020},
  journal = {arXiv},
  url = {http://arxiv.org/abs/2004.03794},
  abstract = {Training large language representation models has become a standard in the natural language processing community. This allows for fine tuning on any number of specific tasks, however, these large high capacity models can continue to train on domain specific unlabeled data to make initialization even more robust for supervised tasks. We demonstrate that in practice these pre-trained models present performance deterioration in the form of catastrophic forgetting when evaluated on tasks from a general domain such as GLUE. In this work we propose CALM, Continuous Adaptive Learning for Language Modeling: techniques to render models which retain knowledge across multiple domains. With these methods, we are able to reduce the performance gap across supervised tasks introduced by task specific models which we demonstrate using a continual learning setting in biomedical and clinical domains.},
  keywords = {[nlp],/unread,\#nosource,⛔ No DOI found},
  annotation = {\_eprint: 2004.03794}
}

@inproceedings{asghar2019,
  title = {Progressive {{Memory Banks}} for {{Incremental Domain Adaptation}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Asghar, Nabiha and Mou, Lili and Selby, Kira A and Pantasdo, Kevin D and Poupart, Pascal and Jiang, Xin},
  year = {2019},
  url = {https://openreview.net/forum?id=BkepbpNFwr},
  abstract = {This paper addresses the problem of incremental domain adaptation (IDA) in natural language processing (NLP). We assume each domain comes one after another, and that we could only access data in...},
  keywords = {[nlp],[rnn],/unread,\#nosource,⛔ No DOI found},
  note = {The authors leverage a Recurrent Neural Network with an explicit memory (memory banks) which grows when new computational capabilities are needed. Attention mechanisms are exploited in order to focus on specific component of previous memories.}
}

@inproceedings{ashfahani2019,
  title = {Autonomous {{Deep Learning}}: {{Continual Learning Approach}} for {{Dynamic Environments}}},
  booktitle = {Proceedings of the 2019 {{SIAM International Conference}} on {{Data Mining}}},
  author = {Ashfahani, Andri and Pratama, Mahardhika},
  year = {2019},
  pages = {666--674},
  publisher = {{Society for Industrial and Applied Mathematics}},
  address = {{Philadelphia, PA}},
  doi = {10/gnq33c},
  url = {https://epubs.siam.org/doi/10.1137/1.9781611975673.75},
  abstract = {The feasibility of deep neural networks (DNNs) to address data stream problems still requires intensive study because of the static and offline nature of conventional deep learning approaches. A deep continual learning algorithm, namely autonomous deep learning (ADL), is proposed in this paper. Unlike traditional deep learning methods, ADL features a flexible structure where its network structure can be constructed from scratch with the absence of initial network structure via the self-constructing network structure. ADL specifically addresses catastrophic forgetting by having a different-depth structure which is capable of achieving a trade-off between plasticity and stability. Network significance (NS) formula is proposed to drive the hidden nodes growing and pruning mechanism. Drift detection scenario (DDS) is put forward to signal distributional changes in data streams which induce the creation of a new hidden layer. Maximum information compression index (MICI) method plays an important role as a complexity reduction module eliminating redundant layers. The efficacy of ADL is numerically validated under the prequential test-then-train procedure in lifelong environments using nine popular data stream problems. The numerical results demonstrate that ADL consistently outperforms recent continual learning methods while characterizing the automatic construction of network structures.},
  isbn = {978-1-61197-567-3},
  keywords = {[mnist],/unread,\#nosource},
  annotation = {\_eprint: 1810.07348}
}

@inproceedings{awasthi2019,
  title = {Continual {{Learning}} with {{Neural Networks}}: {{A Review}}},
  booktitle = {Proceedings of the {{ACM India Joint International Conference}} on {{Data Science}} and {{Management}} of {{Data}}},
  author = {Awasthi, Abhijeet and Sarawagi, Sunita},
  year = {2019},
  pages = {362--365},
  doi = {10.1145},
  url = {https://dl.acm.org/doi/pdf/10.1145/3297001.3297062},
  abstract = {Continual learning broadly refers to the algorithms which aim to learn continuously over time across varying domains, tasks or data distributions. This is in contrast to algorithms restricted to learning a fixed number of tasks in a given domain, assuming a static data distribution. In this survey we aim to discuss a wide breadth of challenges faced in a continual learning setup and review existing work in the area. We discuss parameter regularization techniques to avoid catastrophic forgetting in neural networks followed by memory based approaches and the role of generative models in assisting continual learning algorithms. We discuss how dynamic neural networks assist continual learning by endowing neural networks with a new capacity to learn further. We conclude by discussing possible future directions.},
  keywords = {/unread,\#nosource,⚠️ Invalid DOI}
}

@article{ayub2020,
  title = {Storing {{Encoded Episodes}} as {{Concepts}} for {{Continual Learning}}},
  author = {Ayub, Ali and Wagner, Alan R.},
  year = {2020},
  journal = {arXiv},
  url = {https://arxiv.org/abs/2007.06637 http://arxiv.org/abs/2007.06637},
  abstract = {The two main challenges faced by continual learning approaches are catastrophic forgetting and memory limitations on the storage of data. To cope with these challenges, we propose a novel, cognitively-inspired approach which trains autoencoders with Neural Style Transfer to encode and store images. Reconstructed images from encoded episodes are replayed when training the classifier model on a new task to avoid catastrophic forgetting. The loss function for the reconstructed images is weighted to reduce its effect during classifier training to cope with image degradation. When the system runs out of memory the encoded episodes are converted into centroids and covariance matrices, which are used to generate pseudo-images during classifier training, keeping classifier performance stable with less memory. Our approach increases classification accuracy by 13-17\% over state-of-the-art methods on benchmark datasets, while requiring 78\% less storage space.},
  keywords = {[generative],[imagenet],[mnist],/unread,\#nosource,⛔ No DOI found,catastrophic forgetting,continual learning},
  annotation = {\_eprint: 2007.06637}
}

@inproceedings{ayub2020a,
  title = {Cognitively-{{Inspired Model}} for {{Incremental Learning Using}} a {{Few Examples}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}}},
  author = {Ayub, A. and Wagner, A. R.},
  year = {2020},
  doi = {10/gnq33v},
  url = {https://openaccess.thecvf.com/content_CVPRW_2020/html/w15/Ayub_Cognitively-Inspired_Model_for_Incremental_Learning_Using_a_Few_Examples_CVPRW_2020_paper.html},
  abstract = {Incremental learning attempts to develop a classifier which learns continuously from a stream of data segregated into different classes. Deep learning approaches suffer from catastrophic forgetting when learning classes incrementally, while most incremental learning approaches require a large amount of training data per class. We examine the problem of incremental learning using only a few training examples, referred to as Few-Shot Incremental Learning (FSIL). To solve this problem, we propose a novel approach inspired by the concept learning model of the hippocampus and the neocortex that represents each image class as centroids and does not suffer from catastrophic forgetting. We evaluate our approach on three class-incremental learning benchmarks: Caltech-101, CUBS-200-2011 and CIFAR-100 for incremental and few-shot incremental learning and show that our approach achieves state-of-the-art results in terms of classification accuracy over all learned classes.},
  keywords = {[cifar],[cubs],[dual],/unread,\#nosource,catastrophic forgetting,cognitively-inspired learning,continual learning}
}

@article{ayub2020b,
  title = {Tell Me What This Is: {{Few-Shot Incremental Object Learning}} by a {{Robot}}},
  author = {Ayub, Ali and Wagner, Alan R.},
  year = {2020},
  journal = {arXiv},
  url = {http://arxiv.org/abs/2008.00819},
  abstract = {For many applications, robots will need to be incrementally trained to recognize the specific objects needed for an application. This paper presents a practical system for incrementally training a robot to recognize different object categories using only a small set of visual examples provided by a human. The paper uses a recently developed state-of-the-art method for few-shot incremental learning of objects. After learning the object classes incrementally, the robot performs a table cleaning task organizing objects into categories specified by the human. We also demonstrate the system's ability to learn arrangements of objects and predict missing or incorrectly placed objects. Experimental evaluations demonstrate that our approach achieves nearly the same performance as a system trained with all examples at one time (batch training), which constitutes a theoretical upper bound.},
  keywords = {/unread,\#nosource,⛔ No DOI found,catastrophic forgetting,continual learning,few-shot incremenatl learning,robotics},
  annotation = {\_eprint: 2008.00819}
}

@article{bapna2019,
  title = {Non-{{Parametric Adaptation}} for {{Neural Machine Translation}}},
  author = {Bapna, Ankur and Firat, Orhan},
  year = {2019},
  journal = {arXiv},
  url = {http://arxiv.org/abs/1903.00058},
  abstract = {Neural Networks trained with gradient descent are known to be susceptible to catastrophic forgetting caused by parameter shift during the training process. In the context of Neural Machine Translation (NMT) this results in poor performance on heterogeneous datasets and on sub-tasks like rare phrase translation. On the other hand, non-parametric approaches are immune to forgetting, perfectly complementing the generalization ability of NMT. However, attempts to combine non-parametric or retrieval based approaches with NMT have only been successful on narrow domains, possibly due to over-reliance on sentence level retrieval. We propose a novel n-gram level retrieval approach that relies on local phrase level similarities, allowing us to retrieve neighbors that are useful for translation even when overall sentence similarity is low. We complement this with an expressive neural network, allowing our model to extract information from the noisy retrieved context. We evaluate our semi-parametric NMT approach on a heterogeneous dataset composed of WMT, IWSLT, JRC-Acquis and OpenSubtitles, and demonstrate gains on all 4 evaluation sets. The semi-parametric nature of our approach opens the door for non-parametric domain adaptation, demonstrating strong inference-time adaptation performance on new domains without the need for any parameter updates.},
  keywords = {[nlp],/unread,\#nosource,⛔ No DOI found},
  annotation = {\_eprint: 1903.00058}
}

@inproceedings{barrault2020,
  title = {Findings of the {{First Shared Task}} on {{Lifelong Learning Machine Translation}}},
  booktitle = {Proceedings of the {{Fifth Conference}} on {{Machine Translation}}},
  author = {Barrault, Lo{\"i}c and Biesialska, Magdalena and {Costa-juss{\`a}}, Marta R. and Bougares, Fethi and Galibert, Olivier},
  year = {2020},
  pages = {56--64},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  url = {https://www.aclweb.org/anthology/2020.wmt-1.2},
  urldate = {2021-02-03},
  abstract = {A lifelong learning system can adapt to new data without forgetting previously acquired knowledge. In this paper, we introduce the first benchmark for lifelong learning machine translation. For this purpose, we provide training, lifelong and test data sets for two language pairs: English-German and English-French. Additionally, we report the results of our baseline systems, which we make available to the public. The goal of this shared task is to encourage research on the emerging topic of lifelong learning machine translation.},
  keywords = {[framework],[nlp],/unread,\#nosource,Active Learning,Continual Learning,Lifelong Learning,workshop}
}

@article{barron2021,
  title = {Neural Inhibition for Continual Learning and Memory},
  author = {Barron, Helen C},
  year = {2021},
  journal = {Current Opinion in Neurobiology},
  series = {Neurobiology of {{Learning}} and {{Plasticity}}},
  volume = {67},
  pages = {85--94},
  issn = {0959-4388},
  doi = {10/gj6dvk},
  url = {https://www.sciencedirect.com/science/article/pii/S0959438820301343},
  urldate = {2021-07-20},
  abstract = {Humans are able to continually learn new information and acquire skills that meet the demands of an ever-changing environment. Yet, this new learning does not necessarily occur at the expense of old memories. The specialised biological mechanisms that permit continual learning in humans and other mammals are not fully understood. Here I explore the possibility that neural inhibition plays an important role. I present recent findings from studies in humans that suggest inhibition regulates the stability of neural networks to gate cortical plasticity and memory retrieval. These studies use non-invasive methods to obtain an indirect measure of neural inhibition and corroborate comparable findings in animals. Together these studies reveal a model whereby neural inhibition protects memories from interference to permit continual learning. Neural inhibition may, therefore, play a critical role in the computations that underlie higher-order cognition and adaptive behaviour.},
  langid = {english},
  keywords = {/unread,\#nosource}
}

@article{baweja2018,
  title = {Towards Continual Learning in Medical Imaging},
  author = {Baweja, Chaitanya and Glocker, Ben and Kamnitsas, Konstantinos},
  year = {2018},
  journal = {NeurIPS Workshop on Continual Learning},
  pages = {1--4},
  doi = {arXiv:1811.02496v1},
  url = {http://arxiv.org/abs/1811.02496},
  abstract = {This work investigates continual learning of two segmentation tasks in brain MRI with neural networks. To explore in this context the capabilities of current methods for countering catastrophic forgetting of the first task when a new one is learned, we investigate elastic weight consolidation, a recently proposed method based on Fisher information, originally evaluated on reinforcement learning of Atari games. We use it to sequentially learn segmentation of normal brain structures and then segmentation of white matter lesions. Our findings show this recent method reduces catastrophic forgetting, while large room for improvement exists in these challenging settings for continual learning.},
  keywords = {[vision],/unread,\#nosource,⚠️ Invalid DOI},
  annotation = {\_eprint: 1811.02496}
}

@inproceedings{beaulieu2020,
  title = {Learning to {{Continually Learn}}},
  booktitle = {{{ECAI}}},
  author = {Beaulieu, Shawn and Frati, Lapo and Miconi, Thomas and Lehman, Joel and Stanley, Kenneth O. and Clune, Jeff and Cheney, Nick},
  year = {2020},
  url = {http://arxiv.org/abs/2002.09571},
  abstract = {Continual lifelong learning requires an agent or model to learn many sequentially ordered tasks, building on previous knowledge without catastrophically forgetting it. Much work has gone towards preventing the default tendency of machine learning models to catastrophically forget, yet virtually all such work involves manually-designed solutions to the problem. We instead advocate meta-learning a solution to catastrophic forgetting, allowing AI to learn to continually learn. Inspired by neuromodulatory processes in the brain, we propose A Neuromodulated Meta-Learning Algorithm (ANML). It differentiates through a sequential learning process to meta-learn an activation-gating function that enables context-dependent selective activation within a deep neural network. Specifically, a neuromodulatory (NM) neural network gates the forward pass of another (otherwise normal) neural network called the prediction learning network (PLN). The NM network also thus indirectly controls selective plasticity (i.e. the backward pass of) the PLN. ANML enables continual learning without catastrophic forgetting at scale: it produces state-of-the-art continual learning performance, sequentially learning as many as 600 classes (over 9,000 SGD updates).},
  keywords = {[vision],/unread,\#nosource},
  annotation = {\_eprint: 2002.09571}
}

@inproceedings{belouadah2018,
  title = {{{DeeSIL}}: {{Deep-Shallow Incremental Learning}}},
  booktitle = {Computer {{Vision}} - {{ECCV}} 2018 {{Workshops}} - {{Munich}}, {{Germany}}, {{September}} 8-14, 2018, {{Proceedings}}, {{Part II}}},
  author = {Belouadah, Eden and Popescu, Adrian},
  editor = {{Leal-Taix{\'e}}, Laura and Roth, Stefan},
  year = {2018},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {11130},
  pages = {151--157},
  publisher = {{Springer}},
  doi = {10/gpt3cs},
  url = {https://doi.org/10.1007/978-3-030-11012-3\_11},
  keywords = {/unread}
}

@inproceedings{belouadah2019,
  title = {{{IL2M}}: {{Class Incremental Learning With Dual Memory}}},
  booktitle = {2019 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}}, {{ICCV}} 2019, {{Seoul}}, {{Korea}} ({{South}}), {{October}} 27 - {{November}} 2, 2019},
  author = {Belouadah, Eden and Popescu, Adrian},
  year = {2019},
  pages = {583--592},
  publisher = {{IEEE}},
  doi = {10/ghbfg6},
  url = {https://doi.org/10.1109/ICCV.2019.00067},
  keywords = {/unread}
}

@inproceedings{belouadah2020,
  title = {Active {{Class Incremental Learning}} for {{Imbalanced Datasets}}},
  booktitle = {Computer {{Vision}} - {{ECCV}} 2020 {{Workshops}} - {{Glasgow}}, {{UK}}, {{August}} 23-28, 2020, {{Proceedings}}, {{Part VI}}},
  author = {Belouadah, Eden and Popescu, Adrian and Aggarwal, Umang and Saci, L{\'e}o},
  editor = {Bartoli, Adrien and Fusiello, Andrea},
  year = {2020},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {12540},
  pages = {146--162},
  publisher = {{Springer}},
  doi = {10/gpt3cv},
  url = {https://doi.org/10.1007/978-3-030-65414-6\_12},
  keywords = {/unread}
}

@inproceedings{belouadah2020a,
  title = {{{ScaIL}}: {{Classifier Weights Scaling}} for {{Class Incremental Learning}}},
  booktitle = {{{IEEE Winter Conference}} on {{Applications}} of {{Computer Vision}}, {{WACV}} 2020, {{Snowmass Village}}, {{CO}}, {{USA}}, {{March}} 1-5, 2020},
  author = {Belouadah, Eden and Popescu, Adrian},
  year = {2020},
  pages = {1255--1264},
  publisher = {{IEEE}},
  doi = {10/gmmd24},
  url = {https://doi.org/10.1109/WACV45572.2020.9093562},
  keywords = {/unread}
}

@inproceedings{belouadah2020b,
  title = {Initial {{Classifier Weights Replay}} for {{Memoryless Class Incremental Learning}}},
  booktitle = {31st {{British Machine Vision Conference}} 2020, {{BMVC}} 2020, {{Virtual Event}}, {{UK}}, {{September}} 7-10, 2020},
  author = {Belouadah, Eden and Popescu, Adrian and Kanellos, Ioannis},
  year = {2020},
  publisher = {{BMVA Press}},
  url = {https://www.bmvc2020-conference.com/assets/papers/0743.pdf},
  keywords = {/unread,⛔ No DOI found}
}

@article{belouadah2021,
  title = {A Comprehensive Study of Class Incremental Learning Algorithms for Visual Tasks},
  author = {Belouadah, Eden and Popescu, Adrian and Kanellos, Ioannis},
  year = {2021},
  journal = {Neural Networks},
  volume = {135},
  pages = {38--54},
  doi = {10/gm8f64},
  url = {https://doi.org/10.1016/j.neunet.2020.12.003},
  keywords = {/unread}
}

@phdthesis{belouadah2021a,
  title = {Large-Scale Deep Class-Incremental Learning. ({{Apprentissage}} Incr\'emental Profond \`a Large \'Echelle)},
  author = {Belouadah, Eden},
  year = {2021},
  url = {https://tel.archives-ouvertes.fr/tel-03478553},
  school = {Ecole nationale sup\'erieure Mines-T\'el\'ecom Atlantique, France},
  keywords = {/unread}
}

@inproceedings{biesialska2020,
  title = {Continual {{Lifelong Learning}} in {{Natural Language Processing}}: {{A Survey}}},
  shorttitle = {Continual {{Lifelong Learning}} in {{Natural Language Processing}}},
  booktitle = {Proceedings of the 28th {{International Conference}} on {{Computational Linguistics}}},
  author = {Biesialska, Magdalena and Biesialska, Katarzyna and {Costa-juss{\`a}}, Marta R.},
  year = {2020},
  pages = {6523--6541},
  publisher = {{International Committee on Computational Linguistics}},
  address = {{Barcelona, Spain (Online)}},
  doi = {10/gh3d2k},
  url = {https://www.aclweb.org/anthology/2020.coling-main.574},
  urldate = {2021-02-09},
  abstract = {Continual learning (CL) aims to enable information systems to learn from a continuous data stream across time. However, it is difficult for existing deep learning architectures to learn a new task without largely forgetting previously acquired knowledge. Furthermore, CL is particularly challenging for language learning, as natural language is ambiguous: it is discrete, compositional, and its meaning is context-dependent. In this work, we look at the problem of CL through the lens of various NLP tasks. Our survey discusses major challenges in CL and current methods applied in neural network models. We also provide a critical review of the existing CL evaluation methods and datasets in NLP. Finally, we present our outlook on future research directions.},
  keywords = {[nlp],/unread,\#nosource,Catastrophic Forgetting,Continual Learning,Lifelong Learning,Review}
}

@article{bossens2021,
  title = {Lifetime Policy Reuse and the Importance of Task Capacity},
  author = {Bossens, David M. and Sobey, Adam J.},
  year = {2021},
  journal = {arXiv},
  eprint = {2106.01741},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2106.01741},
  urldate = {2021-06-07},
  abstract = {A long-standing challenge in artificial intelligence is lifelong learning. In lifelong learning, many tasks are presented in sequence and learners must efficiently transfer knowledge between tasks while avoiding catastrophic forgetting over long lifetimes. On these problems, policy reuse and other multi-policy reinforcement learning techniques can learn many tasks. However, they can generate many temporary or permanent policies, resulting in memory issues. Consequently, there is a need for lifetime-scalable methods that continually refine a policy library of a pre-defined size. This paper presents a first approach to lifetime-scalable policy reuse. To pre-select the number of policies, a notion of task capacity, the maximal number of tasks that a policy can accurately solve, is proposed. To evaluate lifetime policy reuse using this method, two state-of-the-art single-actor base-learners are compared: 1) a value-based reinforcement learner, Deep Q-Network (DQN) or Deep Recurrent Q-Network (DRQN); and 2) an actor-critic reinforcement learner, Proximal Policy Optimisation (PPO) with or without Long Short-Term Memory layer. By selecting the number of policies based on task capacity, D(R)QN achieves near-optimal performance with 6 policies in a 27-task MDP domain and 9 policies in an 18-task POMDP domain; with fewer policies, catastrophic forgetting and negative transfer are observed. Due to slow, monotonic improvement, PPO requires fewer policies, 1 policy for the 27-task domain and 4 policies for the 18-task domain, but it learns the tasks with lower accuracy than D(R)QN. These findings validate lifetime-scalable policy reuse and suggest using D(R)QN for larger and PPO for smaller library sizes.},
  archiveprefix = {arXiv},
  keywords = {/unread,\#nosource,⛔ No DOI found,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing}
}

@inproceedings{buzzega2020,
  title = {Dark {{Experience}} for {{General Continual Learning}}: A {{Strong}}, {{Simple Baseline}}},
  shorttitle = {Dark {{Experience}} for {{General Continual Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Buzzega, Pietro and Boschini, Matteo and Porrello, Angelo and Abati, Davide and CALDERARA, SIMONE},
  year = {2020},
  volume = {33},
  pages = {15920--15930},
  publisher = {{Curran Associates, Inc.}},
  url = {https://papers.nips.cc/paper/2020/hash/b704ea2c39778f07c617f6b7ce480e9e-Abstract.html},
  urldate = {2022-07-14},
  abstract = {Continual Learning has inspired a plethora of approaches and evaluation settings; however, the majority of them overlooks the properties of a practical scenario, where the data stream cannot be shaped as a sequence of tasks and offline training is not viable. We work towards General Continual Learning (GCL), where task boundaries blur and the domain and class distributions shift either gradually or suddenly. We address it through mixing rehearsal with knowledge distillation and regularization; our simple baseline, Dark Experience Replay, matches the network's logits sampled throughout the optimization trajectory, thus promoting consistency with its past. By conducting an extensive analysis on both standard benchmarks and a novel GCL evaluation setting (MNIST-360), we show that such a seemingly simple baseline outperforms consolidated approaches and leverages limited resources. We further explore the generalization capabilities of our objective, showing its regularization being beneficial beyond mere performance.},
  keywords = {/unread,⛔ No DOI found},
  note = {A hybrid strategy combining replay and regularization (distillation).
\par
A buffer stores input-logits pairs from previous experiences and at each iteration the strategy samples from the buffer and minimizes the squared distance between the logits sampled from the buffer and the logits predicted by the current model.}
}

@article{caccia2020,
  title = {Online {{Fast Adaptation}} and {{Knowledge Accumulation}}: A {{New Approach}} to {{Continual Learning}}},
  author = {Caccia, Massimo and Rodriguez, Pau and Ostapenko, Oleksiy and Normandin, Fabrice and Lin, Min and Caccia, Lucas and Laradji, Issam and Rish, Irina and Lacoste, Alexande and Vazquez, David and Charlin, Laurent},
  year = {2020},
  journal = {arXiv},
  url = {http://arxiv.org/abs/2003.05856},
  abstract = {Learning from non-stationary data remains a great challenge for machine learning. Continual learning addresses this problem in scenarios where the learning agent faces a stream of changing tasks. In these scenarios, the agent is expected to retain its highest performance on previous tasks without revisiting them while adapting well to the new tasks. Two new recent continual-learning scenarios have been proposed. In meta-continual learning, the model is pre-trained to minimize catastrophic forgetting when trained on a sequence of tasks. In continual-meta learning, the goal is faster remembering, i.e., focusing on how quickly the agent recovers performance rather than measuring the agent's performance without any adaptation. Both scenarios have the potential to propel the field forward. Yet in their original formulations, they each have limitations. As a remedy, we propose a more general scenario where an agent must quickly solve (new) out-of-distribution tasks, while also requiring fast remembering. We show that current continual learning, meta learning, meta-continual learning, and continual-meta learning techniques fail in this new scenario. Accordingly, we propose a strong baseline: Continual-MAML, an online extension of the popular MAML algorithm. In our empirical experiments, we show that our method is better suited to the new scenario than the methodologies mentioned above, as well as standard continual learning and meta learning approaches.},
  keywords = {[fashion],[framework],[mnist],/unread,\#nosource,⛔ No DOI found,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,continual meta learning,framework,MAML,meta continual learning,OSAKA},
  annotation = {\_eprint: 2003.05856},
  note = {arXiv: 2003.05856}
}

@article{camp2020,
  title = {Continual {{Learning}} with {{Deep Artificial Neurons}}},
  author = {Camp, Blake and Mandivarapu, Jaya Krishna and Estrada, Rolando},
  year = {2020},
  journal = {arXiv},
  url = {http://arxiv.org/abs/2011.07035},
  abstract = {Neurons in real brains are enormously complex computational units. Among other things, they're responsible for transforming inbound electro-chemical vectors into outbound action potentials, updating the strengths of intermediate synapses, regulating their own internal states, and modulating the behavior of other nearby neurons. One could argue that these cells are the only things exhibiting any semblance of real intelligence. It is odd, therefore, that the machine learning community has, for so long, relied upon the assumption that this complexity can be reduced to a simple sum and fire operation. We ask, might there be some benefit to substantially increasing the computational power of individual neurons in artificial systems? To answer this question, we introduce Deep Artificial Neurons (DANs), which are themselves realized as deep neural networks. Conceptually, we embed DANs inside each node of a traditional neural network, and we connect these neurons at multiple synaptic sites, thereby vectorizing the connections between pairs of cells. We demonstrate that it is possible to meta-learn a single parameter vector, which we dub a neuronal phenotype, shared by all DANs in the network, which facilitates a meta-objective during deployment. Here, we isolate continual learning as our meta-objective, and we show that a suitable neuronal phenotype can endow a single network with an innate ability to update its synapses with minimal forgetting, using standard backpropagation, without experience replay, nor separate wake/sleep phases. We demonstrate this ability on sequential non-linear regression tasks.},
  keywords = {[experimental],/unread,\#nosource,⛔ No DOI found},
  annotation = {\_eprint: 2011.07035},
  note = {The authors replace each neuron of a standard feedforward network with a small neural network with its own parameters, meta-learned and shared throughout the whole network. They experiment with regression on sine waves.}
}

@article{campo2020,
  title = {Continual {{Learning}} of {{Predictive Models}} in {{Video Sequences}} via {{Variational Autoencoders}}},
  author = {Campo, Damian and Slavic, Giulia and Baydoun, Mohamad and Marcenaro, Lucio and Regazzoni, Carlo},
  year = {2020},
  journal = {arXiv},
  url = {http://arxiv.org/abs/2006.01945},
  abstract = {This paper proposes a method for performing continual learning of predictive models that facilitate the inference of future frames in video sequences. For a first given experience, an initial Variational Autoencoder, together with a set of fully connected neural networks are utilized to respectively learn the appearance of video frames and their dynamics at the latent space level. By employing an adapted Markov Jump Particle Filter, the proposed method recognizes new situations and integrates them as predictive models avoiding catastrophic forgetting of previously learned tasks. For evaluating the proposed method, this article uses video sequences from a vehicle that performs different tasks in a controlled environment.},
  keywords = {[vision],/unread,\#nosource,⛔ No DOI found},
  annotation = {\_eprint: 2006.01945}
}

@inproceedings{carlson2010,
  title = {Toward an Architecture for Never-Ending Language Learning},
  booktitle = {Proceedings of the {{Twenty-Fourth AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Carlson, Andrew and Betteridge, Justin and Kisiel, Bryan and Settles, Burr and Hruschka, Estevam R. and Mitchell, Tom M.},
  year = {2010},
  series = {{{AAAI}}'10},
  pages = {1306--1313},
  publisher = {{AAAI Press}},
  address = {{Atlanta, Georgia}},
  url = {https://www.aaai.org/ocs/index.php/AAAI/AAAI10/paper/view/1879},
  urldate = {2021-06-27},
  abstract = {We consider here the problem of building a never-ending language learner; that is, an intelligent computer agent that runs forever and that each day must (1) extract, or read, information from the web to populate a growing structured knowledge base, and (2) learn to perform this task better than on the previous day. In particular, we propose an approach and a set of design principles for such an agent, describe a partial implementation of such a system that has already learned to extract a knowledge base containing over 242,000 beliefs with an estimated precision of 74\% after running for 67 days, and discuss lessons learned from this preliminary attempt to build a never-ending learning agent.},
  keywords = {[nlp],/unread,\#nosource}
}

@article{carpenter1988,
  title = {The {{ART}} of {{Adaptive Pattern Recognition}} by a {{Self-Organizing Neural Network}}},
  author = {Carpenter, Gail A. and Grossberg, Stephen},
  year = {1988},
  journal = {Computer},
  volume = {21},
  number = {3},
  pages = {77--88},
  issn = {00189162},
  doi = {10/dqmbg2},
  url = {https://ieeexplore.ieee.org/document/33},
  abstract = {The adaptive resonance theory (ART) suggests a solution to the stability-plasticity dilemma facing designers of learning systems, namely how to design a learning system that will remain plastic, or adaptive, in response to significant events and yet remain stable in response to irrelevant events. ART architectures are discussed that are neural networks that self-organize stable recognition codes in real time in response to arbitrary sequences of input patterns. Within such an ART architecture, the process of adaptive pattern recognition is a special case of the more general cognitive process of hypothesis discovery, testing, search, classification, and learning. This property opens up the possibility of applying ART systems to more general problems of adaptively processing large abstract information sources and databases. The main computational properties of these ART architectures are outlined and contrasted with those of alternative learning and recognition systems.\textbackslash textless \textbackslash textgreater},
  keywords = {/unread,\#nosource},
  note = {Seminal paper on the stability-plasticity dilemma.}
}

@article{cermelli2020,
  title = {Modeling the {{Background}} for {{Incremental Learning}} in {{Semantic Segmentation}}},
  author = {Cermelli, Fabio and Mancini, Massimiliano and Bul{\`o}, Samuel Rota and Ricci, Elisa and Caputo, Barbara},
  year = {2020},
  journal = {CVPR},
  pages = {9233--9242},
  url = {http://arxiv.org/abs/2002.00718},
  abstract = {Despite their effectiveness in a wide range of tasks, deep architectures suffer from some important limitations. In particular, they are vulnerable to catastrophic forgetting, i.e. they perform poorly when they are required to update their model as new classes are available but the original training set is not retained. This paper addresses this problem in the context of semantic segmentation. Current strategies fail on this task because they do not consider a peculiar aspect of semantic segmentation: since each training step provides annotation only for a subset of all possible classes, pixels of the background class (i.e. pixels that do not belong to any other classes) exhibit a semantic distribution shift. In this work we revisit classical incremental learning methods, proposing a new distillation-based framework which explicitly accounts for this shift. Furthermore, we introduce a novel strategy to initialize classifier's parameters, thus preventing biased predictions toward the background class. We demonstrate the effectiveness of our approach with an extensive evaluation on the Pascal-VOC 2012 and ADE20K datasets, significantly outperforming state of the art incremental learning methods.},
  keywords = {/unread,\#nosource,⛔ No DOI found},
  annotation = {\_eprint: 2002.00718}
}

@article{cha2021,
  title = {Co\$\^2\${{L}}: {{Contrastive Continual Learning}}},
  shorttitle = {Co\$\^2\${{L}}},
  author = {Cha, Hyuntak and Lee, Jaeho and Shin, Jinwoo},
  year = {2021},
  journal = {arXiv},
  eprint = {2106.14413},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2106.14413},
  urldate = {2021-07-01},
  abstract = {Recent breakthroughs in self-supervised learning show that such algorithms learn visual representations that can be transferred better to unseen tasks than joint-training methods relying on task-specific supervision. In this paper, we found that the similar holds in the continual learning con-text: contrastively learned representations are more robust against the catastrophic forgetting than jointly trained representations. Based on this novel observation, we propose a rehearsal-based continual learning algorithm that focuses on continually learning and maintaining transferable representations. More specifically, the proposed scheme (1) learns representations using the contrastive learning objective, and (2) preserves learned representations using a self-supervised distillation step. We conduct extensive experimental validations under popular benchmark image classification datasets, where our method sets the new state-of-the-art performance.},
  archiveprefix = {arXiv},
  keywords = {/unread,\#nosource,⛔ No DOI found,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  note = {Comment: 14 pages, 5 figures}
}

@inproceedings{chaudhry2018,
  title = {Riemannian {{Walk}} for {{Incremental Learning}}: {{Understanding Forgetting}} and {{Intransigence}}},
  shorttitle = {Riemannian {{Walk}} for {{Incremental Learning}}},
  booktitle = {Proceedings of the {{European Conference}} on {{Computer Vision}} ({{ECCV}})},
  author = {Chaudhry, Arslan and Dokania, Puneet K. and Ajanthan, Thalaiyasingam and Torr, Philip H. S.},
  year = {2018},
  pages = {532--547},
  url = {https://openaccess.thecvf.com/content_ECCV_2018/html/Arslan_Chaudhry__Riemannian_Walk_ECCV_2018_paper.html},
  urldate = {2021-01-05},
  keywords = {/unread,\#nosource}
}

@article{chaudhry2019,
  title = {On {{Tiny Episodic Memories}} in {{Continual Learning}}},
  author = {Chaudhry, Arslan and Rohrbach, Marcus and Elhoseiny, Mohamed and Ajanthan, Thalaiyasingam and Dokania, Puneet K and Torr, Philip H S and Ranzato, Marc'Aurelio},
  year = {2019},
  journal = {arXiv},
  url = {https://github.com/facebookresearch/agem http://arxiv.org/abs/1902.10486},
  abstract = {In continual learning (CL), an agent learns from a stream of tasks leveraging prior experience to transfer knowledge to future tasks. It is an ideal framework to decrease the amount of supervision in the existing learning algorithms. But for a successful knowledge transfer, the learner needs to remember how to perform previous tasks. One way to endow the learner the ability to perform tasks seen in the past is to store a small memory, dubbed episodic memory, that stores few examples from previous tasks and then to replay these examples when training for future tasks. In this work, we empirically analyze the effectiveness of a very small episodic memory in a CL setup where each training example is only seen once. Surprisingly, across four rather different supervised learning benchmarks adapted to CL, a very simple baseline, that jointly trains on both examples from the current task as well as examples stored in the episodic memory, significantly outperforms specifically designed CL approaches with and without episodic memory. Interestingly, we find that repetitive training on even tiny memories of past tasks does not harm generalization, on the contrary, it improves it, with gains between 7\$\textbackslash backslash\$\% and 17\$\textbackslash backslash\$\% when the memory is populated with a single example per class.},
  keywords = {[cifar],[imagenet],[mnist],/unread,\#nosource,⛔ No DOI found},
  annotation = {\_eprint: 1902.10486},
  note = {Training directly on past examples with very small replay memories enhances performances beyond the size of the replay memory.}
}

@inproceedings{chaudhry2019a,
  title = {Efficient {{Lifelong Learning}} with {{A-GEM}}},
  booktitle = {{{ICLR}}},
  author = {Chaudhry, Arslan and Ranzato, Marc'Aurelio and Rohrbach, Marcus and Elhoseiny, Mohamed},
  year = {2019},
  url = {http://arxiv.org/abs/1812.00420},
  abstract = {In lifelong learning, the learner is presented with a sequence of tasks, incrementally building a data-driven prior which may be leveraged to speed up learning of a new task. In this work, we investigate the efficiency of current lifelong approaches, in terms of sample complexity, computational and memory cost. Towards this end, we first introduce a new and a more realistic evaluation protocol, whereby learners observe each example only once and hyper-parameter selection is done on a small and disjoint set of tasks, which is not used for the actual learning experience and evaluation. Second, we introduce a new metric measuring how quickly a learner acquires a new skill. Third, we propose an improved version of GEM (Lopez-Paz \& Ranzato, 2017), dubbed Averaged GEM (A-GEM), which enjoys the same or even better performance as GEM, while being almost as computationally and memory efficient as EWC (Kirkpatrick et al., 2016) and other regularization-based methods. Finally, we show that all algorithms including A-GEM can learn even more quickly if they are provided with task descriptors specifying the classification tasks under consideration. Our experiments on several standard lifelong learning benchmarks demonstrate that A-GEM has the best trade-off between accuracy and efficiency.},
  langid = {english},
  keywords = {[cifar],[mnist],/unread,\#nosource,⛔ No DOI found,Computer Science - Machine Learning,Statistics - Machine Learning},
  note = {Comment: Published as a conference paper at ICLR 2019 arXiv: 1812.00420}
}

@article{chaudhry2021,
  title = {Using {{Hindsight}} to {{Anchor Past Knowledge}} in {{Continual Learning}}},
  author = {Chaudhry, Arslan and Gordo, Albert and Dokania, Puneet K. and Torr, Philip and {Lopez-Paz}, David},
  year = {2021},
  journal = {arXiv},
  eprint = {2002.08165},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2002.08165},
  urldate = {2022-05-03},
  abstract = {In continual learning, the learner faces a stream of data whose distribution changes over time. Modern neural networks are known to suffer under this setting, as they quickly forget previously acquired knowledge. To address such catastrophic forgetting, many continual learning methods implement different types of experience replay, re-learning on past data stored in a small buffer known as episodic memory. In this work, we complement experience replay with a new objective that we call anchoring, where the learner uses bilevel optimization to update its knowledge on the current task, while keeping intact the predictions on some anchor points of past tasks. These anchor points are learned using gradient-based optimization to maximize forgetting, which is approximated by fine-tuning the currently trained model on the episodic memory of past tasks. Experiments on several supervised learning benchmarks for continual learning demonstrate that our approach improves the standard experience replay in terms of both accuracy and forgetting metrics and for various sizes of episodic memories.},
  archiveprefix = {arXiv},
  keywords = {/unread,⛔ No DOI found,Computer Science - Machine Learning,Statistics - Machine Learning},
  annotation = {ZSCC:00055},
  note = {Comment: Accepted at AAAI 2021}
}

@article{chaudhry2021a,
  title = {Using {{Hindsight}} to {{Anchor Past Knowledge}} in {{Continual Learning}}},
  author = {Chaudhry, Arslan and Gordo, Albert and Dokania, Puneet K. and Torr, Philip and {Lopez-Paz}, David},
  year = {2021},
  journal = {arXiv},
  eprint = {2002.08165},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2002.08165},
  urldate = {2022-05-03},
  abstract = {In continual learning, the learner faces a stream of data whose distribution changes over time. Modern neural networks are known to suffer under this setting, as they quickly forget previously acquired knowledge. To address such catastrophic forgetting, many continual learning methods implement different types of experience replay, re-learning on past data stored in a small buffer known as episodic memory. In this work, we complement experience replay with a new objective that we call anchoring, where the learner uses bilevel optimization to update its knowledge on the current task, while keeping intact the predictions on some anchor points of past tasks. These anchor points are learned using gradient-based optimization to maximize forgetting, which is approximated by fine-tuning the currently trained model on the episodic memory of past tasks. Experiments on several supervised learning benchmarks for continual learning demonstrate that our approach improves the standard experience replay in terms of both accuracy and forgetting metrics and for various sizes of episodic memories.},
  archiveprefix = {arXiv},
  keywords = {/unread,⛔ No DOI found,Computer Science - Machine Learning,Statistics - Machine Learning},
  annotation = {ZSCC:00055},
  note = {Comment: Accepted at AAAI 2021}
}

@inproceedings{chen2016,
  title = {{{Net2Net}}: {{Accelerating Learning}} via {{Knowledge Transfer}}},
  shorttitle = {{{Net2Net}}},
  booktitle = {{{ICLR}}},
  author = {Chen, Tianqi and Goodfellow, Ian and Shlens, Jonathon},
  year = {2016},
  eprint = {1511.05641},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1511.05641},
  urldate = {2021-01-07},
  abstract = {We introduce techniques for rapidly transferring the information stored in one neural net into another neural net. The main purpose is to accelerate the training of a significantly larger neural net. During real-world workflows, one often trains very many different neural networks during the experimentation and design process. This is a wasteful process in which each new model is trained from scratch. Our Net2Net technique accelerates the experimentation process by instantaneously transferring the knowledge from a previous network to each new deeper or wider network. Our techniques are based on the concept of function-preserving transformations between neural network specifications. This differs from previous approaches to pre-training that altered the function represented by a neural net when adding layers to it. Using our knowledge transfer mechanism to add depth to Inception modules, we demonstrate a new state of the art accuracy rating on the ImageNet dataset.},
  archiveprefix = {arXiv},
  keywords = {/unread,\#nosource,⛔ No DOI found,Computer Science - Machine Learning},
  note = {Comment: ICLR 2016 submission}
}

@book{chen2018,
  title = {Lifelong {{Machine Learning}}, {{Second Edition}}},
  author = {Chen, Zhiyuan and Liu, Bing},
  year = {2018},
  journal = {Synthesis Lectures on Artificial Intelligence and Machine Learning},
  volume = {12},
  publisher = {{Morgan \& Claypool Publishers LLC}},
  issn = {1939-4608},
  doi = {10.2200/s00832ed1v01y201802aim037},
  url = {https://www.cs.uic.edu/ liub/lifelong-machine-learning.html},
  abstract = {Lifelong Machine Learning or Lifelong Learning (LL) is an advanced machine learning (ML) paradigm that learns continuously, accumulates the knowledge learned in the past, and uses/adapts it to help future learning and problem solving. In the process, the learner becomes more and more knowledgeable and better and better at learning. This continuous learning ability is one of the hallmarks of human intelligence. However, the current dominant ML paradigm learns in isolation: given a training dataset, it runs a ML algorithm only on the dataset to produce a model. It makes no attempt to retain the learned knowledge and use it in subsequent learning. Although this isolated ML paradigm, primarily based on data-driven optimization, has been very successful, it requires a large number of training examples, and is only suitable for well-defined and narrow tasks in closed environments. In contrast, we humans learn effectively with a few examples and in the dynamic and open world or environment in a self-supervised manner because our learning is also very much knowledge-driven: the knowledge learned in the past helps us learn new things with little data or effort and adapt to new/unseen situations. This self-suprevised (or self-aware) learning also enables us to learn on the job in the interaction with others and with the real-world environment with no external supervision. LL aims to achieve all these capabilities. Applications such as chatbots, s elf-driving cars, or any AI systems that interact with humans/physical environments are calling for these capabilities because they need to cope with their dynamic and open environments which leave them with no choice but to continuously learn new things in order to function well. Without the LL ability, an AI system cannot be considered truly intelligent, i.e., LL is necessary for intelligence or AGI (artificial general intelligence)},
  keywords = {/unread,\#nosource}
}

@article{chen2019,
  title = {Facilitating {{Bayesian Continual Learning}} by {{Natural Gradients}} and {{Stein Gradients}}},
  author = {Chen, Yu and Diethe, Tom and Lawrence, Neil},
  year = {2019},
  journal = {arXiv},
  url = {http://arxiv.org/abs/1904.10644},
  abstract = {Continual learning aims to enable machine learning models to learn a general solution space for past and future tasks in a sequential manner. Conventional models tend to forget the knowledge of previous tasks while learning a new task, a phenomenon known as catastrophic forgetting. When using Bayesian models in continual learning, knowledge from previous tasks can be retained in two ways: 1). posterior distributions over the parameters, containing the knowledge gained from inference in previous tasks, which then serve as the priors for the following task; 2). coresets, containing knowledge of data distributions of previous tasks. Here, we show that Bayesian continual learning can be facilitated in terms of these two means through the use of natural gradients and Stein gradients respectively.},
  keywords = {[bayes],/unread,\#nosource,⛔ No DOI found},
  annotation = {\_eprint: 1904.10644}
}

@inproceedings{chen2020,
  title = {Long {{Live}} the {{Lottery}}: {{The Existence}} of {{Winning Tickets}} in {{Lifelong Learning}}},
  shorttitle = {Long {{Live}} the {{Lottery}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Chen, Tianlong and Zhang, Zhenyu and Liu, Sijia and Chang, Shiyu and Wang, Zhangyang},
  year = {2020},
  url = {https://openreview.net/forum?id=LXMSvPmsm0g},
  urldate = {2021-01-17},
  abstract = {The lottery ticket hypothesis demonstrates that a highly sparsified sub-network can be trained in isolation, given the appropriate weight initialization. This paper extends that hypothesis from...},
  langid = {english},
  keywords = {/unread,\#nosource}
}

@inproceedings{chen2021,
  title = {A {{Biologically Plausible Audio-Visual Integration Model}} for {{Continual Learning}}},
  booktitle = {{{IJCNN}}},
  author = {Chen, Wenjie and Du, Fengtong and Wang, Ye and Cao, Lihong},
  year = {2021},
  eprint = {2007.08855},
  eprinttype = {arxiv},
  doi = {10/gnq33g},
  url = {http://arxiv.org/abs/2007.08855},
  urldate = {2021-07-21},
  abstract = {The problem of catastrophic forgetting has a history of more than 30 years and has not been completely solved yet. Since the human brain has natural ability to perform continual lifelong learning, learning from the brain may provide solutions to this problem. In this paper, we propose a novel biologically plausible audio-visual integration model (AVIM) based on the assumption that the integration of audio and visual perceptual information in the medial temporal lobe during learning is crucial to form concepts and make continual learning possible. Specifically, we use multi-compartment Hodgkin-Huxley neurons to build the model and adopt the calcium-based synaptic tagging and capture as the model's learning rule. Furthermore, we define a new continual learning paradigm to simulate the possible continual learning process in the human brain. We then test our model under this new paradigm. Our experimental results show that the proposed AVIM can achieve state-of-the-art continual learning performance compared with other advanced methods such as OWM, iCaRL and GEM. Moreover, it can generate stable representations of objects during learning. These results support our assumption that concept formation is essential for continuous lifelong learning and suggest the proposed AVIM is a possible concept formation mechanism.},
  archiveprefix = {arXiv},
  keywords = {/unread,\#nosource,Computer Science - Artificial Intelligence,Computer Science - Neural and Evolutionary Computing,Quantitative Biology - Neurons and Cognition},
  note = {Comment: Accepted by 2021 International Joint Conference on Neural Networks}
}

@article{cheung2019,
  title = {Superposition of Many Models into One},
  author = {Cheung, Brian and Terekhov, Alex and Chen, Yubei and Agrawal, Pulkit and Olshausen, Bruno},
  year = {2019},
  journal = {arXiv},
  url = {http://arxiv.org/abs/1902.05522},
  abstract = {We present a method for storing multiple models within a single set of parameters. Models can coexist in superposition and still be retrieved individually. In experiments with neural networks, we show that a surprisingly large number of models can be effectively stored within a single parameter instance. Furthermore, each of these models can undergo thousands of training steps without significantly interfering with other models within the superposition. This approach may be viewed as the online complement of compression: rather than reducing the size of a network after training, we make use of the unrealized capacity of a network during training.},
  keywords = {[cifar],[mnist],/unread,\#nosource,⛔ No DOI found},
  annotation = {\_eprint: 1902.05522}
}

@article{clopath2012,
  title = {Synaptic Consolidation: An Approach to Long-Term Learning},
  author = {Clopath, Claudia},
  year = {2012},
  journal = {Cognitive Neurodynamics},
  volume = {6},
  number = {3},
  pages = {251--257},
  issn = {1871-4080},
  doi = {10/fsmq5x},
  url = {http://link.springer.com/10.1007/s11571-011-9177-6},
  abstract = {Synaptic plasticity is thought to be the basis of learning and memory, but it is mostly studied on the timescale of mere minutes. This review discusses synaptic consolidation, a process that enables synapses to retain their strength for a much longer time (days to years), instead of returning to their original value. The process involves specific plasticity-related proteins, and depends on the dopamine D1/D5 receptors. Here, we review the research on synaptic consolidation, describing electrophysiology experiments, recent modeling work, as well as behavioral correlates.},
  isbn = {1871-4080 (Print)},
  pmid = {23730356},
  keywords = {[hebbian],/unread,\#nosource,Behavior,Electrophysiology,Model,Review,Synaptic consolidation,Synaptic plasticity,Synaptic tagging}
}

@inproceedings{coop2012,
  title = {Mitigation of Catastrophic Interference in Neural Networks Using a Fixed Expansion Layer},
  booktitle = {2012 {{IEEE}} 55th {{International Midwest Symposium}} on {{Circuits}} and {{Systems}} ({{MWSCAS}})},
  author = {Coop, Robert and Arel, Itamar},
  year = {2012},
  pages = {726--729},
  publisher = {{IEEE}},
  doi = {10/gqjr2j},
  url = {http://ieeexplore.ieee.org/document/6292123/},
  abstract = {In this paper we present the fixed expansion layer (FEL) feedforward neural network designed for balancing plasticity and stability in the presence of non-stationary inputs. Catastrophic interference (or catastrophic forgetting) refers to the drastic loss of previously learned information when a neural network is trained on new or different information. The goal of the FEL network is to reduce the effect of catastrophic interference by augmenting a multilayer perceptron with a layer of sparse neurons with binary activations. We compare the FEL network's performance to that of other algorithms designed to combat the effects of catastrophic interference and demonstrate that the FEL network is able to retain information for significantly longer periods of time with substantially lower computational requirements.},
  isbn = {978-1-4673-2527-1},
  keywords = {[sparsity],/unread,\#nosource,Accuracy,binary activations,Biological neural networks,catastrophic forgetting,catastrophic interference,Feedforward neural networks,fixed expansion layer feedforward neural network,Interference,multilayer perceptron,multilayer perceptrons,Neurons,non-stationary inputs,sparse neurons,Training},
  note = {ISSN: 1548-3746}
}

@inproceedings{coop2013,
  title = {Mitigation of Catastrophic Forgetting in Recurrent Neural Networks Using a {{Fixed Expansion Layer}}},
  booktitle = {The 2013 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  author = {Coop, Robert and Arel, Itamar},
  year = {2013},
  pages = {1--7},
  publisher = {{IEEE}},
  address = {{Dallas, TX, USA}},
  doi = {10/gqjr2h},
  url = {http://ieeexplore.ieee.org/document/6707047/},
  abstract = {Catastrophic forgetting (or catastrophic interference) in supervised learning systems is the drastic loss of previously stored information caused by the learning of new information. While substantial work has been published on addressing catastrophic forgetting in memoryless supervised learning systems (e.g. feedforward neural networks), the problem has received limited attention in the context of dynamic systems, particularly recurrent neural networks. In this paper, we introduce a solution for mitigating catastrophic forgetting in RNNs based on enhancing the Fixed Expansion Layer (FEL) neural network which exploits sparse coding of hidden neuron activations. Simulation results on several non-stationary data sets clearly demonstrate the effectiveness of the proposed architecture.},
  isbn = {978-1-4673-6129-3 978-1-4673-6128-6},
  langid = {english},
  keywords = {[mnist],[rnn],[sparsity],/unread,\#nosource,fel,recurrent fel}
}

@inproceedings{cossu2020,
  title = {Continual {{Learning}} with {{Gated Incremental Memories}} for Sequential Data Processing},
  booktitle = {Proceedings of the 2020 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}} 2020)},
  author = {Cossu, Andrea and Carta, Antonio and Bacciu, Davide},
  year = {2020},
  url = {http://arxiv.org/abs/2004.04077},
  abstract = {The ability to learn in dynamic, nonstationary environments without forgetting previous knowledge, also known as Continual Learning (CL), is a key enabler for scalable and trustworthy deployments of adaptive solutions. While the importance of continual learning is largely acknowledged in machine vision and reinforcement learning problems, this is mostly under-documented for sequence processing tasks. This work proposes a Recurrent Neural Network (RNN) model for CL that is able to deal with concept drift in input distribution without forgetting previously acquired knowledge. We also implement and test a popular CL approach, Elastic Weight Consolidation (EWC), on top of two different types of RNNs. Finally, we compare the performances of our enhanced architecture against EWC and RNNs on a set of standard CL benchmarks, adapted to the sequential data processing scenario. Results show the superior performance of our architecture and highlight the need for special solutions designed to address CL in RNNs.},
  keywords = {[mnist],[rnn],/unread,\#nosource,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computi,Statistics - Machine Learning},
  note = {An evaluation of RNNs (LSTM and LMN) inspired by Progressive networks, leading to the Gated Incremental Memory approach to overcome catastrophic forgetting.
\par
An evaluation of RNNs (LSTM and LMN) inspired by Progressive networks, leading to the Gated Incremental Memory approach to overcome catastrophic forgetting.}
}

@article{cossu2021,
  title = {Continual Learning for Recurrent Neural Networks: {{An}} Empirical Evaluation},
  shorttitle = {Continual Learning for Recurrent Neural Networks},
  author = {Cossu, Andrea and Carta, Antonio and Lomonaco, Vincenzo and Bacciu, Davide},
  year = {2021},
  journal = {Neural Networks},
  volume = {143},
  pages = {607--627},
  issn = {0893-6080},
  doi = {10/gnq33k},
  url = {https://www.sciencedirect.com/science/article/pii/S0893608021002847},
  urldate = {2021-08-12},
  abstract = {Learning continuously during all model lifetime is fundamental to deploy machine learning solutions robust to drifts in the data distribution. Advances in Continual Learning (CL) with recurrent neural networks could pave the way to a large number of applications where incoming data is non stationary, like natural language processing and robotics. However, the existing body of work on the topic is still fragmented, with approaches which are application-specific and whose assessment is based on heterogeneous learning protocols and datasets. In this paper, we organize the literature on CL for sequential data processing by providing a categorization of the contributions and a review of the benchmarks. We propose two new benchmarks for CL with sequential data based on existing datasets, whose characteristics resemble real-world applications. We also provide a broad empirical evaluation of CL and Recurrent Neural Networks in class-incremental scenario, by testing their ability to mitigate forgetting with a number of different strategies which are not specific to sequential data processing. Our results highlight the key role played by the sequence length and the importance of a clear specification of the CL scenario.},
  langid = {english},
  keywords = {[rnn],/unread,\#nosource,Benchmarks,Continual learning,Evaluation,Recurrent neural networks}
}

@inproceedings{cossu2021a,
  title = {Sustainable {{Artificial Intelligence}} through {{Continual Learning}}},
  booktitle = {International {{Conference}} on {{AI}} for {{People}} ({{CAIP}})},
  author = {Cossu, Andrea and Ziosi, Marta and Lomonaco, Vincenzo},
  year = {2021},
  publisher = {{EAI CORE}},
  doi = {10/gpt3cw},
  url = {https://arxiv.org/abs/2111.09437},
  abstract = {The increasing attention on Artificial Intelligence (AI) regulamentation has led to the definition of a set of ethical principles grouped into the Sustainable AI framework. In this article, we identify Continual Learning, an active area of AI research, as a promising approach towards the design of systems compliant with the Sustainable AI principles. While Sustainable AI outlines general desiderata for ethical applications, Continual Learning provides means to put such desiderata into practice.},
  copyright = {All rights reserved},
  keywords = {/unread,\#nosource}
}

@article{cossu2021b,
  title = {Is {{Class-Incremental Enough}} for {{Continual Learning}}?},
  author = {Cossu, Andrea and Graffieti, Gabriele and Pellegrini, Lorenzo and Maltoni, Davide and Bacciu, Davide and Carta, Antonio and Lomonaco, Vincenzo},
  year = {2021},
  journal = {arXiv},
  eprint = {2112.02925},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2112.02925},
  urldate = {2021-12-07},
  abstract = {The ability of a model to learn continually can be empirically assessed in different continual learning scenarios. Each scenario defines the constraints and the opportunities of the learning environment. Here, we challenge the current trend in the continual learning literature to experiment mainly on class-incremental scenarios, where classes present in one experience are never revisited. We posit that an excessive focus on this setting may be limiting for future research on continual learning, since class-incremental scenarios artificially exacerbate catastrophic forgetting, at the expense of other important objectives like forward transfer and computational efficiency. In many real-world environments, in fact, repetition of previously encountered concepts occurs naturally and contributes to softening the disruption of previous knowledge. We advocate for a more in-depth study of alternative continual learning scenarios, in which repetition is integrated by design in the stream of incoming information. Starting from already existing proposals, we describe the advantages such class-incremental with repetition scenarios could offer for a more comprehensive assessment of continual learning models.},
  archiveprefix = {arXiv},
  keywords = {/unread,\#nosource,⛔ No DOI found,Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  note = {Comment: Under review}
}

@article{cui2016,
  title = {Continuous {{Online Sequence Learning}} with an {{Unsupervised Neural Network Model}}},
  author = {Cui, Yuwei and Ahmad, Subutai and Hawkins, Jeff},
  year = {2016},
  journal = {Neural Computation},
  volume = {28},
  number = {11},
  pages = {2474--2504},
  issn = {0899-7667},
  doi = {10/bsx5},
  url = {https://doi.org/10.1162/NECO_a_00893},
  abstract = {The ability to recognize and predict temporal sequences of sensory inputs is vital for survival in natural environments. Based on many known properties of cortical neurons, hierarchical temporal memory (HTM) sequence memory recently has been proposed as a theoretical framework for sequence learning in the cortex. In this letter, we analyze properties of HTM sequence memory and apply it to sequence learning and prediction problems with streaming data. We show the model is able to continuously learn a large number of variable order temporal sequences using an unsupervised Hebbian-like learning rule. The sparse temporal codes formed by the model can robustly handle branching temporal sequences by maintaining multiple predictions until there is sufficient disambiguating evidence. We compare the HTM sequence memory with other sequence learning algorithms, including statistical methods\textemdash autoregressive integrated moving average; feedforward neural networks\textemdash time delay neural network and online sequential extreme learning machine; and recurrent neural networks\textemdash long short-term memory and echo-state networks on sequence prediction problems with both artificial and real-world data. The HTM model achieves comparable accuracy to other state-of-the-art algorithms. The model also exhibits properties that are critical for sequence learning, including continuous online learning, the ability to handle multiple predictions and branching sequences with high-order statistics, robustness to sensor noise and fault tolerance, and good performance without task-specific hyperparameter tuning. Therefore, the HTM sequence memory not only advances our understanding of how the brain may solve the sequence learning problem but is also applicable to real-world sequence learning problems from continuous data streams.},
  keywords = {[spiking],/unread,\#nosource,htm},
  note = {Publisher: MIT Press}
}

@inproceedings{dautume2019,
  title = {Episodic {{Memory}} in {{Lifelong Language Learning}}},
  booktitle = {{{NeurIPS}}},
  author = {D'Autume, Cyprien de Masson and Ruder, Sebastian and Kong, Lingpeng and Yogatama, Dani},
  year = {2019},
  url = {http://arxiv.org/abs/1906.01076},
  abstract = {We introduce a lifelong language learning setup where a model needs to learn from a stream of text examples without any dataset identifier. We propose an episodic memory model that performs sparse experience replay and local adaptation to mitigate catastrophic forgetting in this setup. Experiments on text classification and question answering demonstrate the complementary benefits of sparse experience replay and local adaptation to allow the model to continuously learn from new datasets. We also show that the space complexity of the episodic memory module can be reduced significantly ({$\sim$}50-90\%) by randomly choosing which examples to store in memory with a minimal decrease in performance. We consider an episodic memory component as a crucial building block of general linguistic intelligence and see our model as a first step in that direction.},
  keywords = {[nlp],/unread,\#nosource},
  annotation = {\_eprint: 1906.01076}
}

@inproceedings{davidson2020,
  title = {Sequential Mastery of Multiple Visual Tasks: {{Networks}} Naturally Learn to Learn and Forget to Forget},
  booktitle = {{{CVPR}}},
  author = {Davidson, Guy and Mozer, Michael C},
  year = {2020},
  pages = {9282--9293},
  url = {https://openaccess.thecvf.com/content_CVPR_2020/papers/Davidson_Sequential_Mastery_of_Multiple_Visual_Tasks_Networks_Naturally_Learn_to_CVPR_2020_paper.pdf},
  abstract = {We explore the behavior of a standard convolutional neural net in a continual-learning setting that introduces visual classification tasks sequentially and requires the net to master new tasks while preserving mastery of previously learned tasks. This setting corresponds to that which human learners face as they acquire domain expertise serially, for example, as an individual studies a textbook. Through simulations involving sequences of ten related visual tasks, we find reason for optimism that nets will scale well as they advance from having a single skill to becoming multi-skill domain experts. We observe two key phenomena. First, forward facilitation-the accelerated learning of task n+1 having learned n previous tasks-grows with n. Second, backward interference-the forgetting of the n previous tasks when learning task n + 1-diminishes with n. Amplifying forward facilitation is the goal of research on metalearning, and attenuating backward interference is the goal of research on catastrophic forgetting. We find that both of these goals are attained simply through broader exposure to a domain.},
  keywords = {[vision],/unread,\#nosource,⛔ No DOI found}
}

@inproceedings{dehghan2019,
  title = {Online {{Object}} and {{Task Learning}} via {{Human Robot Interaction}}},
  booktitle = {2019 {{International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Dehghan, M. and Zhang, Z. and Siam, M. and Jin, J. and Petrich, L. and Jagersand, M.},
  year = {2019},
  doi = {10/gnq33h},
  url = {https://arxiv.org/abs/1809.08722},
  abstract = {This work describes the development of a robotic system that acquires knowledge incrementally through human interaction where new tools and motions are taught on the fly. The robotic system developed was one of the five finalists in the KUKA Innovation Award competition and demonstrated during the Hanover Messe 2018 in Germany. The main contributions of the system are a) a novel incremental object learning module - a deep learning based localization and recognition system - that allows a human to teach new objects to the robot, b) an intuitive user interface for specifying 3D motion task associated with the new object, c) a hybrid force-vision control module for performing compliant motion on an unstructured surface. This paper describes the implementation and integration of the main modules of the system and summarizes the lessons learned from the competition.},
  keywords = {/unread,\#nosource}
}

@article{delange2020,
  title = {Unsupervised {{Model Personalization}} While {{Preserving Privacy}} and {{Scalability}}: {{An Open Problem}}},
  author = {De Lange, Matthias and Jia, Xu and Parisot, Sarah and Leonardis, Ales and Slabaugh, Gregory and Tuytelaars, Tinne},
  year = {2020},
  journal = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages = {14451--14460},
  doi = {10/ghbcw4},
  url = {http://arxiv.org/abs/2003.13296},
  abstract = {This work investigates the task of unsupervised model personalization, adapted to continually evolving, unlabeled local user images. We consider the practical scenario where a high capacity server interacts with a myriad of resource-limited edge devices, imposing strong requirements on scalability and local data privacy. We aim to address this challenge within the continual learning paradigm and provide a novel Dual User-Adaptation framework (DUA) to explore the problem. This framework flexibly disentangles user-adaptation into model personalization on the server and local data regularization on the user device, with desirable properties regarding scalability and privacy constraints. First, on the server, we introduce incremental learning of task-specific expert models, subsequently aggregated using a concealed unsupervised user prior. Aggregation avoids retraining, whereas the user prior conceals sensitive raw user data, and grants unsupervised adaptation. Second, local user-adaptation incorporates a domain adaptation point of view, adapting regularizing batch normalization parameters to the user data. We explore various empirical user configurations with different priors in categories and a tenfold of transforms for MIT Indoor Scene recognition, and classify numbers in a combined MNIST and SVHN setup. Extensive experiments yield promising results for data-driven local adaptation and elicit user priors for server adaptation to depend on the model rather than user data. Hence, although user-adaptation remains a challenging open problem, the DUA framework formalizes a principled foundation for personalizing both on server and user device, while maintaining privacy and scalability.},
  keywords = {[framework],[mnist],[vision],/unread,\#nosource},
  annotation = {\_eprint: 2003.13296}
}

@article{delange2021,
  title = {A Continual Learning Survey: {{Defying}} Forgetting in Classification Tasks},
  author = {De Lange, Matthias and Aljundi, Rahaf and Masana, Marc and Parisot, Sarah and Jia, Xu and Leonardis, Ales and Slabaugh, Gregory and Tuytelaars, Tinne},
  year = {2021},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  doi = {10/gk9mt2},
  url = {http://arxiv.org/abs/1909.08383},
  abstract = {Artificial neural networks thrive in solving the classification problem for a particular rigid task, acquiring knowledge through generalized learning behaviour from a distinct training phase. The resulting network resembles a static entity of knowledge, with endeavours to extend this knowledge without targeting the original task resulting in a catastrophic forgetting. Continual learning shifts this paradigm towards networks that can continually accumulate knowledge over different tasks without the need to retrain from scratch. We focus on task incremental classification, where tasks arrive sequentially and are delineated by clear boundaries. Our main contributions concern 1) a taxonomy and extensive overview of the state-of-the-art, 2) a novel framework to continually determine the stability-plasticity trade-off of the continual learner, 3) a comprehensive experimental comparison of 11 state-of-the-art continual learning methods and 4 baselines. We empirically scrutinize method strengths and weaknesses on three benchmarks, considering Tiny Imagenet and large-scale unbalanced iNaturalist and a sequence of recognition datasets. We study the influence of model capacity, weight decay and dropout regularization, and the order in which the tasks are presented, and qualitatively compare methods in terms of required memory, computation time, and storage.},
  keywords = {[framework],/unread,\#nosource,catastrophic forgetting,classification,Index Terms-Continual Learning,lifelong learning,neural networks ✦,task incremental learning},
  annotation = {\_eprint: 1909.08383}
}

@inproceedings{delange2021a,
  title = {Continual {{Prototype Evolution}}: {{Learning Online}} from {{Non-Stationary Data Streams}}},
  shorttitle = {Continual {{Prototype Evolution}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {De Lange, Matthias and Tuytelaars, Tinne},
  year = {2021},
  pages = {8250--8259},
  url = {https://openaccess.thecvf.com/content/ICCV2021/html/De_Lange_Continual_Prototype_Evolution_Learning_Online_From_Non-Stationary_Data_Streams_ICCV_2021_paper.html},
  abstract = {Attaining prototypical features to represent class distributions is well established in representation learning. However, learning prototypes online from streaming data proves a challenging endeavor as they rapidly become outdated, caused by an ever-changing parameter space during the learning process. Additionally, continual learning does not assume the data stream to be stationary, typically resulting in catastrophic forgetting of previous knowledge. As a first, we introduce a system addressing both problems, where prototypes evolve continually in a shared latent space, enabling learning and prediction at any point in time. To facilitate learning, a novel objective function synchronizes the latent space with the continually evolving prototypes. In contrast to the major body of work in continual learning, data streams are processed in an online fashion without task information and can be highly imbalanced, for which we propose an efficient memory scheme. As an additional contribution, we propose the learner-evaluator framework that i) generalizes existing paradigms in continual learning, ii) introduces data incremental learning, and iii) models the bridge between continual learning and concept drift. We obtain state-of-the-art performance by a significant margin on eight benchmarks, including three highly imbalanced data streams. Code is publicly available.},
  keywords = {[cifar],[framework],[mnist],[vision],/unread,\#nosource,⛔ No DOI found},
  note = {Definition of the data-incremental continual learning scenario}
}

@misc{delange2021b,
  title = {Continual {{Prototype Evolution}}: {{Learning Online}} from {{Non-Stationary Data Streams}}},
  shorttitle = {Continual {{Prototype Evolution}}},
  author = {De Lange, Matthias and Tuytelaars, Tinne},
  year = {2021},
  number = {arXiv:2009.00919},
  eprint = {2009.00919},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2009.00919},
  urldate = {2022-07-08},
  abstract = {Attaining prototypical features to represent class distributions is well established in representation learning. However, learning prototypes online from streaming data proves a challenging endeavor as they rapidly become outdated, caused by an ever-changing parameter space during the learning process. Additionally, continual learning does not assume the data stream to be stationary, typically resulting in catastrophic forgetting of previous knowledge. As a first, we introduce a system addressing both problems, where prototypes evolve continually in a shared latent space, enabling learning and prediction at any point in time. In contrast to the major body of work in continual learning, data streams are processed in an online fashion, without additional task-information, and an efficient memory scheme provides robustness to imbalanced data streams. Besides nearest neighbor based prediction, learning is facilitated by a novel objective function, encouraging cluster density about the class prototype and increased inter-class variance. Furthermore, the latent space quality is elevated by pseudo-prototypes in each batch, constituted by replay of exemplars from memory. As an additional contribution, we generalize the existing paradigms in continual learning to incorporate data incremental learning from data streams by formalizing a two-agent learner-evaluator framework. We obtain state-of-the-art performance by a significant margin on eight benchmarks, including three highly imbalanced data streams.},
  archiveprefix = {arXiv},
  keywords = {/unread,Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: 10 pages, code publicly available}
}

@inproceedings{dhar2019,
  title = {Learning without {{Memorizing}}},
  booktitle = {{{CVPR}}},
  author = {Dhar, Prithviraj and Vikram Singh, Rajat and Peng, Kuan-Chuan and Wu, Ziyan and Chellappa, Rama},
  year = {2019},
  doi = {10/gg2677},
  url = {https://openaccess.thecvf.com/content_CVPR_2019/papers/Dhar_Learning_Without_Memorizing_CVPR_2019_paper.pdf},
  abstract = {Incremental learning (IL) is an important task aimed at increasing the capability of a trained model, in terms of the number of classes recognizable by the model. The key problem in this task is the requirement of storing data (e.g. images) associated with existing classes, while teaching the classifier to learn new classes. However, this is impractical as it increases the memory requirement at every incre-mental step, which makes it impossible to implement IL algorithms on edge devices with limited memory. Hence, we propose a novel approach, called 'Learning without Memorizing (LwM)', to preserve the information about existing (base) classes, without storing any of their data, while making the classifier progressively learn the new classes. In LwM, we present an information preserving penalty: Attention Distillation Loss (L AD), and demonstrate that penalizing the changes in classifiers' attention maps helps to retain information of the base classes, as new classes are added. We show that adding L AD to the distillation loss which is an existing information preserving loss consistently outperforms the state-of-the-art performance in the iILSVRC-small and iCIFAR-100 datasets in terms of the overall accuracy of base and incrementally learned classes.},
  keywords = {[cifar],/unread,\#nosource}
}

@article{diaz-rodriguez2018,
  title = {Don't Forget, There Is More than Forgetting: New Metrics for {{Continual Learning}}},
  shorttitle = {Don't Forget, There Is More than Forgetting},
  author = {{D{\'i}az-Rodr{\'i}guez}, Natalia and Lomonaco, Vincenzo and Filliat, David and Maltoni, Davide},
  year = {2018},
  journal = {arXiv},
  url = {http://arxiv.org/abs/1810.13166},
  abstract = {Continual learning consists of algorithms that learn from a stream of data/tasks continuously and adaptively thought time, enabling the incremental development of ever more complex knowledge and skills. The lack of consensus in evaluating continual learning algorithms and the almost exclusive focus on forgetting motivate us to propose a more comprehensive set of implementation independent metrics accounting for several factors we believe have practical implications worth considering in the deployment of real AI systems that learn continually: accuracy or performance over time, backward and forward knowledge transfer, memory overhead as well as computational efficiency. Drawing inspiration from the standard Multi-Attribute Value Theory (MAVT) we further propose to fuse these metrics into a single score for ranking purposes and we evaluate our proposal with five continual learning strategies on the iCIFAR-100 continual learning benchmark.},
  keywords = {[cifar],[framework],/unread,\#nosource,⛔ No DOI found,68T05,Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Rec,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computi,cs.AI,cs.CV,cs.LG,cs.NE,stat.ML},
  note = {arXiv: 1810.13166}
}

@article{diethe2019,
  title = {Continual {{Learning}} in {{Practice}}},
  author = {Diethe, Tom and Borchert, Tom and Thereska, Eno and Balle, Borja and Lawrence, Neil},
  year = {2019},
  journal = {arXiv},
  url = {http://arxiv.org/abs/1903.05202},
  abstract = {This paper describes a reference architecture for self-maintaining systems that can learn continually, as data arrives. In environments where data evolves, we need architectures that manage Machine Learning (ML) models in production, adapt to shifting data distributions, cope with outliers, retrain when necessary, and adapt to new tasks. This represents continual AutoML or Automatically Adaptive Machine Learning. We describe the challenges and proposes a reference architecture.},
  keywords = {/unread,\#nosource,⛔ No DOI found},
  annotation = {\_eprint: 1903.05202}
}

@article{ditzler2015,
  title = {Learning in {{Nonstationary Environments}}: {{A Survey}}},
  shorttitle = {Learning in {{Nonstationary Environments}}},
  author = {Ditzler, Gregory and Roveri, Manuel and Alippi, Cesare and Polikar, Robi},
  year = {2015},
  journal = {IEEE Computational Intelligence Magazine},
  volume = {10},
  number = {4},
  pages = {12--25},
  issn = {1556-603X},
  doi = {10/f7vzg9},
  url = {http://ieeexplore.ieee.org/document/7296710/},
  abstract = {The prevalence of mobile phones, the internet-of-things technology, and networks of sensors has led to an enormous and ever increasing amount of data that are now more commonly available in a streaming fashion [1]-[5]. Often, it is assumed - either implicitly or explicitly - that the process generating such a stream of data is stationary, that is, the data are drawn from a fixed, albeit unknown probability distribution. In many real-world scenarios, however, such an assumption is simply not true, and the underlying process generating the data stream is characterized by an intrinsic nonstationary (or evolving or drifting) phenomenon. The nonstationarity can be due, for example, to seasonality or periodicity effects, changes in the users' habits or preferences, hardware or software faults affecting a cyber-physical system, thermal drifts or aging effects in sensors. In such nonstationary environments, where the probabilistic properties of the data change over time, a non-adaptive model trained under the false stationarity assumption is bound to become obsolete in time, and perform sub-optimally at best, or fail catastrophically at worst.},
  langid = {english},
  keywords = {/unread,\#nosource}
}

@article{dohare2021,
  title = {Continual {{Backprop}}: {{Stochastic Gradient Descent}} with {{Persistent Randomness}}},
  shorttitle = {Continual {{Backprop}}},
  author = {Dohare, Shibhansh and Mahmood, A. Rupam and Sutton, Richard S.},
  year = {2021},
  journal = {arXiv},
  eprint = {2108.06325},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2108.06325},
  urldate = {2021-08-17},
  abstract = {The Backprop algorithm for learning in neural networks utilizes two mechanisms: first, stochastic gradient descent and second, initialization with small random weights, where the latter is essential to the effectiveness of the former. We show that in continual learning setups, Backprop performs well initially, but over time its performance degrades. Stochastic gradient descent alone is insufficient to learn continually; the initial randomness enables only initial learning but not continual learning. To the best of our knowledge, ours is the first result showing this degradation in Backprop's ability to learn. To address this issue, we propose an algorithm that continually injects random features alongside gradient descent using a new generate-and-test process. We call this the Continual Backprop algorithm. We show that, unlike Backprop, Continual Backprop is able to continually adapt in both supervised and reinforcement learning problems. We expect that as continual learning becomes more common in future applications, a method like Continual Backprop will be essential where the advantages of random initialization are present throughout learning.},
  archiveprefix = {arXiv},
  keywords = {/unread,\#nosource,⛔ No DOI found,Computer Science - Machine Learning}
}

@article{douillard2020,
  title = {{{PLOP}}: {{Learning}} without {{Forgetting}} for {{Continual Semantic Segmentation}}},
  author = {Douillard, Arthur and Chen, Yifu and Dapogny, Arnaud and Cord, Matthieu},
  year = {2020},
  journal = {arXiv},
  url = {https://arxiv.org/abs/2011.11390},
  abstract = {Deep learning approaches are nowadays ubiquitously used to tackle computer vision tasks such as semantic segmentation, requiring large datasets and substantial computational power. Continual learning for semantic segmentation (CSS) is an emerging trend that consists in updating an old model by sequentially adding new classes. However, continual learning methods are usually prone to catastrophic forgetting. This issue is further aggravated in CSS where, at each step, old classes from previous iterations are collapsed into the background. In this paper, we propose Local POD, a multi-scale pooling distillation scheme that preserves long- and short-range spatial relationships at feature level. Furthermore, we design an entropy-based pseudo-labelling of the background w.r.t. classes predicted by the old model to deal with background shift and avoid catastrophic forgetting of the old classes. Our approach, called PLOP, significantly outperforms state-of-the-art methods in existing CSS scenarios, as well as in newly proposed challenging benchmarks.},
  keywords = {/unread,\#nosource,⛔ No DOI found}
}

@article{douillard2020a,
  title = {Insights from the {{Future}} for {{Continual Learning}}},
  author = {Douillard, Arthur and Valle, Eduardo and Ollion, Charles and Robert, Thomas and Cord, Matthieu},
  year = {2020},
  journal = {arXiv},
  url = {https://arxiv.org/abs/2006.13748},
  abstract = {Continual learning aims to learn tasks sequentially, with (often severe) constraints on the storage of old learning samples, without suffering from catastrophic forgetting. In this work, we propose prescient continual learning, a novel experimental setting, to incorporate existing information about the classes, prior to any training data. Usually, each task in a traditional continual learning setting evaluates the model on present and past classes, the latter with a limited number of training samples. Our setting adds future classes, with no training samples at all. We introduce Ghost Model, a representation-learning-based model for continual learning using ideas from zero-shot learning. A generative model of the representation space in concert with a careful adjustment of the losses allows us to exploit insights from future classes to constraint the spatial arrangement of the past and current classes. Quantitative results on the AwA2 and aP\textbackslash\&Y datasets and detailed visualizations showcase the interest of this new setting and the method we propose to address it.},
  keywords = {/unread,\#nosource,⛔ No DOI found}
}

@article{douillard2020b,
  title = {{{PODNet}}: {{Pooled Outputs Distillation}} for {{Small-Tasks Incremental Learning}}},
  author = {Douillard, Arthur and Cord, Matthieu and Ollion, Charles and Robert, Thomas and Valle, Eduardo},
  year = {2020},
  journal = {European Conference on Computer Vision (ECCV)},
  url = {https://arxiv.org/abs/2004.13513},
  abstract = {Lifelong learning has attracted much attention, but existing works still struggle to fight catastrophic forgetting and accumulate knowledge over long stretches of incremental learning. In this work, we propose PODNet, a model inspired by representation learning. By carefully balancing the compromise between remembering the old classes and learning new ones, PODNet fights catastrophic forgetting, even over very long runs of small incremental tasks --a setting so far unexplored by current works. PODNet innovates on existing art with an efficient spatial-based distillation-loss applied throughout the model and a representation comprising multiple proxy vectors for each class. We validate those innovations thoroughly, comparing PODNet with three state-of-the-art models on three datasets: CIFAR100, ImageNet100, and ImageNet1000. Our results showcase a significant advantage of PODNet over existing art, with accuracy gains of 12.10, 6.51, and 2.85 percentage points, respectively. Code is available at this https URL},
  keywords = {/unread,\#nosource,⛔ No DOI found},
  note = {Official Code\\
\href{https://github.com/arthurdouillard/incremental_learning.pytorch}{https://github.com/arthurdouillard/incremental\_learning.pytorch}}
}

@article{douillard2021,
  title = {Continuum: {{Simple Management}} of {{Complex Continual Learning Scenarios}}},
  author = {Douillard, Arthur and Lesort, Timoth{\'e}e},
  year = {2021},
  journal = {arXiv},
  url = {https://arxiv.org/abs/2102.06253},
  abstract = {Continual learning is a machine learning sub-field specialized in settings with non-iid data. Hence, the training data distribution is not static and drifts through time. Those drifts might cause interferences in the trained model and knowledge learned on previous states of the data distribution might be forgotten. Continual learning's challenge is to create algorithms able to learn an ever-growing amount of knowledge while dealing with data distribution drifts. One implementation difficulty in these field is to create data loaders that simulate non-iid scenarios. Indeed, data loaders are a key component for continual algorithms. They should be carefully designed and reproducible. Small errors in data loaders have a critical impact on algorithm results, e.g. with bad preprocessing, wrong order of data or bad test set. Continuum is a simple and efficient framework with numerous data loaders that avoid researcher to spend time on designing data loader and eliminate time-consuming errors. Using our proposed framework, it is possible to directly focus on the model design by using the multiple scenarios and evaluation metrics implemented. Furthermore the framework is easily extendable to add novel settings for specific needs.},
  keywords = {/unread,\#nosource,⛔ No DOI found}
}

@inproceedings{draelos2017,
  title = {Neurogenesis {{Deep Learning}}},
  booktitle = {{{IJCNN}}},
  author = {Draelos, Timothy John and Miner, Nadine E and Lamb, Christopher and Cox, Jonathan A and Vineyard, Craig Michael and Carlson, Kristofor David and Severa, William Mark and James, Conrad D and Aimone, James Bradley},
  year = {2017},
  url = {https://www.osti.gov/biblio/1424868},
  abstract = {Neural machine learning methods, such as deep neural networks (DNN), have achieved remarkable success in a number of complex data processing tasks. These methods have arguably had their strongest impact on tasks such as image and audio processing - data processing domains in which humans have long held clear advantages over conventional algorithms. In contrast to biological neural systems, which are capable of learning continuously, deep artificial networks have a limited ability for incorporating new information in an already trained network. As a result, methods for continuous learning are potentially highly impactful in enabling the application of deep networks to dynamic data sets. Here, inspired by the process of adult neurogenesis in the hippocampus, we explore the potential for adding new neurons to deep layers of artificial neural networks in order to facilitate their acquisition of novel information while preserving previously trained data representations. Our results on the MNIST handwritten digit dataset and the NIST SD 19 dataset, which includes lower and upper case letters and digits, demonstrate that neurogenesis is well suited for addressing the stability-plasticity dilemma that has long challenged adaptive machine learning algorithms.},
  langid = {english},
  keywords = {[mnist],/unread,\#nosource,⛔ No DOI found,autoencoder,autoencoders,neurogenesis,reconstruction},
  note = {The neurogenesis algorithm selectively expand the original multi-layer autoencoder at the neuron level depending on its reconstruction performance measured at each layer. The model is capable of maintaining plasticity while mitigating forgetting through replay of old samples.}
}

@article{du2019,
  title = {Single-{{Net Continual Learning}} with {{Progressive Segmented Training}} ({{PST}})},
  author = {Du, Xiaocong and Charan, Gouranga and Liu, Frank and Cao, Yu},
  year = {2019},
  journal = {arXiv},
  number = {2},
  pages = {1629--1636},
  doi = {10/gnq328},
  url = {http://arxiv.org/abs/1905.11550},
  abstract = {There is an increasing need of continual learning in dynamic systems, such as the self-driving vehicle, the surveillance drone, and the robotic system. Such a system requires learning from the data stream, training the model to preserve previous information and adapt to a new task, and generating a single-headed vector for future inference. Different from previous approaches with dynamic structures, this work focuses on a single network and model segmentation to prevent catastrophic forgetting. Leveraging the redundant capacity of a single network, model parameters for each task are separated into two groups: one important group which is frozen to preserve current knowledge, and secondary group to be saved (not pruned) for a future learning. A fixed-size memory containing a small amount of previously seen data is further adopted to assist the training. Without additional regularization, the simple yet effective approach of PST successfully incorporates multiple tasks and achieves the state-of-the-art accuracy in the single-head evaluation on CIFAR-10 and CIFAR-100 datasets. Moreover, the segmented training significantly improves computation efficiency in continual learning.},
  isbn = {9781728145501},
  keywords = {[cifar],/unread,\#nosource},
  annotation = {\_eprint: 1905.11550}
}

@inproceedings{duncker2020,
  title = {Organizing Recurrent Network Dynamics by Task-Computation to Enable Continual Learning},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Duncker, Lea and Driscoll, Laura N and Shenoy, Krishna V and Sahani, Maneesh and Sussillo, David},
  year = {2020},
  volume = {33},
  url = {https://proceedings.neurips.cc/paper/2020/file/a576eafbce762079f7d1f77fca1c5cc2-Paper.pdf},
  abstract = {Biological systems face dynamic environments that require continual learning. It is not well understood how these systems balance the tension between flexibility for learning and robustness for memory of previous behaviors. Continual learning without catastrophic interference also remains a challenging problem in machine learning. Here, we develop a novel learning rule designed to minimize interference between sequentially learned tasks in recurrent networks. Our learning rule preserves network dynamics within activity-defined subspaces used for previously learned tasks. It encourages dynamics associated with new tasks that might otherwise interfere to instead explore orthogonal subspaces, and it allows for reuse of previously established dynamical motifs where possible. Employing a set of tasks used in neuroscience, we demonstrate that our approach successfully eliminates catastrophic interference and offers a substantial improvement over previous continual learning algorithms. Using dynamical systems analysis, we show that networks trained using our approach can reuse similar dynamical structures across similar tasks. This possibility for shared computation allows for faster learning during sequential training. Finally, we identify organizational differences that emerge when training tasks sequentially versus simultaneously.},
  keywords = {[rnn],/unread,\#nosource,⛔ No DOI found},
  note = {The hidden state pre-activation and the input are projected during learning in a subspace orthogonal to all the ones of the previous tasks, if new task is dissimilar. Projection on orthogonal subspace avoid interference with previous knowledge.}
}

@article{early2019,
  title = {Reducing Catastrophic Forgetting When Evolving Neural Networks},
  author = {Early, Joseph},
  year = {2019},
  journal = {arXiv},
  url = {http://arxiv.org/abs/1904.03178},
  abstract = {A key stepping stone in the development of an artificial general intelligence (a machine that can perform any task), is the production of agents that can perform multiple tasks at once instead of just one. Unfortunately, canonical methods are very prone to catastrophic forgetting (CF) - the act of overwriting previous knowledge about a task when learning a new task. Recent efforts have developed techniques for overcoming CF in learning systems, but no attempt has been made to apply these new techniques to evolutionary systems. This research presents a novel technique, weight protection, for reducing CF in evolutionary systems by adapting a method from learning systems. It is used in conjunction with other evolutionary approaches for overcoming CF and is shown to be effective at alleviating CF when applied to a suite of reinforcement learning tasks. It is speculated that this work could indicate the potential for a wider application of existing learning-based approaches to evolutionary systems and that evolutionary techniques may be competitive with or better than learning systems when it comes to reducing CF.},
  keywords = {/unread,\#nosource,⛔ No DOI found},
  annotation = {\_eprint: 1904.03178}
}

@inproceedings{ebrahimi2020,
  title = {Uncertainty-Guided {{Continual Learning}} with {{Bayesian Neural Networks}}},
  booktitle = {{{ICLR}}},
  author = {Ebrahimi, Sayna and Elhoseiny, Mohamed and Darrell, Trevor and Rohrbach, Marcus},
  year = {2020},
  url = {https://openreview.net/pdf?id=HklUCCVKDB},
  abstract = {Continual learning aims to learn new tasks without forgetting previously learned ones. This is especially challenging when one cannot access data from previous tasks and when the model has a fixed capacity. Current regularization-based continual learning algorithms need an external representation and extra computation to measure the parameters' importance. In contrast, we propose Uncertainty-guided Continual Bayesian Neural Networks (UCB), where the learning rate adapts according to the uncertainty defined in the probability distribution of the weights in networks. Uncertainty is a natural way to identify what to remember and what to change as we continually learn, and thus mitigate catastrophic forgetting. We also show a variant of our model, which uses uncertainty for weight pruning and retains task performance after pruning by saving binary masks per tasks. We evaluate our UCB approach extensively on diverse object classification datasets with short and long sequences of tasks and report superior or on-par performance compared to existing approaches. Additionally, we show that our model does not necessarily need task information at test time, i.e. it does not presume knowledge of which task a sample belongs to.},
  keywords = {[bayes],[cifar],[fashion],[mnist],/unread,\#nosource,⛔ No DOI found,capacity,catastrophic forgetting,continual learning,learning rate,pruning,regularization,uncertainty}
}

@inproceedings{ehret2020,
  title = {Continual Learning in Recurrent Neural Networks},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Ehret, Benjamin and Henning, Christian and Cervera, Maria and Meulemans, Alexander and Oswald, Johannes Von and Grewe, Benjamin F.},
  year = {2020},
  url = {https://openreview.net/forum?id=8xeBUgD8u9},
  urldate = {2021-01-16},
  abstract = {While a diverse collection of continual learning (CL) methods has been proposed to prevent catastrophic forgetting, a thorough investigation of their effectiveness for processing sequential data...},
  langid = {english},
  keywords = {[audio],[rnn],/unread,\#nosource}
}

@incollection{elkhatib2019,
  title = {Strategies for {{Improving Single-Head Continual Learning Performance}}},
  booktitle = {Lecture {{Notes}} in {{Computer Science}} (Including Subseries {{Lecture Notes}} in {{Artificial Intelligence}} and {{Lecture Notes}} in {{Bioinformatics}})},
  author = {El Khatib, Alaa and Karray, Fakhri},
  year = {2019},
  volume = {11662 LNCS},
  pages = {452--460},
  publisher = {{Springer Verlag}},
  issn = {16113349},
  doi = {10.1007/978-3-030-27202-9_41},
  url = {http://link.springer.com/10.1007/978-3-030-27202-9_41},
  abstract = {Catastrophic forgetting has long been seen as the main obstacle to building continual learning models. We argue in this paper that an equally challenging characteristic of the continual learning framework is that data are never completely available at the same time, making it difficult to learn joint conditional distributions over them. This is most evident in the usually large gap between single-head and multi-head performance of continual learning models. We propose in this paper two strategies to improve performance of continual learning models, particularly in the single-head framework and for image classification tasks. First, we argue that learning multiple binary classifiers, rather than a single multi-class classifier, for each presentation of data is more consistent with the single-head framework. Moreover, we argue that auxiliary, unlabelled data can be used in tandem with this approach to slow the decay in performance of these binary classifiers over time.},
  isbn = {978-3-030-27201-2},
  keywords = {[cifar],[mnist],/unread,\#nosource,Catastrophic forgetting,Continual learning}
}

@inproceedings{farquhar2018,
  title = {A {{Unifying Bayesian View}} of {{Continual Learning}}},
  booktitle = {{{NeurIPS Bayesian Deep Learning Workshop}}},
  author = {Farquhar, Sebastian and Gal, Yarin},
  year = {2018},
  url = {http://bayesiandeeplearning.org/2018/papers/74.pdf},
  keywords = {[bayes],[cifar],[mnist],/unread,\#nosource,⛔ No DOI found},
  note = {The authors distinguish between prior-focused and likelihood-focused bayesian methods and then combine the two. They carefully analyze the difference between multi-head and single-head approach.}
}

@inproceedings{farquhar2019,
  title = {Towards {{Robust Evaluations}} of {{Continual Learning}}},
  booktitle = {Privacy in {{Machine Learning}} and {{Artificial Intelligence}} Workshop, {{ICML}}},
  author = {Farquhar, Sebastian and Gal, Yarin},
  year = {2019},
  url = {http://arxiv.org/abs/1805.09733},
  abstract = {Experiments used in current continual learning research do not faithfully assess fundamental challenges of learning continually. Instead of assessing performance on challenging and representative experiment designs, recent research has focused on increased dataset difficulty, while still using flawed experiment set-ups. We examine standard evaluations and show why these evaluations make some continual learning approaches look better than they are. We introduce desiderata for continual learning evaluations and explain why their absence creates misleading comparisons. Based on our desiderata we then propose new experiment designs which we demonstrate with various continual learning approaches and datasets. Our analysis calls for a reprioritization of research effort by the community.},
  keywords = {[fashion],[framework],/unread,\#nosource,Computer Science - Machine Learning,critique,evaluation,metrics,Statistics - Machine Learning},
  note = {arXiv: 1805.09733}
}

@phdthesis{fayek2019,
  title = {Continual {{Deep Learning}} via {{Progressive Learning}}},
  author = {Fayek, Haytham M.},
  year = {2019},
  journal = {RMIT University},
  url = {http://researchbank.rmit.edu.au/eserv/rmit:162646/Fayek.pdf},
  abstract = {Machine learning is one of several approaches to artificial intelligence. It allows us to build machines that can learn from experience as opposed to being explicitly programmed. Current machine learning formulations are mostly designed for learning and performing a particular task from a tabula rasa using data available for that task. For machine learning to converge to artificial intelligence, in addition to other desiderata, it must be in a state of continual learning, i.e., have the ability to be in a continuous learning process, such that when a new task is presented, the system can leverage prior knowledge from prior tasks, in learning and performing this new task, and augment the prior knowledge with the newly acquired knowledge without having a significant adverse effect on the prior knowledge. Continual learning is key to advancing machine learning and artificial intelligence. Deep learning is a powerful general-purpose approach to machine learning that is able to solve numerous and various tasks with minimal modification. Deep learning extends machine learning, and specially neural networks, to learn multiple levels of distributed representations together with the required mapping function into a single composite function. The emergence of deep learning and neural networks as a generic approach to machine learning, coupled with their ability to learn versatile hierarchical representations, has paved the way for continual learning. The main aim of this thesis is the study and development of a structured approach to continual learning, leveraging the success of deep learning and neural networks. This thesis studies the application of deep learning to a number of supervised learning tasks, and in particular, classification tasks in machine perception, e.g., image recognition, automatic speech recognition, and speech emotion recognition. The relation between the systems developed for these tasks is investigated to illuminate the layer-wise relevance of features in deep networks trained for these tasks via transfer learning, and these independent systems are unified into continual learning systems. The main contribution of this thesis is the construction and formulation of a deep learning framework, denoted progressive learning, that allows a holistic and systematic approach to continual learning. Progressive learning comprises a number of procedures that address the continual learning desiderata. It is shown that, when tasks are related, progressive learning leads to faster learning that converges to better generalization performance using less amounts of data and a smaller number of dedicated parameters, for the tasks studied in this thesis, by accumulating and leveraging knowledge learned across tasks in a continuous manner. It is envisioned that progressive learning is a step towards a fully general continual learning framework.},
  school = {RMIT University},
  keywords = {[audio],[cifar],[imagenet],[sparsity],/unread,\#nosource}
}

@inproceedings{finn2019,
  title = {Online {{Meta-Learning}}},
  booktitle = {{{ICML}}},
  author = {Finn, Chelsea and Rajeswaran, Aravind and Kakade, Sham and Levine, Sergey},
  year = {2019},
  url = {http://proceedings.mlr.press/v97/finn19a/finn19a.pdf},
  abstract = {A central capability of intelligent systems is the ability to continuously build upon previous experiences to speed up and enhance learning of new tasks. Two distinct research paradigms have studied this question. Meta-learning views this problem as learning a prior over model parameters that is amenable for fast adaptation on a new task, but typically assumes the tasks are available together as a batch. In contrast, online (regret based) learning considers a setting where tasks are revealed one after the other, but conventionally trains a single model without task-specific adaptation. This work introduces an online meta-learning setting, which merges ideas from both paradigms to better capture the spirit and practice of continual lifelong learning. We propose the follow the meta leader (FTML) algorithm which extends the MAML algorithm to this setting. Theoretically, this work provides an O(log T) regret guarantee with one additional higher order smoothness assumption (in comparison to the standard online setting). Our experimental evaluation on three different large-scale problems suggest that the proposed algorithm significantly outperforms alternatives based on traditional online learning approaches.},
  keywords = {[experimental],[mnist],/unread,\#nosource,⛔ No DOI found},
  note = {This paper focuses on meta learning a stream of tasks. As for most of the online learning literature the focus is not on catastrophic forgetting, which is not taken into consideration, but on forward / backward transfer and few-shot learning. It stores a replay buffer for each task in order to meta-optimize in the outer loop.}
}

@inproceedings{french1991,
  title = {Using {{Semi-Distributed Representations}} to {{Overcome Catastrophic Forgetting}} in {{Connectionist Networks}}},
  booktitle = {In {{Proceedings}} of the 13th {{Annual Cognitive Science Society Conference}}},
  author = {French, Robert},
  year = {1991},
  pages = {173--178},
  publisher = {{Erlbaum}},
  url = {https://www.aaai.org/Papers/Symposia/Spring/1993/SS-93-06/SS93-06-007.pdf},
  abstract = {In connectionist networks, newly-learned information destroys previously-learned information unless the network is continually retrained on the old information. This behavior, known as catastrophic forgetting, is unacceptable both for practical purposes and as a model of mind. This paper advances the claim that catastrophic forgetting is a direct consequence of the overlap of the system's distributed representations and can be reduced by reducing this overlap. A simple algorithm is presented that allows a standard feedforward backpropagation network to develop semi-distributed representations, thereby significantly reducing the problem of catastrophic forgetting. 1 Introduction Catastrophic forgetting is the inability of a neural network to retain old information in the presence of new. New information destroys old unless the old information is continually relearned by the net. McCloskey \& Cohen [1990] and Ratcliff [1989] have demonstrated that this is a serious problem with c...},
  keywords = {[sparsity],/unread,\#nosource,⛔ No DOI found,activation sharpening}
}

@inproceedings{french1997,
  title = {Using {{Pseudo-Recurrent Connectionist Networks}} to {{Solve}} the {{Problem}} of {{Sequential Learning}}},
  booktitle = {Proceedings of the 19th {{Annual Cognitive Science Society Conference}}},
  author = {French, Robert},
  year = {1997},
  url = {http://leadserv.u-bourgogne.fr/IMG/pdf/psdnet1.pdf},
  abstract = {A major problem facing connectionist models of memory is  how to make them simultaneously sensitive to, but not  disrupted by, new input. This paper describes one solution  to this problem. The resulting connectionist architecture is  capable of sequential learning and exhibits gradual  forgetting where standard connectionist architectures may  forget catastrophically. The proposed architecture relies on  separating previously learned representations from those  that are currently being learned. Crucially, a method is  described in which approximations of the previously  learned data, called pseudopatterns, will be extracted from  the network and mixed in with the new patterns to be  learned, thereby alleviating sudden forgetting caused by  new learning and allowing the network to forget gracefully.  Introduction  One of the most important problems facing connectionist models of memory --- in fact, facing any model of memory --- is how to make them simultaneously sensitive to, but not ...},
  keywords = {[dual],/unread,\#nosource,⛔ No DOI found}
}

@article{french1997a,
  title = {Pseudo-Recurrent {{Connectionist Networks}}: {{An Approach}} to the '{{Sensitivity-Stability}}' {{Dilemma}}},
  shorttitle = {Pseudo-Recurrent {{Connectionist Networks}}},
  author = {French, Robert},
  year = {1997},
  journal = {Connection Science},
  volume = {9},
  number = {4},
  pages = {353--380},
  issn = {0954-0091, 1360-0494},
  doi = {10/bfc942},
  url = {http://www.tandfonline.com/doi/abs/10.1080/095400997116595},
  abstract = {In order to solve the ``sensitivity-stability'' problem \textemdash{} and its immediate correlate, the problem of sequential learning \textemdash{} it is crucial to develop connectionist architectures that are simultaneously sensitive to, but not excessively disrupted by, new input. French (1992) suggested that to alleviate a particularly severe form of this disruption, catastrophic forgetting, it was necessary for networks to dynamically separate their internal representations during learning. McClelland, McNaughton, \& O'Reilly (1995) went even further. They suggested that nature's way of implementing this obligatory separation was the evolution of two separate areas of the brain, the hippocampus and the neocortex. In keeping with this idea of radical separation, a ``pseudo-recurrent'' memory model is presented here that partitions a connectionist network into two functionally distinct, but continually interacting areas. One area serves as a final-storage area for representations; the other is an early-processing area where new representations are first learned by the system. The final-storage area continually supplies internally generated patterns (pseudopatterns, Robins (1995)), which are approximations of its content, to the early-processing area, where they are interleaved with the new patterns to be learned. Transfer of the new learning is done either by weight-copying from the early-processing area to the final-storage area or by pseudopattern transfer. A number of experiments are presented that demonstrate the effectiveness of this approach, allowing, in particular, effective sequential learning with gradual forgetting in the presence of new input. Finally, it is shown that the two interacting areas automatically produce representational compaction and it is suggested that similar representational streamlining may exist in the brain.},
  langid = {english},
  keywords = {[dual],/unread,\#nosource,Catastrophic Interference,dilemma,Dual Memory,Keywords: Pseudopatterns,plasticity,Semi-distributed Representations,Sensitivity-stability Transfer,stability},
  note = {In this seminal paper the author introduces many different forms of rehearsal in order to mitigate the catastrophic forgetting phenomenon}
}

@article{french1999,
  title = {Catastrophic Forgetting in Connectionist Networks},
  author = {French, Robert},
  year = {1999},
  journal = {Trends in Cognitive Sciences},
  volume = {3},
  number = {4},
  pages = {128--135},
  issn = {1364-6613, 1879-307X},
  doi = {10/cdxp7n},
  url = {https://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(99)01294-2},
  langid = {english},
  pmid = {10322466},
  keywords = {[sparsity],/unread,\#nosource,biology,Catastrophic forgetting,Connectionism,Connectionist networks,Interference,Learning,Memory,Neuroscience}
}

@incollection{french2019,
  title = {Dynamically Constraining Connectionist Networks to Produce Distributed, Orthogonal Representations to Reduce Catastrophic Interference},
  booktitle = {Proceedings of the {{Sixteenth Annual Conference}} of the {{Cognitive Science Society}}},
  author = {French, Robert},
  editor = {Ram, Ashwin and Eiselt, Kurt},
  year = {2019},
  edition = {First},
  pages = {335--340},
  publisher = {{Routledge}},
  doi = {10.4324/9781315789354-58},
  url = {https://www.taylorfrancis.com/books/9781317729266/chapters/10.4324/9781315789354-58},
  abstract = {It is well known that when a connectionist network is trained on one set of patterns and then attempts to add new patterns to its repertoire, catastrophic interference may result. The use of sparse, orthogonal hidden-layer representations has been shown to reduce catastrophic interference. The author demonstrates that the use of sparse representations may, in certain cases, actually result in worse performance on catastrophic interference. This paper argues for the necessity of maintaining hidden-layer representations that are both as highly distributed and as highly orthogonal as possible. The author presents a learning algorithm, called context-biasing, that dynamically solves the problem of constraining hiddenlayer representations to simultaneously produce good orthogonality and distributedness. On the data tested for this study, context-biasing is shown to reduce catastrophic interference by more than 50\% compared to standard backpropagation. In particular, this technique succeeds in reducing catastrophic interference on data where sparse, orthogonal distributions failed to produce any improvement.},
  isbn = {978-1-315-78935-4},
  langid = {english},
  keywords = {/unread,\#nosource,sparsity}
}

@article{fu2020,
  title = {Incremental {{Learning}} for {{End-to-End Automatic Speech Recognition}}},
  author = {Fu, Li and Li, Xiaoxiao and Zi, Libo},
  year = {2020},
  journal = {arXiv},
  url = {https://arxiv.org/abs/2005.04288},
  abstract = {\textdagger{} We propose an incremental learning for end-to-end Automatic Speech Recognition (ASR) to extend the model's capacity on a new task while retaining the performance on existing ones. The proposed method is effective without accessing to the old dataset to address the issues of high training cost and old dataset unavailability. To achieve this, knowledge distillation is applied as a guidance to retain the recognition ability from the previous model, which is then combined with the new ASR task for model optimization. With an ASR model pre-trained on 12,000h Mandarin speech, we test our proposed method on 300h new scenario task and 1h new named entities task. Experiments show that our method yields 3.25\% and 0.88\% absolute Character Error Rate (CER) reduction on the new scenario, when compared with the pre-trained model and the full-data retraining baseline, respectively. It even yields a surprising 0.37\% absolute CER reduction on the new scenario than the fine-tuning. For the new named entities task, our method significantly improves the accuracy compared with the pre-trained model, i.e. 16.95\% absolute CER reduction. For both of the new task adaptions, the new models still maintain a same accuracy with the baseline on the old tasks.},
  keywords = {[audio],/unread,\#nosource,⛔ No DOI found,end-to-end,incremental learning,Index Terms: automatic speech recognition,knowledge distillation}
}

@inproceedings{garcia2019,
  title = {A {{Meta-MDP Approach}} to {{Exploration}} for {{Lifelong Reinforcement Learning}}},
  booktitle = {{{NeurIPS}}},
  author = {Garcia, Francisco M and Thomas, Philip S},
  year = {2019},
  pages = {5691--5700},
  url = {https://papers.nips.cc/paper/8806-a-meta-mdp-approach-to-exploration-for-lifelong-reinforcement-learning.pdf},
  abstract = {In this paper we consider the problem of how a reinforcement learning agent that is tasked with solving a sequence of reinforcement learning problems (a sequence of Markov decision processes) can use knowledge acquired early in its lifetime to improve its ability to solve new problems. We argue that previous experience with similar problems can provide an agent with information about how it should explore when facing a new but related problem. We show that the search for an optimal exploration strategy can be formulated as a reinforcement learning problem itself and demonstrate that such strategy can leverage patterns found in the structure of related problems. We conclude with experiments that show the benefits of optimizing an exploration strategy using our proposed framework.},
  keywords = {/unread,\#nosource,⛔ No DOI found}
}

@article{garg2017,
  title = {Neurogenesis-Inspired Dictionary Learning: {{Online}} Model Adaption in a Changing World},
  author = {Garg, Sahil and Rish, Irina and Cecchi, Guillermo and Lozano, Aurelie},
  year = {2017},
  journal = {IJCAI International Joint Conference on Artificial Intelligence},
  pages = {1696--1702},
  issn = {10450823},
  doi = {10/gnq324},
  url = {https://arxiv.org/abs/1701.06106},
  abstract = {We address the problem of online model adaptation when learning representations from non-stationary data streams. Specifically, we focus here on online dictionary learning (i.e. sparse linear autoencoder), and propose a simple but effective online modelselection approach involving "birth" (addition) and "death" (removal) of hidden units representing dictionary elements, in response to changing inputs; we draw inspiration from the adult neurogenesis phenomenon in the dentate gyrus of the hippocampus, known to be associated with better adaptation to new environments. Empirical evaluation on real-life datasets (images and text), as well as on synthetic data, demonstrates that the proposed approach can considerably outperform the state-of-art non-adaptive online sparse coding of [Mairal et al., 2009] in the presence of non-stationary data. Moreover, we identify certain data- and model properties associated with such improvements.},
  isbn = {9780999241103},
  keywords = {[nlp],[vision],/unread,\#nosource},
  annotation = {\_eprint: 1701.06106}
}

@inproceedings{gidaris2018,
  title = {Dynamic {{Few-Shot Visual Learning Without Forgetting}}},
  booktitle = {Proceedings of the {{IEEE Computer Society Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Gidaris, Spyros and Komodakis, Nikos},
  year = {2018},
  pages = {4367--4375},
  issn = {10636919},
  doi = {10/gfx43x},
  url = {http://openaccess.thecvf.com/content_cvpr_2018/html/Gidaris_Dynamic_Few-Shot_Visual_CVPR_2018_paper.html},
  abstract = {The human visual system has the remarkably ability to be able to effortlessly learn novel concepts from only a few examples. Mimicking the same behavior on machine learning vision systems is an interesting and very challenging research problem with many practical advantages on real world vision applications. In this context, the goal of our work is to devise a few-shot visual learning system that during test time it will be able to efficiently learn novel categories from only a few training data while at the same time it will not forget the initial categories on which it was trained (here called base categories). To achieve that goal we propose (a) to extend an object recognition system with an attention based few-shot classification weight generator, and (b) to redesign the classifier of a ConvNet model as the cosine similarity function between feature representations and classification weight vectors. The latter, apart from unifying the recognition of both novel and base categories, it also leads to feature representations that generalize better on 'unseen' categories. We extensively evaluate our approach on Mini-ImageNet where we manage to improve the prior state-of-the-art on few-shot recognition (i.e., we achieve 56.20\% and 73.00\% on the 1-shot and 5-shot settings respectively) while at the same time we do not sacrifice any accuracy on the base categories, which is a characteristic that most prior approaches lack. Finally, we apply our approach on the recently introduced few-shot benchmark of Bharath and Girshick [4] where we also achieve state-of-the-art results.},
  isbn = {978-1-5386-6420-9},
  keywords = {[imagenet],[vision],/unread,\#nosource},
  annotation = {\_eprint: 1804.09458}
}

@article{golkar2019,
  title = {Continual {{Learning}} via {{Neural Pruning}}},
  author = {Golkar, Siavash and Kagan, Michael and Cho, Kyunghyun},
  year = {2019},
  journal = {arXiv},
  url = {http://arxiv.org/abs/1903.04476},
  abstract = {We introduce Continual Learning via Neural Pruning (CLNP), a new method aimed at lifelong learning in fixed capacity models based on neuronal model sparsification. In this method, subsequent tasks are trained using the inactive neurons and filters of the sparsified network and cause zero deterioration to the performance of previous tasks. In order to deal with the possible compromise between model sparsity and performance, we formalize and incorporate the concept of graceful forgetting: the idea that it is preferable to suffer a small amount of forgetting in a controlled manner if it helps regain network capacity and prevents uncontrolled loss of performance during the training of future tasks. CLNP also provides simple continual learning diagnostic tools in terms of the number of free neurons left for the training of future tasks as well as the number of neurons that are being reused. In particular, we see in experiments that CLNP verifies and automatically takes advantage of the fact that the features of earlier layers are more transferable. We show empirically that CLNP leads to significantly improved results over current weight elasticity based methods.},
  keywords = {[cifar],[mnist],[sparsity],/unread,\#nosource,⛔ No DOI found,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computi,Quantitative Biology - Neurons and Cognition,Statistics - Machine Learning},
  note = {Comment: 12 pages, 5 figures, 3 tables arXiv: 1903.04476}
}

@article{gonzalez2019,
  title = {Can Sleep Protect Memories from Catastrophic Forgetting?},
  author = {Gonzalez, Oscar C and Sokolov, Yury and Krishnan, Giri and Bazhenov, Maxim},
  year = {2019},
  journal = {bioRxiv},
  pages = {569038},
  doi = {10/gkt74s},
  url = {https://www.biorxiv.org/content/10.1101/569038v1},
  abstract = {Previously encoded memories can be damaged by encoding of new memories, especially when they are relevant to the new data and hence can be disrupted by new training - a phenomenon called "catastrophic forgetting." Human and animal brains are capable of continual learning, allowing them to learn from past experience and to integrate newly acquired information with previously stored memories. A range of empirical data suggest important role of sleep in consolidation of recent memories and protection of the past knowledge from catastrophic forgetting. To explore potential mechanisms of how sleep can enable continual learning in neuronal networks, we developed a biophysically-realistic thalamocortical network model where we could train multiple memories with different degree of interference. We found that in a wake-like state of the model, training of a "new" memory that overlaps with previously stored "old" memory results in degradation of the old memory. Simulating NREM sleep state immediately after new learning led to replay of both old and new memories - this protected old memory from forgetting and ultimately enhanced both memories. The effect of sleep was similar to the interleaved training of the old and new memories. The study revealed that the network slow-wave oscillatory activity during simulated deep sleep leads to a complex reorganization of the synaptic connectivity matrix that maximizes separation between groups of synapses responsible for conflicting memories in the overlapping population of neurons. The study predicts that sleep may play a protective role against catastrophic forgetting and enables brain networks to undergo continual learning.},
  keywords = {/unread,\#nosource,catastrophic,continual learning,memory consolidation,neural network,sleep}
}

@article{goodfellow2015,
  title = {An {{Empirical Investigation}} of {{Catastrophic Forgetting}} in {{Gradient-Based Neural Networks}}},
  author = {Goodfellow, Ian J. and Mirza, Mehdi and Xiao, Da and Courville, Aaron and Bengio, Yoshua},
  year = {2015},
  journal = {arXiv},
  eprint = {1312.6211},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1312.6211},
  urldate = {2022-03-28},
  abstract = {Catastrophic forgetting is a problem faced by many machine learning models and algorithms. When trained on one task, then trained on a second task, many machine learning models "forget" how to perform the first task. This is widely believed to be a serious problem for neural networks. Here, we investigate the extent to which the catastrophic forgetting problem occurs for modern neural networks, comparing both established and recent gradient-based training algorithms and activation functions. We also examine the effect of the relationship between the first task and the second task on catastrophic forgetting. We find that it is always best to train using the dropout algorithm--the dropout algorithm is consistently best at adapting to the new task, remembering the old task, and has the best tradeoff curve between these two extremes. We find that different tasks and relationships between tasks result in very different rankings of activation function performance. This suggests the choice of activation function should always be cross-validated.},
  archiveprefix = {arXiv},
  keywords = {/unread,⛔ No DOI found,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning}
}

@article{grossberg1980,
  title = {How Does a Brain Build a Cognitive Code?},
  author = {Grossberg, Stephen},
  year = {1980},
  journal = {Psychological Review},
  volume = {87},
  number = {1},
  pages = {1--51},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-1471(Electronic),0033-295X(Print)},
  doi = {10/dwdph6},
  url = {https://psycnet.apa.org/record/1980-06768-001},
  abstract = {Discusses how competition between afferent data and learned feedback expectancies can stabilize a developing code by buffering committed populations of detectors against continual erosion by new environmental demands. The gating phenomena that result lead to dynamically maintained critical periods and to attentional phenomena such as overshadowing in the adult. The functional unit of cognitive coding is suggested to be an adaptive resonance, or amplification and prolongation of neural activity, that occurs when afferent data and efferent expectancies reach consensus through a matching process. The resonant state embodies the perceptual event, and its amplified and sustained activities are capable of driving slow changes of long-term memory. These mechanisms help to explain and predict (a) positive and negative aftereffects, the McCollough effect, spatial frequency adaptation, monocular rivalry, binocular rivalry and hysteresis, pattern completion, and Gestalt switching; (b) analgesia, partial reinforcement acquisition effect, conditioned reinforcers, underaroused vs overaroused depression; (c) the contingent negative variation, P300, and pontogeniculo-occipital waves; and (d) olfactory coding, corticogeniculate feedback, matching of proprioceptive and terminal motor maps, and cerebral dominance. (125 ref) (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {/unread,\#nosource,Attention,Cognitive Processes,Electrical Activity,Expectations,Human Information Storage,Neurophysiology,stability-plasticity}
}

@article{gupta2020,
  title = {La-{{MAML}}: {{Look-ahead Meta Learning}} for {{Continual Learning}}},
  author = {Gupta, Gunshi and Yadav, Karmesh and Paull, Liam},
  year = {2020},
  journal = {arXiv},
  url = {https://arxiv.org/abs/2007.13904},
  abstract = {The continual learning problem involves training models with limited capacity to perform well on a set of an unknown number of sequentially arriving tasks. While meta-learning shows great potential for reducing interference between old and new tasks, the current training procedures tend to be either slow or offline, and sensitive to many hyper-parameters. In this work, we propose Look-ahead MAML (La-MAML), a fast optimisation-based meta-learning algorithm for online-continual learning, aided by a small episodic memory. Our proposed modulation of per-parameter learning rates in our meta-learning update allows us to draw connections to prior work on hypergradients and meta-descent. This provides a more flexible and efficient way to mitigate catastrophic forgetting compared to conventional prior-based methods. La-MAML achieves performance superior to other replay-based, prior-based and meta-learning based approaches for continual learning on real-world visual classification benchmarks.},
  keywords = {/unread,\#nosource,⛔ No DOI found},
  annotation = {\_eprint: 2007.13904}
}

@inproceedings{gupta2020a,
  title = {Neural {{Topic Modeling}} with {{Continual Lifelong Learning}}},
  booktitle = {{{ICML}}},
  author = {Gupta, Pankaj and Chaudhary, Yatin and Runkler, Thomas and Sch{\"u}tze, Hinrich},
  year = {2020},
  url = {http://arxiv.org/abs/2006.10909},
  abstract = {Lifelong learning has recently attracted attention in building machine learning systems that continually accumulate and transfer knowledge to help future learning. Unsupervised topic modeling has been popularly used to discover topics from document collections. However, the application of topic modeling is challenging due to data sparsity, e.g., in a small collection of (short) documents and thus, generate incoherent topics and sub-optimal document representations. To address the problem, we propose a lifelong learning framework for neural topic modeling that can continuously process streams of document collections, accumulate topics and guide future topic modeling tasks by knowledge transfer from several sources to better deal with the sparse data. In the lifelong process, we particularly investigate jointly: (1) sharing generative homologies (latent topics) over lifetime to transfer prior knowledge, and (2) minimizing catastrophic forgetting to retain the past learning via novel selective data augmentation, co-training and topic regularization approaches. Given a stream of document collections, we apply the proposed Lifelong Neural Topic Modeling (LNTM) framework in modeling three sparse document collections as future tasks and demonstrate improved performance quantified by perplexity, topic coherence and information retrieval task.},
  keywords = {[nlp],/unread,\#nosource},
  annotation = {\_eprint: 2006.10909}
}

@article{hadsell2020,
  title = {Embracing {{Change}}: {{Continual Learning}} in {{Deep Neural Networks}}},
  author = {Hadsell, Raia and Rao, Dushyant and Rusu, Andrei A and Pascanu, Razvan},
  year = {2020},
  journal = {Trends in Cognitive Sciences},
  issn = {13646613},
  doi = {10/ghk2ds},
  url = {https://doi.org/10.1016/j.tics.2020.09.004},
  abstract = {Artificial intelligence research has seen enormous progress over the past few decades, but it predominantly relies on fixed datasets and stationary environments. Continual learning is an increasingly relevant area of study that asks how artificial systems might learn sequentially, as biological systems do, from a continuous stream of correlated data. In the present review, we relate continual learning to the learning dynamics of neural networks, highlighting the potential it has to considerably improve data efficiency. We further consider the many new biologically inspired approaches that have emerged in recent years, focusing on those that utilize regularization, modularity, memory, and meta-learning, and highlight some of the most promising and impactful directions.},
  keywords = {/unread,\#nosource,artificial intelligence,lifelong,memory,meta-learning,non-stationary},
  note = {A CL review which highlights the problem of a tug-of-war dynamics in gradient-based learning and credit assignment. It also points to meta-learning as a valuable solution to continually learn while being data efficient.}
}

@article{han2021,
  title = {Contrastive {{Continual Learning}} with {{Feature Propagation}}},
  author = {Han, Xuejun and Guo, Yuhong},
  year = {2021},
  journal = {arXiv:2112.01713 [cs]},
  eprint = {2112.01713},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2112.01713},
  urldate = {2022-05-03},
  abstract = {Classical machine learners are designed only to tackle one task without capability of adopting new emerging tasks or classes whereas such capacity is more practical and human-like in the real world. To address this shortcoming, continual machine learners are elaborated to commendably learn a stream of tasks with domain and class shifts among different tasks. In this paper, we propose a general feature-propagation based contrastive continual learning method which is capable of handling multiple continual learning scenarios. Specifically, we align the current and previous representation spaces by means of feature propagation and contrastive representation learning to bridge the domain shifts among distinct tasks. To further mitigate the class-wise shifts of the feature representation, a supervised contrastive loss is exploited to make the example embeddings of the same class closer than those of different classes. The extensive experimental results demonstrate the outstanding performance of the proposed method on six continual learning benchmarks compared to a group of cutting-edge continual learning methods.},
  archiveprefix = {arXiv},
  keywords = {/unread,⛔ No DOI found,Computer Science - Machine Learning},
  annotation = {ZSCC:00000}
}

@article{harrison2019,
  title = {Continuous Meta-Learning without Tasks},
  author = {Harrison, James and Sharma, Apoorva and Finn, Chelsea and Pavone, Marco},
  year = {2019},
  journal = {arXiv},
  url = {https://arxiv.org/abs/1912.08866},
  abstract = {Meta-learning is a promising strategy for learning to efficiently learn within new tasks, using data gathered from a distribution of tasks. However, the meta-learning literature thus far has focused on the task segmented setting, where at train-time, offline data is assumed to be split according to the underlying task, and at test-time, the algorithms are optimized to learn in a single task. In this work, we enable the application of generic meta-learning algorithms to settings where this task segmentation is unavailable, such as continual online learning with a time-varying task. We present meta-learning via online changepoint analysis (MOCA), an approach which augments a meta-learning algorithm with a differentiable Bayesian changepoint detection scheme. The framework allows both training and testing directly on time series data without segmenting it into discrete tasks. We demonstrate the utility of this approach on a nonlinear meta-regression benchmark as well as two meta-image-classification benchmarks.},
  keywords = {[imagenet],[mnist],/unread,\#nosource,⛔ No DOI found},
  annotation = {\_eprint: 1912.08866}
}

@article{hasselmo2017,
  title = {Avoiding {{Catastrophic Forgetting}}},
  author = {Hasselmo, Michael E.},
  year = {2017},
  journal = {Trends in Cognitive Sciences},
  volume = {21},
  number = {6},
  pages = {407--408},
  publisher = {{Elsevier Ltd}},
  issn = {13646613},
  doi = {10/gkjs2n},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1364661317300736},
  abstract = {Humans regularly perform new learning without losing memory for previous information, but neural network models suffer from the phenomenon of catastrophic forgetting in which new learning impairs prior function. A recent article presents an algorithm that spares learning at synapses important for previously learned function, reducing catastrophic forgetting.},
  keywords = {/unread,\#nosource,hasselmo2017a}
}

@inproceedings{hawkins2019,
  title = {Continual Adaptation for Efficient Machine Communication},
  booktitle = {Proceedings of the {{ICML Workshop}} on {{Adaptive}} \& {{Multitask Learning}}: {{Algorithms}} \& {{Systems}}},
  author = {Hawkins, Robert D and Kwon, Minae and Sadigh, Dorsa and Goodman, Noah D},
  year = {2019},
  url = {http://arxiv.org/abs/1911.09896},
  abstract = {To communicate with new partners in new contexts, humans rapidly form new linguistic conventions. Recent language models trained with deep neural networks are able to comprehend and produce the existing conventions present in their training data, but are not able to flexibly and interactively adapt those conventions on the fly as humans do. We introduce a repeated reference task as a benchmark for models of adaptation in communication and propose a regularized continual learning framework that allows an artificial agent initialized with a generic language model to more accurately and efficiently communicate with a partner over time. We evaluate this framework through simulations on COCO and in real-time reference game experiments with human partners.},
  keywords = {/unread,\#nosource},
  annotation = {\_eprint: 1911.09896}
}

@inproceedings{hayes2018,
  title = {New {{Metrics}} and {{Experimental Paradigms}} for {{Continual Learning}}},
  booktitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})},
  author = {Hayes, Tyler L. and Kemker, Ronald and Cahill, Nathan D. and Kanan, Christopher},
  year = {2018},
  pages = {2112--21123},
  issn = {2160-7516},
  doi = {10/gfx42x},
  url = {https://ieeexplore.ieee.org/document/8575441},
  abstract = {In order for a robotic agent to learn successfully in an uncontrolled environment, it must be able to immediately alter its behavior. Deep neural networks are the dominant approach for classification tasks in computer vision, but typical algorithms and architectures are incapable of immediately learning new tasks without catastrophically forgetting previously acquired knowledge. There has been renewed interest in solving this problem, but there are limitations to existing solutions, including poor performance compared to offline models, large memory footprints, and learning slowly. In this Abstract, we formalize the continual learning paradigm and propose new benchmarks for assessing continual learning agents.},
  keywords = {/unread,Computational modeling,Data models,Measurement,Neural networks,Robots,Task analysis,Training}
}

@article{hayes2019,
  title = {Memory {{Efficient Experience Replay}} for {{Streaming Learning}}},
  author = {Hayes, Tyler L and Cahill, Nathan D and Kanan, Christopher},
  year = {2019},
  journal = {IEEE International Conference on Robotics and Automation (ICRA)},
  url = {https://github.com/tyler-hayes/ExStream. http://arxiv.org/abs/1809.05922},
  abstract = {In supervised machine learning, an agent is typically trained once and then deployed. While this works well for static settings, robots often operate in changing environments and must quickly learn new things from data streams. In this paradigm, known as streaming learning, a learner is trained online, in a single pass, from a data stream that cannot be assumed to be independent and identically distributed (iid). Streaming learning will cause conventional deep neural networks (DNNs) to fail for two reasons: 1) they need multiple passes through the entire dataset; and 2) non-iid data will cause catastrophic forgetting. An old fix to both of these issues is rehearsal. To learn a new example, rehearsal mixes it with previous examples, and then this mixture is used to update the DNN. Full rehearsal is slow and memory intensive because it stores all previously observed examples, and its effectiveness for preventing catastrophic forgetting has not been studied in modern DNNs. Here, we describe the ExStream algorithm for memory efficient rehearsal and compare it to alternatives. We find that full rehearsal can eliminate catastrophic forgetting in a variety of streaming learning settings, with ExStream performing well using far less memory and computation.},
  keywords = {[core50],/unread,\#nosource,⛔ No DOI found},
  annotation = {\_eprint: 1809.05922},
  note = {The replay memory is of very small size due to the online update of class-representative centroids.}
}

@inproceedings{hayes2020,
  title = {{{REMIND Your Neural Network}} to {{Prevent Catastrophic Forgetting}}},
  booktitle = {Proceedings of the 2020 {{ECCV}}},
  author = {Hayes, Tyler L. and Kafle, Kushal and Shrestha, Robik and Acharya, Manoj and Kanan, Christopher},
  year = {2020},
  eprint = {1910.02509},
  eprinttype = {arxiv},
  doi = {10/gm5898},
  url = {http://arxiv.org/abs/1910.02509},
  urldate = {2021-03-26},
  abstract = {People learn throughout life. However, incrementally updating conventional neural networks leads to catastrophic forgetting. A common remedy is replay, which is inspired by how the brain consolidates memory. Replay involves fine-tuning a network on a mixture of new and old instances. While there is neuroscientific evidence that the brain replays compressed memories, existing methods for convolutional networks replay raw images. Here, we propose REMIND, a brain-inspired approach that enables efficient replay with compressed representations. REMIND is trained in an online manner, meaning it learns one example at a time, which is closer to how humans learn. Under the same constraints, REMIND outperforms other methods for incremental class learning on the ImageNet ILSVRC-2012 dataset. We probe REMIND's robustness to data ordering schemes known to induce catastrophic forgetting. We demonstrate REMIND's generality by pioneering online learning for Visual Question Answering (VQA).},
  archiveprefix = {arXiv},
  keywords = {/unread,\#nosource,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  note = {Comment: To appear in the European Conference on Computer Vision (ECCV-2020)}
}

@inproceedings{hayes2020a,
  title = {Lifelong {{Machine Learning}} with {{Deep Streaming Linear Discriminant Analysis}}},
  booktitle = {{{CLVision Workshop}} at {{CVPR}} 2020},
  author = {Hayes, Tyler L and Kanan, Christopher},
  year = {2020},
  pages = {1--15},
  url = {http://arxiv.org/abs/1909.01520},
  abstract = {When a robot acquires new information, ideally it would immediately be capable of using that information to understand its environment. While deep neural networks are now widely used by robots for inferring semantic information, conventional neural networks suffer from catastrophic forgetting when they are incrementally updated, with new knowledge overwriting established representations. While a variety of approaches have been developed that attempt to mitigate catastrophic forgetting in the incremental batch learning scenario, in which an agent learns a large collection of labeled samples at once, streaming learning has been much less studied in the robotics and deep learning communities. In streaming learning, an agent learns instances one-by-one and can be tested at any time. Here, we revisit streaming linear discriminant analysis, which has been widely used in the data mining research community. By combining streaming linear discriminant analysis with deep learning, we are able to outperform both incremental batch learning and streaming learning algorithms on both ImageNet-1K and CORe50.},
  keywords = {[core50],[imagenet],/unread,\#nosource,⛔ No DOI found,deep learning,streaming learning},
  annotation = {\_eprint: 1909.01520}
}

@article{hayes2021,
  title = {Replay in {{Deep Learning}}: {{Current Approaches}} and {{Missing Biological Elements}}},
  shorttitle = {Replay in {{Deep Learning}}},
  author = {Hayes, Tyler L. and Krishnan, Giri P. and Bazhenov, Maxim and Siegelmann, Hava T. and Sejnowski, Terrence J. and Kanan, Christopher},
  year = {2021},
  journal = {arXiv},
  eprint = {2104.04132},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2104.04132},
  urldate = {2021-04-13},
  abstract = {Replay is the reactivation of one or more neural patterns, which are similar to the activation patterns experienced during past waking experiences. Replay was first observed in biological neural networks during sleep, and it is now thought to play a critical role in memory formation, retrieval, and consolidation. Replay-like mechanisms have been incorporated into deep artificial neural networks that learn over time to avoid catastrophic forgetting of previous knowledge. Replay algorithms have been successfully used in a wide range of deep learning methods within supervised, unsupervised, and reinforcement learning paradigms. In this paper, we provide the first comprehensive comparison between replay in the mammalian brain and replay in artificial neural networks. We identify multiple aspects of biological replay that are missing in deep learning systems and hypothesize how they could be utilized to improve artificial neural networks.},
  archiveprefix = {arXiv},
  keywords = {/unread,\#nosource,⛔ No DOI found,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Quantitative Biology - Neurons and Cognition}
}

@article{hayes2021a,
  title = {Replay in {{Deep Learning}}: {{Current Approaches}} and {{Missing Biological Elements}}},
  shorttitle = {Replay in {{Deep Learning}}},
  author = {Hayes, Tyler L. and Krishnan, Giri P. and Bazhenov, Maxim and Siegelmann, Hava T. and Sejnowski, Terrence J. and Kanan, Christopher},
  year = {2021},
  journal = {Neural Computation},
  volume = {33},
  number = {11},
  pages = {2908--2950},
  issn = {1530-888X},
  doi = {10/gn276q},
  url = {https://direct.mit.edu/neco/article-abstract/33/11/2908/107071/Replay-in-Deep-Learning-Current-Approaches-and?redirectedFrom=fulltext},
  abstract = {Replay is the reactivation of one or more neural patterns that are similar to the activation patterns experienced during past waking experiences. Replay was first observed in biological neural networks during sleep, and it is now thought to play a critical role in memory formation, retrieval, and consolidation. Replay-like mechanisms have been incorporated in deep artificial neural networks that learn over time to avoid catastrophic forgetting of previous knowledge. Replay algorithms have been successfully used in a wide range of deep learning methods within supervised, unsupervised, and reinforcement learning paradigms. In this letter, we provide the first comprehensive comparison between replay in the mammalian brain and replay in artificial neural networks. We identify multiple aspects of biological replay that are missing in deep learning systems and hypothesize how they could be used to improve artificial neural networks.},
  langid = {english},
  pmid = {34474476},
  keywords = {/unread,Algorithms,Animals,Deep Learning,Hippocampus,Neural Networks; Computer,Reinforcement; Psychology,Sleep},
  annotation = {ZSCC: 0000005}
}

@article{hayes2022,
  title = {Online {{Continual Learning}} for {{Embedded Devices}}},
  author = {Hayes, Tyler L. and Kanan, Christopher},
  year = {2022},
  journal = {arXiv},
  eprint = {2203.10681},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2203.10681},
  urldate = {2022-03-26},
  abstract = {Real-time on-device continual learning is needed for new applications such as home robots, user personalization on smartphones, and augmented/virtual reality headsets. However, this setting poses unique challenges: embedded devices have limited memory and compute capacity and conventional machine learning models suffer from catastrophic forgetting when updated on non-stationary data streams. While several online continual learning models have been developed, their effectiveness for embedded applications has not been rigorously studied. In this paper, we first identify criteria that online continual learners must meet to effectively perform real-time, on-device learning. We then study the efficacy of several online continual learning methods when used with mobile neural networks. We measure their performance, memory usage, compute requirements, and ability to generalize to out-of-domain inputs.},
  archiveprefix = {arXiv},
  keywords = {/unread,⛔ No DOI found,Computer Science - Artificial Intelligence,Computer Science - Machine Learning}
}

@inproceedings{he2018,
  title = {Overcoming {{Catastrophic}} Interference Using Conceptor-Aided Backpropagation},
  booktitle = {{{ICLR}}},
  author = {He, Xu and Jaeger, Herbert},
  year = {2018},
  url = {https://openreview.net/pdf?id=B1al7jg0b},
  abstract = {Catastrophic interference has been a major roadblock in the research of continual learning. Here we propose a variant of the back-propagation algorithm, "conceptor-aided backprop" (CAB), in which gradients are shielded by concep-tors against degradation of previously learned tasks. Conceptors have their origin in reservoir computing, where they have been previously shown to overcome catastrophic forgetting. CAB extends these results to deep feedforward networks. On the disjoint and permuted MNIST tasks, CAB outperforms two other methods for coping with catastrophic interference that have recently been proposed.},
  keywords = {[mnist],/unread,\#nosource,⛔ No DOI found}
}

@book{he2019,
  title = {Task {{Agnostic Continual Learning}} via {{Meta Learning}}},
  author = {He, Xu and Sygnowski, Jakub and Galashov, Alexandre and Rusu, Andrei A and Teh, Yee Whye and Pascanu, Razvan},
  year = {2019},
  journal = {arXiv:1906.05201 [cs, stat]},
  eprint = {1906.05201},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1906.05201},
  abstract = {While neural networks are powerful function approximators, they suffer from catastrophic forgetting when the data distribution is not stationary. One particular formalism that studies learning under non-stationary distribution is provided by continual learning, where the non-stationarity is imposed by a sequence of distinct tasks. Most methods in this space assume, however, the knowledge of task boundaries, and focus on alleviating catastrophic forgetting. In this work, we depart from this view and move the focus towards faster remembering \textendash{} i.e measuring how quickly the network recovers performance rather than measuring the network's performance without any adaptation. We argue that in many settings this can be more effective and that it opens the door to combining meta-learning and continual learning techniques, leveraging their complementary advantages. We propose a framework specific for the scenario where no information about task boundaries or task identity is given. It relies on a separation of concerns into what task is being solved and how the task should be solved. This framework is implemented by differentiating task specific parameters from task agnostic parameters, where the latter are optimized in a continual meta learning fashion, without access to multiple tasks at the same time. We showcase this framework in a supervised learning scenario and discuss the implication of the proposed formalism.},
  archiveprefix = {arXiv},
  keywords = {[mnist],/unread,\#nosource,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computi,Statistics - Machine Learning},
  note = {arXiv: 1906.05201
\par
arXiv: 1906.05201}
}

@inproceedings{he2021,
  ids = {he2021a},
  title = {Unsupervised {{Lifelong Learning}} with {{Curricula}}},
  booktitle = {Proceedings of the {{Web Conference}} 2021},
  author = {He, Yi and Chen, Sheng and Wu, Baijun and Yuan, Xu and Wu, Xindong},
  year = {2021},
  series = {{{WWW}} '21},
  pages = {3534--3545},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10/gnnd8h},
  url = {https://doi.org/10.1145/3442381.3449839},
  urldate = {2021-06-09},
  abstract = {Lifelong machine learning (LML) has driven the development of extensive web applications, enabling the learning systems deployed on web servers to deal with a sequence of tasks in an incremental fashion. Such systems can retain knowledge from learned tasks in a knowledge base and seamlessly apply it to improve the future learning. Unfortunately, most existing LML methods require labels in every task, whereas providing persistent human labeling for all future tasks is costly, onerous, error-prone, and hence impractical. Motivated by this situation, we propose a new paradigm named unsupervised lifelong learning with curricula (ULLC), where only one task needs to be labeled for initialization and the system then performs lifelong learning for subsequent tasks in an unsupervised fashion. A main challenge of realizing this paradigm lies in the occurrence of negative knowledge transfer, where partial old knowledge becomes detrimental for learning a given task yet cannot be filtered out by the learner without the help of labels. To overcome this challenge, we draw insights from the learning behaviors of humans. Specifically, when faced with a difficult task that cannot be well tackled by our current knowledge, we usually postpone it and work on some easier tasks first, which allows us to grow our knowledge. Thereafter, once we go back to the postponed task, we are more likely to tackle it well as we are more knowledgeable now. The key idea of ULLC is similar \textendash{} at any time, a pool of candidate tasks are organized in a curriculum by their distances to the knowledge base. The learner then starts from the closer tasks, accumulates knowledge from learning them, and moves to learn the faraway tasks with a gradually augmented knowledge base. The viability and effectiveness of our proposal are substantiated through extensive empirical studies on both synthetic and real datasets.},
  isbn = {978-1-4503-8312-7},
  keywords = {/unread,\#nosource,adversarial networks,lifelong learning,transfer Learning}
}

@book{hebb2002,
  title = {The {{Organization}} of {{Behavior}}: {{A Neuropsychological Theory}}},
  shorttitle = {The {{Organization}} of {{Behavior}}},
  author = {Hebb, D O},
  year = {2002},
  journal = {Lawrence Erlbaum},
  publisher = {{Psychology Press}},
  url = {https://www.amazon.com/Organization-Behavior-Neuropsychological-Theory/dp/0805843000 https://books.google.it/books/about/The_Organization_of_Behavior.html?id=ddB4AgAAQBAJ&printsec=frontcover&source=kp_read_button&redir_esc=y#v=onepage&q&f=false},
  abstract = {Since its publication in 1949, D.O. Hebb's, The Organization of Behavior has been one of the most influential books in the fields of psychology and neuroscience. However, the original edition has been unavailable since 1966, ensuring that Hebb's comment that a classic normally means "cited but not read" is true in his case. This new edition rectifies a long-standing problem for behavioral neuroscientists\textendash the inability to obtain one of the most cited publications in the field. The Organization of Behavior played a significant part in stimulating the investigation of the neural foundations of behavior and continues to be inspiring because it provides a general framework for relating behavior to synaptic organization through the dynamics of neural networks. D.O. Hebb was also the first to examine the mechanisms by which environment and experience can influence brain structure and function, and his ideas formed the basis for work on enriched environments as stimulants for behavioral development. References to Hebb, the Hebbian cell assembly, the Hebb synapse, and the Hebb rule increase each year. These forceful ideas of 1949 are now applied in engineering, robotics, and computer science, as well as neurophysiology, neuroscience, and psychology\textendash a tribute to Hebb's foresight in developing a foundational neuropsychological theory of the organization of behavior.},
  isbn = {978-1-135-63191-8},
  langid = {english},
  keywords = {[hebbian],/unread,\#nosource,Psychology / Cognitive Psychology \& Cognition,Psychology / General,Psychology / Neuropsychology,Psychology / Physiological Psychology}
}

@article{hemati2022,
  title = {Continual {{Learning}} for {{Unsupervised Anomaly Detection}} in {{Continuous Auditing}} of {{Financial Accounting Data}}},
  author = {Hemati, Hamed and Schreyer, Marco and Borth, Damian},
  year = {2022},
  journal = {AAAI 2022 Workshop on AI in Financial Services: Adaptiveness, Resilience \& Governance},
  url = {https://arxiv.org/abs/2112.13215},
  abstract = {International audit standards require the direct assessment of a financial statement's underlying accounting journal entries. Driven by advances in artificial intelligence, deep-learning inspired audit techniques emerged to examine vast quantities of journal entry data. However, in regular audits, most of the proposed methods are applied to learn from a comparably stationary journal entry population, e.g., of a financial quarter or year. Ignoring situations where audit relevant distribution changes are not evident in the training data or become incrementally available over time. In contrast, in continuous auditing, deep-learning models are continually trained on a stream of recorded journal entries, e.g., of the last hour. Resulting in situations where previous knowledge interferes with new information and will be entirely overwritten. This work proposes a continual anomaly detection framework to overcome both challenges and designed to learn from a stream of journal entry data experiences. The framework is evaluated based on deliberately designed audit scenarios and two real-world datasets. Our experimental results provide initial evidence that such a learning scheme offers the ability to reduce false-positive alerts and false-negative decisions.},
  keywords = {/unread,⛔ No DOI found}
}

@inproceedings{henning2021,
  title = {Posterior {{Meta-Replay}} for {{Continual Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Henning, Christian and Cervera, Maria and D'Angelo, Francesco and Oswald, Johannes Von and Traber, Regina and Ehret, Benjamin and Kobayashi, Seijin and Grewe, Benjamin F. and Sacramento, Joao},
  year = {2021},
  url = {https://openreview.net/forum?id=AhuVLaYp6gn},
  urldate = {2022-03-28},
  abstract = {Learning task-specific posteriors continually in a shared meta-model and studying the properties of predictive uncertainty for task inference.},
  langid = {english},
  keywords = {[bayes],/unread}
}

@phdthesis{henning2022,
  type = {Doctoral {{Thesis}}},
  title = {Knowledge Uncertainty and Lifelong Learning in Neural Systems},
  author = {Henning, Christian},
  year = {2022},
  doi = {10.3929/ethz-b-000523790},
  url = {https://www.research-collection.ethz.ch/handle/20.500.11850/523790},
  urldate = {2022-03-07},
  copyright = {http://rightsstatements.org/page/InC-NC/1.0/},
  langid = {english},
  school = {ETH Zurich},
  keywords = {/unread},
  annotation = {Accepted: 2022-03-07T06:53:40Z}
}

@inproceedings{hess2021,
  title = {A {{Procedural World Generation Framework}} for {{Systematic Evaluation}} of {{Continual Learning}}},
  booktitle = {Thirty-Fifth {{Conference}} on {{Neural Information Processing Systems Datasets}} and {{Benchmarks Track}}},
  author = {Hess, Timm and Mundt, Martin and Pliushch, Iuliia and Ramesh, Visvanathan},
  year = {2021},
  url = {https://openreview.net/forum?id=LlCQWh8-pwK},
  urldate = {2022-07-19},
  abstract = {We introduce a graphics simulator to flexibly compose datasets for deep continual learning.},
  langid = {english},
  keywords = {/unread}
}

@article{heuillet2020,
  title = {Explainability in {{Deep Reinforcement Learning}}},
  author = {Heuillet, Alexandre and Couthouis, Fabien and {D{\'i}az-Rodr{\'i}guez}, Natalia},
  year = {2020},
  journal = {arXiv:2008.06693 [cs]},
  eprint = {2008.06693},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2008.06693},
  urldate = {2021-10-22},
  abstract = {A large set of the explainable Artificial Intelligence (XAI) literature is emerging on feature relevance techniques to explain a deep neural network (DNN) output or explaining models that ingest image source data. However, assessing how XAI techniques can help understand models beyond classification tasks, e.g. for reinforcement learning (RL), has not been extensively studied. We review recent works in the direction to attain Explainable Reinforcement Learning (XRL), a relatively new subfield of Explainable Artificial Intelligence, intended to be used in general public applications, with diverse audiences, requiring ethical, responsible and trustable algorithms. In critical situations where it is essential to justify and explain the agent's behaviour, better explainability and interpretability of RL models could help gain scientific insight on the inner workings of what is still considered a black box. We evaluate mainly studies directly linking explainability to RL, and split these into two categories according to the way the explanations are generated: transparent algorithms and post-hoc explainaility. We also review the most prominent XAI works from the lenses of how they could potentially enlighten the further deployment of the latest advances in RL, in the demanding present and future of everyday problems.},
  archiveprefix = {arXiv},
  keywords = {/unread,⛔ No DOI found,Computer Science - Artificial Intelligence},
  annotation = {ZSCC: NoCitationData[s0]},
  note = {Comment: Article accepted at Knowledge-Based Systems}
}

@inproceedings{hou2018,
  title = {Lifelong {{Learning}} via {{Progressive Distillation}} and {{Retrospection}}},
  booktitle = {{{ECCV}}},
  author = {Hou, Saihui and Pan, Xinyu and Loy, Chen Change and Wang, Zilei and Lin, Dahua},
  year = {2018},
  issn = {16113349},
  doi = {10/gnq329},
  url = {http://link.springer.com/10.1007/978-3-030-01219-9_27},
  abstract = {Lifelong learning aims at adapting a learned model to new tasks while retaining the knowledge gained earlier. A key challenge for lifelong learning is how to strike a balance between the preservation on old tasks and the adaptation to a new one within a given model. Approaches that combine both objectives in training have been explored in previous works. Yet the performance still suffers from considerable degradation in a long sequence of tasks. In this work, we propose a novel approach to lifelong learning, which tries to seek a better balance between preservation and adaptation via two techniques: Distillation and Retrospection. Specifically, the target model adapts to the new task by knowledge distillation from an intermediate expert, while the previous knowledge is more effectively preserved by caching a small subset of data for old tasks. The combination of Distillation and Retrospection leads to a more gentle learning curve for the target model, and extensive experiments demonstrate that our approach can bring consistent improvements on both old and new tasks.},
  isbn = {978-3-030-01218-2},
  keywords = {[imagenet],[vision],/unread,\#nosource,Knowledge distillation,Lifelong learning,Retrospection}
}

@inproceedings{hung2019,
  title = {Compacting, {{Picking}} and {{Growing}} for {{Unforgetting Continual Learning}}},
  booktitle = {{{NeurIPS}}},
  author = {Hung, Steven C Y and Tu, Cheng-Hao and Wu, Cheng-En and Chen, Chien-Hung and Chan, Yi-Ming and Chen, Chu-Song},
  year = {2019},
  pages = {13669--13679},
  url = {http://papers.nips.cc/paper/9518-compacting-picking-and-growing-for-unforgetting-continual-learning.pdf},
  abstract = {Continual lifelong learning is essential to many applications. In this paper, we propose a simple but effective approach to continual deep learning. Our approach leverages the principles of deep model compression, critical weights selection, and progressive networks expansion. By enforcing their integration in an iterative manner, we introduce an incremental learning method that is scalable to the number of sequential tasks in a continual learning process. Our approach is easy to implement and owns several favorable characteristics. First, it can avoid forgetting (i.e., learn new tasks while remembering all previous tasks). Second, it allows model expansion but can maintain the model compactness when handling sequential tasks. Besides, through our compaction and selection/expansion mechanism, we show that the knowledge accumulated through learning previous tasks is helpful to build a better model for the new tasks compared to training the models independently with tasks. Experimental results show that our approach can incrementally learn a deep model tackling multiple tasks without forgetting, while the model compactness is maintained with the performance more satisfiable than individual task training.},
  keywords = {[cifar],[imagenet],/unread,\#nosource,⛔ No DOI found}
}

@misc{hurtado2022,
  title = {It's All {{About Consistency}}: {{A Study}} on {{Memory Composition}} for {{Replay-Based Methods}} in {{Continual Learning}}},
  shorttitle = {It's All {{About Consistency}}},
  author = {Hurtado, Julio and {Raymond-Saez}, Alain and Araujo, Vladimir and Lomonaco, Vincenzo and Bacciu, Davide},
  year = {2022},
  number = {arXiv:2207.01145},
  eprint = {2207.01145},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2207.01145},
  url = {http://arxiv.org/abs/2207.01145},
  urldate = {2022-07-11},
  abstract = {Continual Learning methods strive to mitigate Catastrophic Forgetting (CF), where knowledge from previously learned tasks is lost when learning a new one. Among those algorithms, some maintain a subset of samples from previous tasks when training. These samples are referred to as a memory. These methods have shown outstanding performance while being conceptually simple and easy to implement. Yet, despite their popularity, little has been done to understand which elements to be included into the memory. Currently, this memory is often filled via random sampling with no guiding principles that may aid in retaining previous knowledge. In this work, we propose a criterion based on the learning consistency of a sample called Consistency AWare Sampling (CAWS). This criterion prioritizes samples that are easier to learn by deep networks. We perform studies on three different memory-based methods: AGEM, GDumb, and Experience Replay, on MNIST, CIFAR-10 and CIFAR-100 datasets. We show that using the most consistent elements yields performance gains when constrained by a compute budget; when under no such constrain, random sampling is a strong baseline. However, using CAWS on Experience Replay yields improved performance over the random baseline. Finally, we show that CAWS achieves similar results to a popular memory selection method while requiring significantly less computational resources.},
  archiveprefix = {arXiv},
  keywords = {/unread,Computer Science - Machine Learning}
}

@article{isele2018,
  title = {Selective {{Experience Replay}} for {{Lifelong Learning}}},
  author = {Isele, David and Cosgun, Akansel},
  year = {2018},
  journal = {Thirty-Second AAAI Conference on Artificial Intelligence},
  pages = {3302--3309},
  url = {http://arxiv.org/abs/1802.10269},
  abstract = {Deep reinforcement learning has emerged as a powerful tool for a variety of learning tasks, however deep nets typically exhibit forgetting when learning multiple tasks in sequence. To mitigate forgetting, we propose an experience replay process that augments the standard FIFO buffer and selectively stores experiences in a long-term memory. We explore four strategies for selecting which experiences will be stored: favoring surprise, favoring reward, matching the global training distribution, and maximizing coverage of the state space. We show that distribution matching successfully prevents catastrophic forgetting, and is consistently the best approach on all domains tested. While distribution matching has better and more consistent performance, we identify one case in which coverage maximization is beneficial - when tasks that receive less trained are more important. Overall, our results show that selective experience replay, when suitable selection algorithms are employed, can prevent catastrophic forgetting.},
  keywords = {/unread,\#nosource,⛔ No DOI found,Natural Language Processing and Machine Learning T},
  annotation = {\_eprint: 1802.10269}
}

@inproceedings{javed2019,
  title = {Meta-{{Learning Representations}} for {{Continual Learning}}},
  booktitle = {{{NeurIPS}}},
  author = {Javed, Khurram and White, Martha},
  year = {2019},
  url = {http://papers.nips.cc/paper/8458-meta-learning-representations-for-continual-learning},
  abstract = {A continual learning agent should be able to build on top of existing knowledge to learn on new data quickly while minimizing forgetting. Current intelligent systems based on neural network function approximators arguably do the opposite-they are highly prone to forgetting and rarely trained to facilitate future learning. One reason for this poor behavior is that they learn from a representation that is not explicitly trained for these two goals. In this paper, we propose OML, an objective that directly minimizes catastrophic interference by learning representations that accelerate future learning and are robust to forgetting under online updates in continual learning. We show that it is possible to learn naturally sparse representations that are more effective for online updating. Moreover, our algorithm is complementary to existing continual learning strategies, such as MER and GEM. Finally, we demonstrate that a basic online updating strategy on representations learned by OML is competitive with rehearsal based methods for continual learning. 1},
  keywords = {[omniglot],/unread,\#nosource,⛔ No DOI found}
}

@inproceedings{jerfel2019,
  title = {Reconciling Meta-Learning and Continual Learning with Online Mixtures of Tasks},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Jerfel, Ghassen and Grant, Erin and Griffiths, Tom and Heller, Katherine A},
  year = {2019},
  pages = {9122--9133},
  url = {http://papers.nips.cc/paper/9112-reconciling-meta-learning-and-continual-learning-with-online-mixtures-of-tasks},
  abstract = {Learning-to-learn or meta-learning leverages data-driven inductive bias to increase the efficiency of learning on a novel task. This approach encounters difficulty when transfer is not advantageous, for instance, when tasks are considerably dissimilar or change over time. We use the connection between gradient-based meta-learning and hierarchical Bayes to propose a Dirichlet process mixture of hierarchical Bayesian models over the parameters of an arbitrary parametric model such as a neural network. In contrast to consolidating inductive biases into a single set of hyperparameters, our approach of task-dependent hyperparameter selection better handles latent distribution shift, as demonstrated on a set of evolving, image-based, few-shot learning benchmarks.},
  keywords = {[bayes],[vision],/unread,\#nosource,⛔ No DOI found}
}

@inproceedings{joseph2020,
  title = {Meta-{{Consolidation}} for {{Continual Learning}}},
  booktitle = {{{NeurIPS}}},
  author = {Joseph, K J and Balasubramanian, Vineeth N},
  year = {2020},
  url = {http://arxiv.org/abs/2010.00352},
  abstract = {The ability to continuously learn and adapt itself to new tasks, without losing grasp of already acquired knowledge is a hallmark of biological learning systems, which current deep learning systems fall short of. In this work, we present a novel methodology for continual learning called MERLIN: Meta-Consolidation for Continual Learning. We assume that weights of a neural network \$\textbackslash backslashboldsymbol \textbackslash backslashpsi\$, for solving task \$\textbackslash backslashboldsymbol t\$, come from a meta-distribution \$p(\textbackslash backslashboldsymbol\{\textbackslash backslashpsi|t\})\$. This meta-distribution is learned and consolidated incrementally. We operate in the challenging online continual learning setting, where a data point is seen by the model only once. Our experiments with continual learning benchmarks of MNIST, CIFAR-10, CIFAR-100 and Mini-ImageNet datasets show consistent improvement over five baselines, including a recent state-of-the-art, corroborating the promise of MERLIN.},
  keywords = {[bayes],[cifar],[imagenet],[mnist],/unread,\#nosource},
  annotation = {\_eprint: 2010.00352},
  note = {The authors leverage a bayesian framework in which the parameters of a model are sampled from a generating distribution. This distribution, parameterized by a task label, is used together with a VAE to consolidate online previous and current knowledge. Inference does not require task labels and exploit an ensemble of model, sampled from the generating distribution.
\par
The authors leverage a bayesian framework in which the parameters of a model are sampled from a generating distribution. This distribution, parameterized by a task label, is used together with a VAE to consolidate online previous and current knowledge. Inference does not require task labels and exploit an ensemble of model, sampled from the generating distribution.}
}

@inproceedings{jung2018,
  title = {Less-{{Forgetful Learning}} for {{Domain Expansion}} in {{Deep Neural Networks}}},
  booktitle = {Thirty-{{Second AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Jung, Heechul and Ju, Jeongwoo and Jung, Minju and Kim, Junmo},
  year = {2018},
  url = {https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17073},
  urldate = {2021-10-30},
  abstract = {Expanding the domain that deep neural network has already learned without accessing old domain data is a challenging task because deep neural networks forget previously learned information when learning new data from a new domain. In this paper, we propose a less-forgetful learning method for the domain expansion scenario. While existing domain adaptation techniques solely focused on adapting to new domains, the proposed technique focuses on working well with both old and new domains without needing to know whether the input is from the old or new domain. First, we present two naive approaches which will be problematic, then we provide a new method using two proposed properties for less-forgetful learning. Finally, we prove the effectiveness of our method through experiments on image classification tasks. All datasets used in the paper, will be released on our website for someone's follow-up study.},
  copyright = {Authors who publish a paper in this conference agree to the following terms:   Author(s) agree to transfer their copyrights in their article/paper to the Association for the Advancement of Artificial Intelligence (AAAI), in order to deal with future requests for reprints, translations, anthologies, reproductions, excerpts, and other publications. This grant will include, without limitation, the entire copyright in the article/paper in all countries of the world, including all renewals, extensions, and reversions thereof, whether such rights current exist or hereafter come into effect, and also the exclusive right to create electronic versions of the article/paper, to the extent that such right is not subsumed under copyright.  The author(s) warrants that they are the sole author and owner of the copyright in the above article/paper, except for those portions shown to be in quotations; that the article/paper is original throughout; and that the undersigned right to make the grants set forth above is complete and unencumbered.  The author(s) agree that if anyone brings any claim or action alleging facts that, if true, constitute a breach of any of the foregoing warranties, the author(s) will hold harmless and indemnify AAAI, their grantees, their licensees, and their distributors against any liability, whether under judgment, decree, or compromise, and any legal fees and expenses arising out of that claim or actions, and the undersigned will cooperate fully in any defense AAAI may make to such claim or action. Moreover, the undersigned agrees to cooperate in any claim or other action seeking to protect or enforce any right the undersigned has granted to AAAI in the article/paper. If any such claim or action fails because of facts that constitute a breach of any of the foregoing warranties, the undersigned agrees to reimburse whomever brings such claim or action for expenses and attorneys' fees incurred therein.  Author(s) retain all proprietary rights other than copyright (such as patent rights).  Author(s) may make personal reuse of all or portions of the above article/paper in other works of their own authorship.  Author(s) may reproduce, or have reproduced, their article/paper for the author's personal use, or for company use provided that AAAI copyright and the source are indicated, and that the copies are not used in a way that implies AAAI endorsement of a product or service of an employer, and that the copies per se are not offered for sale. The foregoing right shall not permit the posting of the article/paper in electronic or digital form on any computer network, except by the author or the author's employer, and then only on the author's or the employer's own web page or ftp site. Such web page or ftp site, in addition to the aforementioned requirements of this Paragraph, must provide an electronic reference or link back to the AAAI electronic server, and shall not post other AAAI copyrighted materials not of the author's or the employer's creation (including tables of contents with links to other papers) without AAAI's written permission.  Author(s) may make limited distribution of all or portions of their article/paper prior to publication.  In the case of work performed under U.S. Government contract, AAAI grants the U.S. Government royalty-free permission to reproduce all or portions of the above article/paper, and to authorize others to do so, for U.S. Government purposes.  In the event the above article/paper is not accepted and published by AAAI, or is withdrawn by the author(s) before acceptance by AAAI, this agreement becomes null and void.},
  langid = {english},
  keywords = {/unread,\#nosource}
}

@misc{jung2021,
  title = {Continual {{Learning}} with {{Node-Importance}} Based {{Adaptive Group Sparse Regularization}}},
  author = {Jung, Sangwon and Ahn, Hongjoon and Cha, Sungmin and Moon, Taesup},
  year = {2021},
  number = {arXiv:2003.13726},
  eprint = {2003.13726},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2003.13726},
  urldate = {2022-07-08},
  abstract = {We propose a novel regularization-based continual learning method, dubbed as Adaptive Group Sparsity based Continual Learning (AGS-CL), using two group sparsity-based penalties. Our method selectively employs the two penalties when learning each node based its the importance, which is adaptively updated after learning each new task. By utilizing the proximal gradient descent method for learning, the exact sparsity and freezing of the model is guaranteed, and thus, the learner can explicitly control the model capacity as the learning continues. Furthermore, as a critical detail, we re-initialize the weights associated with unimportant nodes after learning each task in order to prevent the negative transfer that causes the catastrophic forgetting and facilitate efficient learning of new tasks. Throughout the extensive experimental results, we show that our AGS-CL uses much less additional memory space for storing the regularization parameters, and it significantly outperforms several state-of-the-art baselines on representative continual learning benchmarks for both supervised and reinforcement learning tasks.},
  archiveprefix = {arXiv},
  keywords = {/unread,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@inproceedings{kading2016,
  title = {Fine-{{Tuning Deep Neural Networks}} in {{Continuous Learning Scenarios}}},
  booktitle = {{{ACCV Workshop}}},
  author = {K{\"a}ding, Christoph and Rodner, Erik and Freytag, Alexander and Denzler, Joachim},
  year = {2016},
  issn = {16113349},
  doi = {10/gk79q4},
  url = {http://link.springer.com/10.1007/978-3-319-54526-4_43},
  abstract = {The revival of deep neural networks and the availability of ImageNet laid the foundation for recent success in highly complex recognition tasks. However, ImageNet does not cover all visual concepts of all possible application scenarios. Hence, application experts still record new data constantly and expect the data to be used upon its availability. In this paper, we follow this observation and apply the classical concept of fine-tuning deep neural networks to scenarios where data from known or completely new classes is continuously added. Besides a straightforward realization of continuous fine-tuning, we empirically analyze how computational burdens of training can be further reduced. Finally, we visualize how the network's attention maps evolve over time which allows for visually investigating what the network learned during continuous fine-tuning.},
  isbn = {978-3-319-54525-7},
  keywords = {[imagenet],/unread,\#nosource}
}

@inproceedings{kaplanis2018,
  title = {Continual {{Reinforcement Learning}} with {{Complex Synapses}}},
  booktitle = {{{ICML}}},
  author = {Kaplanis, Christos and Shanahan, Murray and Clopath, Claudia},
  year = {2018},
  url = {http://arxiv.org/abs/1802.07239},
  abstract = {Unlike humans, who are capable of continual learning over their lifetimes, artificial neural networks have long been known to suffer from a phenomenon known as catastrophic forgetting, whereby new learning can lead to abrupt erasure of previously acquired knowledge. Whereas in a neural network the parameters are typically modelled as scalar values, an individual synapse in the brain comprises a complex network of interacting biochemical components that evolve at different timescales. In this paper, we show that by equipping tabular and deep reinforcement learning agents with a synaptic model that incorporates this biological complexity (Benna \& Fusi, 2016), catastrophic forgetting can be mitigated at multiple timescales. In particular, we find that as well as enabling continual learning across sequential training of two simple tasks, it can also be used to overcome within-task forgetting by reducing the need for an experience replay database.},
  keywords = {/unread,\#nosource},
  annotation = {\_eprint: 1802.07239}
}

@article{kaplanis2019,
  title = {Policy {{Consolidation}} for {{Continual Reinforcement Learning}}},
  author = {Kaplanis, Christos and Shanahan, Murray and Clopath, Claudia},
  year = {2019},
  journal = {ICML},
  url = {http://arxiv.org/abs/1902.00255},
  abstract = {We propose a method for tackling catastrophic forgetting in deep reinforcement learning that is \$\textbackslash backslash\$textit\{agnostic\} to the timescale of changes in the distribution of experiences, does not require knowledge of task boundaries, and can adapt in \$\textbackslash backslash\$textit\{continuously\} changing environments. In our \$\textbackslash backslash\$textit\{policy consolidation\} model, the policy network interacts with a cascade of hidden networks that simultaneously remember the agent's policy at a range of timescales and regularise the current policy by its own history, thereby improving its ability to learn without forgetting. We find that the model improves continual learning relative to baselines on a number of continuous control tasks in single-task, alternating two-task, and multi-agent competitive self-play settings.},
  keywords = {/unread,\#nosource,⛔ No DOI found},
  annotation = {\_eprint: 1902.00255}
}

@incollection{kapoor2009,
  title = {Principles of {{Lifelong Learning}} for {{Predictive User Modeling}}},
  booktitle = {User {{Modeling}} 2007},
  author = {Kapoor, Ashish and Horvitz, Eric},
  year = {2009},
  pages = {37--46},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  issn = {03029743},
  doi = {10.1007/978-3-540-73078-1_7},
  url = {http://link.springer.com/10.1007/978-3-540-73078-1_7},
  isbn = {978-3-540-73077-4},
  pmid = {16717005},
  keywords = {/unread,\#nosource}
}

@inproceedings{kemker2018,
  title = {Measuring {{Catastrophic Forgetting}} in {{Neural Networks}}},
  booktitle = {Thirty-{{Second AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Kemker, Ronald and McClure, Marc and Abitino, Angelina and Hayes, Tyler L and Kanan, Christopher},
  year = {2018},
  url = {https://arxiv.org/pdf/1708.02072.pdf},
  abstract = {Deep neural networks are used in many state-of-the-art systems for machine perception. Once a network is trained to do a specific task, e.g., bird classification, it cannot easily be trained to do new tasks, e.g., incrementally learning to recognize additional bird species or learning an entirely different task such as flower recognition. When new tasks are added, typical deep neural networks are prone to catastrophically forgetting previous tasks. Networks that are capable of assimilating new information incrementally, much like how humans form new memories over time, will be more efficient than re-training the model from scratch each time a new task needs to be learned. There have been multiple attempts to develop schemes that mitigate catastrophic forgetting, but these methods have not been directly compared, the tests used to evaluate them vary considerably, and these methods have only been evaluated on small-scale problems (e.g., MNIST). In this paper, we introduce new metrics and benchmarks for directly comparing five different mechanisms designed to mitigate catastrophic forgetting in neural networks: regularization, ensembling, rehearsal, dual-memory, and sparse-coding. Our experiments on real-world images and sounds show that the mechanism(s) that are critical for optimal performance vary based on the incremental training paradigm and type of data being used, but they all demonstrate that the catastrophic forgetting problem is not yet solved.},
  langid = {english},
  keywords = {[mnist],/unread,\#nosource,audioset,kemker,review}
}

@inproceedings{kemker2018a,
  title = {{{FearNet}}: {{Brain-Inspired Model}} for {{Incremental Learning}}},
  booktitle = {{{ICLR}}},
  author = {Kemker, Ronald and Kanan, Christopher},
  year = {2018},
  url = {https://openreview.net/pdf?id=SJ1Xmf-Rb},
  abstract = {Incremental class learning involves sequentially learning classes in bursts of examples from the same class. This violates the assumptions that underlie methods for training standard deep neural networks, and will cause them to suffer from catastrophic forgetting. Arguably, the best method for incremental class learning is iCaRL, but it requires storing training examples for each class, making it challenging to scale. Here, we propose FearNet for incremental class learning. FearNet is a generative model that does not store previous examples, making it memory efficient. FearNet uses a brain-inspired dual-memory system in which new memories are consolidated from a network for recent memories inspired by the mammalian hippocampal complex to a network for long-term storage inspired by medial prefrontal cortex. Memory consolidation is inspired by mechanisms that occur during sleep. FearNet also uses a module inspired by the basolateral amygdala for determining which memory system to use for recall. FearNet achieves state-of-the-art performance at incremental class learning on image (CIFAR-100, CUB-200) and audio classification (AudioSet) benchmarks.},
  keywords = {[audio],[cifar],[generative],/unread,\#nosource}
}

@article{kirkpatrick2017,
  title = {Overcoming Catastrophic Forgetting in Neural Networks},
  author = {Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A and Milan, Kieran and Quan, John and Ramalho, Tiago and {Grabska-Barwinska}, Agnieszka and Hassabis, Demis and Clopath, Claudia and Kumaran, Dharshan and Hadsell, Raia},
  year = {2017},
  journal = {PNAS},
  volume = {114},
  number = {13},
  pages = {3521--3526},
  doi = {10/gfvjcp},
  url = {http://arxiv.org/abs/1612.00796},
  abstract = {The ability to learn tasks in a sequential fashion is crucial to the development of artificial intelligence. Neural networks are not, in general, capable of this and it has been widely thought that catastrophic forgetting is an inevitable feature of connectionist models. We show that it is possible to overcome this limitation and train networks that can maintain expertise on tasks which they have not experienced for a long time. Our approach remembers old tasks by selectively slowing down learning on the weights important for those tasks. We demonstrate our approach is scalable and effective by solving a set of classification tasks based on the MNIST hand written digit dataset and by learning several Atari 2600 games sequentially.},
  keywords = {[mnist],/unread,\#nosource,annotated,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,ewc,Statistics - Machine Learning},
  note = {arXiv: 1612.00796
\par
arXiv: 1612.00796}
}

@article{kiyasseh2020,
  title = {{{CLOPS}}: {{Continual Learning}} of {{Physiological Signals}}},
  author = {Kiyasseh, Dani and Zhu, Tingting and Clifton, David A},
  year = {2020},
  journal = {arXiv},
  url = {http://arxiv.org/abs/2004.09578},
  abstract = {Deep learning algorithms are known to experience destructive interference when instances violate the assumption of being independent and identically distributed (i.i.d). This violation, however, is ubiquitous in clinical settings where data are streamed temporally and from a multitude of physiological sensors. To overcome this obstacle, we propose CLOPS, a healthcare-specific replay-based continual learning strategy. In three continual learning scenarios based on three publically-available datasets, we show that CLOPS can outperform its multi-task learning counterpart. Moreover, we propose end-to-end trainable parameters, which we term task-instance parameters, that can be used to quantify task difficulty and similarity. This quantification yields insights into both network interpretability and clinical applications, where task difficulty is poorly quantified.},
  keywords = {/unread,\#nosource,⛔ No DOI found},
  annotation = {\_eprint: 2004.09578}
}

@inproceedings{knoblauch2020,
  title = {Optimal {{Continual Learning}} Has {{Perfect Memory}} and Is {{NP-HARD}}},
  booktitle = {{{ICML}}},
  author = {Knoblauch, Jeremias and Husain, Hisham and Diethe, Tom},
  year = {2020},
  url = {https://proceedings.icml.cc/paper/2020/file/274ad4786c3abca69fa097b85867d9a4-Paper.pdf},
  abstract = {Continual Learning (CL) algorithms incremen-tally learn a predictor or representation across multiple sequentially observed tasks. Designing CL algorithms that perform reliably and avoid so-called catastrophic forgetting has proven a persistent challenge. The current paper develops a theoretical approach that explains why. In particular , we derive the computational properties which CL algorithms would have to possess in order to avoid catastrophic forgetting. Our main finding is that such optimal CL algorithms generally solve an NP-HARD problem and will require perfect memory to do so. The findings are of theoretical interest, but also explain the excellent performance of CL algorithms using experience replay, episodic memory and core sets relative to regularization-based approaches.},
  keywords = {[theoretical],/unread,\#nosource,⛔ No DOI found},
  note = {In this paper optimal continual learning which perfectly solves all tasks without forgetting is proofed to be NP-hard. Also, memorization of previous information is necessary, thus establishing superiority of replay based strategies against regularization based strategies.}
}

@inproceedings{kobayashi2019,
  title = {Continual {{Learning Exploiting Structure}} of {{Fractal Reservoir Computing}}},
  booktitle = {Artificial {{Neural Networks}} and {{Machine Learning}} \textendash{} {{ICANN}} 2019: {{Workshop}} and {{Special Sessions}}},
  author = {Kobayashi, Taisuke and Sugino, Toshiki},
  editor = {Tetko, Igor V and K{\r{u}}rkov{\'a}, V{\v e}ra and Karpov, Pavel and Theis, Fabian},
  year = {2019},
  volume = {11731},
  pages = {35--47},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10/gnq33b},
  url = {http://link.springer.com/10.1007/978-3-030-30493-5_4},
  abstract = {Neural network has a critical problem, called catastrophic forgetting, where memories for tasks already learned are easily overwritten with memories for a task additionally learned. This problem interferes with continual learning required for autonomous robots, which learn many tasks incrementally from daily activities. To mitigate the catastrophic forgetting, it is important for especially reservoir computing to clarify which neurons should be fired corresponding to each task, since only readout weights are updated according to the degree of firing of neurons. We therefore propose the way to design reservoir computing such that the firing neurons are clearly distinguished from others according to the task to be performed. As a key design feature, we employ fractal network, which has modularity and scalability, to be reservoir layer. In particular, its modularity is fully utilized by designing input layer. As a result, simulations of control tasks using reinforcement learning show that our design mitigates the catastrophic forgetting even when random actions from reinforcement learning prompt parameters to be overwritten. Furthermore, learning multiple tasks with a single network suggests that knowledge for the other tasks can facilitate to learn a new task, unlike the case using completely different networks.},
  isbn = {978-3-030-30492-8 978-3-030-30493-5},
  langid = {english},
  keywords = {[rnn],/unread,\#nosource,fractals,rc,reinforcement,reservoir computing},
  note = {A reservoir computing approach with Echo State Networks is implemented in order to learn multiple tasks in reinforcement learning environments.}
}

@article{krishnan2020,
  title = {Meta {{Continual Learning}} via {{Dynamic Programming}}},
  author = {Krishnan, R and Balaprakash, Prasanna},
  year = {2020},
  journal = {arXiv},
  url = {https://arxiv.org/abs/2008.02219},
  abstract = {Meta-continual learning algorithms seek to rapidly train a model when faced with similar tasks sampled sequentially from a task distribution. Although impressive strides have been made in this area, there is no theoretical framework that enables systematic analysis of key learning challenges, such as generalization and catastrophic forgetting. We introduce a new theoretical framework for meta-continual learning using dynamic programming, analyze generalization and catastrophic forgetting, and establish conditions of optimality. We show that existing meta-continual learning methods can be derived from the proposed dynamic programming framework. Moreover, we develop a new dynamic-programming-based meta-continual approach that adopts stochastic-gradient-driven alternating optimization method. We show that, on meta-continual learning benchmark data sets, our theoretically grounded meta-continual learning approach is better than or comparable to the purely empirical strategies adopted by the existing state-of-the-art methods.},
  keywords = {[omniglot],/unread,\#nosource,⛔ No DOI found},
  annotation = {\_eprint: 2008.02219}
}

@article{kruszewski2020,
  title = {Evaluating {{Online Continual Learning}} with {{CALM}}},
  author = {Kruszewski, Germ{\'a}n and Sorodoc, Ionut-Teodor and Mikolov, Tomas},
  year = {2020},
  journal = {arXiv},
  url = {https://arxiv.org/abs/2004.03340v2},
  urldate = {2021-02-05},
  abstract = {Online Continual Learning (OCL) studies learning over a continuous data stream without observing any single example more than once, a setting that is closer to the experience of humans and systems that must learn "on-the-wild". Yet, commonly available benchmarks are far from these real-world conditions, because they explicitly signal different tasks, lack latent similarity structure or assume temporal independence between different examples. Here, we propose a new benchmark for OCL based on language modelling in which input alternates between different languages and domains without any explicit delimitation. Additionally, we propose new metrics to study catastrophic forgetting in this setting and evaluate multiple baseline models based on compositions of experts. Finally, we introduce a simple gating technique that learns the latent similarities between different inputs, improving the performance of a Products of Experts model.},
  langid = {english},
  keywords = {[nlp],[rnn],/unread,\#nosource,⛔ No DOI found}
}

@article{kudithipudi2022,
  title = {Biological Underpinnings for Lifelong Learning Machines},
  author = {Kudithipudi, Dhireesha and {Aguilar-Simon}, Mario and Babb, Jonathan and Bazhenov, Maxim and Blackiston, Douglas and Bongard, Josh and Brna, Andrew P. and Chakravarthi Raja, Suraj and Cheney, Nick and Clune, Jeff and Daram, Anurag and Fusi, Stefano and Helfer, Peter and Kay, Leslie and Ketz, Nicholas and Kira, Zsolt and Kolouri, Soheil and Krichmar, Jeffrey L. and Kriegman, Sam and Levin, Michael and Madireddy, Sandeep and Manicka, Santosh and Marjaninejad, Ali and McNaughton, Bruce and Miikkulainen, Risto and Navratilova, Zaneta and Pandit, Tej and Parker, Alice and Pilly, Praveen K. and Risi, Sebastian and Sejnowski, Terrence J. and Soltoggio, Andrea and Soures, Nicholas and Tolias, Andreas S. and {Urbina-Mel{\'e}ndez}, Dar{\'i}o and {Valero-Cuevas}, Francisco J. and {van de Ven}, Gido M. and Vogelstein, Joshua T. and Wang, Felix and Weiss, Ron and {Yanguas-Gil}, Angel and Zou, Xinyun and Siegelmann, Hava},
  year = {2022},
  journal = {Nature Machine Intelligence},
  volume = {4},
  number = {3},
  pages = {196--210},
  publisher = {{Nature Publishing Group}},
  issn = {2522-5839},
  doi = {10/gpr749},
  url = {https://www.nature.com/articles/s42256-022-00452-0},
  urldate = {2022-03-24},
  abstract = {Biological organisms learn from interactions with their environment throughout their lifetime. For artificial systems to successfully act and adapt in the real world, it is desirable to similarly be able to learn on a continual basis. This challenge is known as lifelong learning, and remains to a large extent unsolved. In this Perspective article, we identify a set of key capabilities that artificial systems will need to achieve lifelong learning. We describe a number of biological mechanisms, both neuronal and non-neuronal, that help explain how organisms solve these challenges, and present examples of biologically inspired models and biologically plausible mechanisms that have been applied to artificial systems in the quest towards development of lifelong learning machines. We discuss opportunities to further our understanding and advance the state of the art in lifelong learning, aiming to bridge the gap between natural and artificial intelligence. It is an outstanding challenge to develop intelligent machines that can learn continually from interactions with their environment, throughout their lifetime. Kudithipudi et al. review neuronal and non-neuronal processes in organisms that address this challenge and discuss pathways to developing biologically inspired approaches for lifelong learning machines.},
  copyright = {2022 Springer Nature Limited},
  langid = {english},
  keywords = {/unread,Computer science,Intelligence,Learning algorithms}
}

@inproceedings{kurle2020,
  title = {Continual {{Learning}} with {{Bayesian Neural Networks}} for {{Non-Stationary Data}}},
  booktitle = {Eighth {{International Conference}} on {{Learning Representations}}},
  author = {Kurle, Richard and Cseke, Botond and Klushyn, Alexej and van der Smagt, Patrick and G{\"u}nnemann, Stephan},
  year = {2020},
  url = {https://iclr.cc/virtual_2020/poster_SJlsFpVtDB.html},
  urldate = {2021-01-01},
  abstract = {This work addresses continual learning for non-stationary data, using Bayesian neural networks and memory-based online variational Bayes. We represent the posterior approximation of the network weights by a diagonal Gaussian distribution and a complementary memory of raw data. This raw data corresponds to likelihood terms that cannot be well approximated by the Gaussian. We introduce a novel method for sequentially updating both components of the posterior approximation. Furthermore, we propose Bayesian forgetting and a Gaussian diffusion process for adapting to non-stationary data. The experimental results show that our update method improves on existing approaches for streaming data. Additionally, the adaptation methods lead to better predictive performance for non-stationary data.},
  langid = {english},
  keywords = {[bayes],/unread,\#nosource,⛔ No DOI found}
}

@article{kurniawan2021,
  title = {Online {{Continual Learning}} via {{Multiple Deep Metric Learning}} and {{Uncertainty-guided Episodic Memory Replay}} -- 3rd {{Place Solution}} for {{ICCV}} 2021 {{Workshop SSLAD Track 3A Continual Object Classification}}},
  author = {Kurniawan, Muhammad Rifki and Wei, Xing and Gong, Yihong},
  year = {2021},
  journal = {arXiv},
  eprint = {2111.02757},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2111.02757},
  urldate = {2022-05-03},
  abstract = {Online continual learning in the wild is a very difficult task in machine learning. Non-stationarity in online continual learning potentially brings about catastrophic forgetting in neural networks. Specifically, online continual learning for autonomous driving with SODA10M dataset exhibits extra problems on extremely long-tailed distribution with continuous distribution shift. To address these problems, we propose multiple deep metric representation learning via both contrastive and supervised contrastive learning alongside soft labels distillation to improve model generalization. Moreover, we exploit modified class-balanced focal loss for sensitive penalization in class imbalanced and hard-easy samples. We also store some samples under guidance of uncertainty metric for rehearsal and perform online and periodical memory updates. Our proposed method achieves considerable generalization with average mean class accuracy (AMCA) 64.01\% on validation and 64.53\% AMCA on test set.},
  archiveprefix = {arXiv},
  keywords = {/unread,⛔ No DOI found,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  annotation = {ZSCC:00000},
  note = {Comment: 6 pages, 2 figures, 3 algorithms, 1 table}
}

@article{kuzina2019,
  title = {{{BooVAE}}: {{A}} Scalable Framework for Continual {{VAE}} Learning under Boosting Approach},
  author = {Kuzina, Anna and Egorov, Evgenii and Burnaev, Evgeny},
  year = {2019},
  journal = {arXiv},
  url = {http://arxiv.org/abs/1908.11853},
  abstract = {Variational Auto Encoders (VAE) are capable of generating realistic images, sounds and video sequences. From practitioners point of view, we are usually interested in solving problems where tasks are learned sequentially, in a way that avoids revisiting all previous data at each stage. We address this problem by introducing a conceptually simple and scalable end-to-end approach of incorporating past knowledge by learning prior directly from the data. We consider scalable boosting-like approximation for intractable theoretical optimal prior. We provide empirical studies on two commonly used benchmarks, namely MNIST and Fashion MNIST on disjoint sequential image generation tasks. For each dataset proposed method delivers the best results or comparable to SOTA, avoiding catastrophic forgetting in a fully automatic way.},
  keywords = {[bayes],[fashion],[mnist],/unread,\#nosource,⛔ No DOI found},
  annotation = {\_eprint: 1908.11853}
}

@article{laborieux2021,
  title = {Synaptic Metaplasticity in Binarized Neural Networks},
  author = {Laborieux, Axel and Ernoult, Maxence and Hirtzlin, Tifenn and Querlioz, Damien},
  year = {2021},
  journal = {Nature Communications},
  volume = {12},
  number = {1},
  pages = {2549},
  publisher = {{Nature Publishing Group}},
  issn = {2041-1723},
  doi = {10/gj3bs8},
  url = {https://www.nature.com/articles/s41467-021-22768-y},
  urldate = {2021-06-11},
  abstract = {While deep neural networks have surpassed human performance in multiple situations, they are prone to catastrophic forgetting: upon training a new task, they rapidly forget previously learned ones. Neuroscience studies, based on idealized tasks, suggest that in the brain, synapses overcome this issue by adjusting their plasticity depending on their past history. However, such ``metaplastic'' behaviors do not transfer directly to mitigate catastrophic forgetting in deep neural networks. In this work, we interpret the hidden weights used by binarized neural networks, a low-precision version of deep neural networks, as metaplastic variables, and modify their training technique to alleviate forgetting. Building on this idea, we propose and demonstrate experimentally, in situations of multitask and stream learning, a training technique that reduces catastrophic forgetting without needing previously presented data, nor formal boundaries between datasets and with performance approaching more mainstream techniques with task boundaries. We support our approach with a theoretical analysis on a tractable task. This work bridges computational neuroscience and deep learning, and presents significant assets for future embedded and neuromorphic systems, especially when using novel nanodevices featuring physics analogous to metaplasticity.},
  copyright = {2021 The Author(s)},
  langid = {english},
  keywords = {/unread,\#nosource}
}

@inproceedings{lee2017,
  title = {Overcoming {{Catastrophic Forgetting}} by {{Incremental Moment Matching}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Lee, Sang-Woo and Kim, Jin-Hwa and Jun, Jaehyun and Ha, Jung-Woo and Zhang, Byoung-Tak},
  year = {2017},
  volume = {2017-Decem},
  pages = {4653--4663},
  publisher = {{Neural information processing systems foundation}},
  url = {http://arxiv.org/abs/1703.08475},
  abstract = {Catastrophic forgetting is a problem of neural networks that loses the information of the first task after training the second task. Here, we propose a method, i.e. incremental moment matching (IMM), to resolve this problem. IMM incrementally matches the moment of the posterior distribution of the neural network which is trained on the first and the second task, respectively. To make the search space of posterior parameter smooth, the IMM procedure is complemented by various transfer learning techniques including weight transfer, L2-norm of the old and the new parameter, and a variant of dropout with the old parameter. We analyze our approach on a variety of datasets including the MNIST, CIFAR-10, Caltech-UCSD-Birds, and Lifelog datasets. The experimental results show that IMM achieves state-of-the-art performance by balancing the information between an old and a new network.},
  keywords = {[bayes],[cifar],[mnist],/unread,\#nosource},
  annotation = {\_eprint: 1703.08475},
  note = {IMM is tested on Disjoint MNIST (2 tasks only), and Shuffled (i.e. permuted) MNIST (3 tasks only). The authors show that EWC does not work in Split (Disjoint) settings.}
}

@article{lee2018,
  title = {Toward {{Continual Learning}} for {{Conversational Agents}}},
  author = {Lee, Sungjin},
  year = {2018},
  journal = {arXiv},
  url = {http://arxiv.org/abs/1712.09943},
  abstract = {While end-to-end neural conversation models have led to promising advances in reducing hand-crafted features and errors induced by the traditional complex system architecture, they typically require an enormous amount of data due to the lack of modularity. Previous studies adopted a hybrid approach with knowledge-based components either to abstract out domain-specific information or to augment data to cover more diverse patterns. On the contrary, we propose to directly address the problem using recent developments in the space of continual learning for neural models. Specifically, we adopt a domain-independent neural conversational model and introduce a novel neural continual learning algorithm that allows a conversational agent to accumulate skills across different tasks in a data-efficient way. To the best of our knowledge, this is the first work that applies continual learning to conversation systems. We verified the efficacy of our method through a conversational skill transfer from either synthetic dialogs or human-human dialogs to human-computer conversations in a customer support domain.},
  keywords = {[nlp],/unread,\#nosource,⛔ No DOI found,chatbot,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Human-Computer Interaction,conversation,conversational agent,ewc,lstm},
  note = {arXiv: 1712.09943}
}

@inproceedings{lee2019,
  title = {Overcoming Catastrophic Forgetting with Unlabeled Data in the Wild},
  booktitle = {Proceedings of the {{IEEE International Conference}} on {{Computer Vision}}},
  author = {Lee, Kibok and Lee, Kimin and Shin, Jinwoo and Lee, Honglak},
  year = {2019},
  volume = {2019-Octob},
  pages = {312--321},
  publisher = {{Institute of Electrical and Electronics Engineers Inc.}},
  issn = {15505499},
  doi = {10/ghbfg8},
  url = {http://arxiv.org/abs/1903.12648},
  abstract = {Lifelong learning with deep neural networks is well-known to suffer from catastrophic forgetting: The performance on previous tasks drastically degrades when learning a new task. To alleviate this effect, we propose to leverage a large stream of unlabeled data easily obtainable in the wild. In particular, we design a novel class-incremental learning scheme with (a) a new distillation loss, termed global distillation, (b) a learning strategy to avoid overfitting to the most recent task, and (c) a confidence-based sampling method to effectively leverage unlabeled external data. Our experimental results on various datasets, including CIFAR and ImageNet, demonstrate the superiority of the proposed methods over prior methods, particularly when a stream of unlabeled data is accessible: Our method shows up to 15.8\% higher accuracy and 46.5\% less forgetting compared to the state-of-the-art method. The code is available at https://github.com/kibok90/iccv2019-inc.},
  isbn = {978-1-72814-803-8},
  keywords = {/unread,\#nosource},
  annotation = {\_eprint: 1903.12648}
}

@article{lee2020,
  title = {Clinical Applications of Continual Learning Machine Learning},
  author = {Lee, Cecilia S and Lee, Aaron Y},
  year = {2020},
  journal = {The Lancet Digital Health},
  volume = {2},
  number = {6},
  pages = {e279--e281},
  issn = {25897500},
  doi = {10/gnk4hc},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S2589750020301023},
  keywords = {/unread,\#nosource}
}

@inproceedings{lee2020a,
  title = {A {{Neural Dirichlet Process Mixture Model}} for {{Task-Free Continual Learning}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Lee, Soochan and Ha, Junsoo and Zhang, Dongsu and Kim, Gunhee},
  year = {2020},
  url = {https://openreview.net/forum?id=SJxSOJStPr},
  keywords = {/unread,\#nosource,⛔ No DOI found},
  annotation = {ZSCC: 0000058}
}

@inproceedings{lee2021,
  title = {Continual {{Learning}} in the {{Teacher-Student Setup}}: {{Impact}} of {{Task Similarity}}},
  shorttitle = {Continual {{Learning}} in the {{Teacher-Student Setup}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Lee, Sebastian and Goldt, Sebastian and Saxe, Andrew},
  year = {2021},
  pages = {6109--6119},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {http://proceedings.mlr.press/v139/lee21e.html},
  urldate = {2021-07-13},
  abstract = {Continual learning\{\textemdash\}the ability to learn many tasks in sequence\{\textemdash\}is critical for artificial learning systems. Yet standard training methods for deep networks often suffer from catastrophic forget...},
  langid = {english},
  keywords = {/unread,\#nosource}
}

@article{lenga2020,
  title = {Continual {{Learning}} for {{Domain Adaptation}} in {{Chest X-ray Classification}}},
  author = {Lenga, Matthias and Schulz, Heinrich and Saalbach, Axel},
  year = {2020},
  journal = {arXiv},
  pages = {1--11},
  url = {http://arxiv.org/abs/2001.05922},
  abstract = {Over the last years, Deep Learning has been successfully applied to a broad range of medical applications. Especially in the context of chest X-ray classification, results have been reported which are on par, or even superior to experienced radiologists. Despite this success in controlled experimental environments, it has been noted that the ability of Deep Learning models to generalize to data from a new domain (with potentially different tasks) is often limited. In order to address this challenge, we investigate techniques from the field of Continual Learning (CL) including Joint Training (JT), Elastic Weight Consolidation (EWC) and Learning Without Forgetting (LWF). Using the ChestX-ray14 and the MIMIC-CXR datasets, we demonstrate empirically that these methods provide promising options to improve the performance of Deep Learning models on a target domain and to mitigate effectively catastrophic forgetting for the source domain. To this end, the best overall performance was obtained using JT, while for LWF competitive results could be achieved - even without accessing data from the source domain.},
  keywords = {[vision],/unread,\#nosource,⛔ No DOI found,catastrophic forgetting,chest x-ray,chestx-ray14,continual learning,convolutional neural networks,elastic weight consolidation,joint training,learning without forgetting,mimic-cxr},
  annotation = {\_eprint: 2001.05922}
}

@article{lesort2018,
  title = {Generative {{Models}} from the Perspective of {{Continual Learning}}},
  author = {Lesort, Timoth{\'e}e and {Caselles-Dupr{\'e}}, Hugo and {Garcia-Ortiz}, Michael and Stoian, Andrei and Filliat, David},
  year = {2018},
  journal = {Proceedings of the International Joint Conference on Neural Networks},
  doi = {10/gnq322},
  url = {http://arxiv.org/abs/1812.09111},
  abstract = {Which generative model is the most suitable for Continual Learning? This paper aims at evaluating and comparing generative models on disjoint sequential image generation tasks. We investigate how several models learn and forget, considering various strategies: rehearsal, regularization, generative replay and fine-tuning. We used two quantitative metrics to estimate the generation quality and memory ability. We experiment with sequential tasks on three commonly used benchmarks for Continual Learning (MNIST, Fashion MNIST and CIFAR10). We found that among all models, the original GAN performs best and among Continual Learning strategies, generative replay outperforms all other methods. Even if we found satisfactory combinations on MNIST and Fashion MNIST, training generative models sequentially on CIFAR10 is particularly instable, and remains a challenge. Our code is available online \$\textbackslash backslash\$footnote\{\$\textbackslash backslash\$url\{https://github.com/TLESORT/Generative\$\textbackslash backslash\$\_Continual\$\textbackslash backslash\$\_Learning\}\}.},
  isbn = {9781728119854},
  keywords = {[cifar],[generative],[mnist],/unread,\#nosource},
  annotation = {\_eprint: 1812.09111}
}

@article{lesort2020,
  title = {Continual Learning for Robotics: {{Definition}}, Framework, Learning Strategies, Opportunities and Challenges},
  shorttitle = {Continual Learning for Robotics},
  author = {Lesort, Timoth{\'e}e and Lomonaco, Vincenzo and Stoian, Andrei and Maltoni, Davide and Filliat, David and {D{\'i}az-Rodr{\'i}guez}, Natalia},
  year = {2020},
  journal = {Information Fusion},
  volume = {58},
  pages = {52--68},
  issn = {1566-2535},
  doi = {10/gk48rb},
  url = {http://www.sciencedirect.com/science/article/pii/S1566253519307377},
  abstract = {Continual learning (CL) is a particular machine learning paradigm where the data distribution and learning objective change through time, or where all the training data and objective criteria are never available at once. The evolution of the learning process is modeled by a sequence of learning experiences where the goal is to be able to learn new skills all along the sequence without forgetting what has been previously learned. CL can be seen as an online learning where knowledge fusion needs to take place in order to learn from streams of data presented sequentially in time. Continual learning also aims at the same time at optimizing the memory, the computation power and the speed during the learning process. An important challenge for machine learning is not necessarily finding solutions that work in the real world but rather finding stable algorithms that can learn in real world. Hence, the ideal approach would be tackling the real world in a embodied platform: an autonomous agent. Continual learning would then be effective in an autonomous agent or robot, which would learn autonomously through time about the external world, and incrementally develop a set of complex skills and knowledge.Robotic agents have to learn to adapt and interact with their environment using a continuous stream of observations. Some recent approaches aim at tackling continual learning for robotics, but most recent papers on continual learning only experiment approaches in simulation or with static datasets. Unfortunately, the evaluation of those algorithms does not provide insights on whether their solutions may help continual learning in the context of robotics. This paper aims at reviewing the existing state of the art of continual learning, summarizing existing benchmarks and metrics, and proposing a framework for presenting and evaluating both robotics and non robotics approaches in a way that makes transfer between both fields easier. We put light on continual learning in the context of robotics to create connections between fields and normalize approaches.},
  langid = {english},
  keywords = {[framework],/unread,\#nosource,Catastrophic Forgetting,Continual Learning,Deep Learning,Lifelong Learning,Reinforcement Learning,Robotics},
  note = {Overview of a CL framework for applications in robotic, together with discussion of existing CL strategies and techniques.}
}

@article{lesort2020a,
  title = {Regularization {{Shortcomings}} for {{Continual Learning}}},
  author = {Lesort, Timoth{\'e}e and Stoian, Andrei and Filliat, David},
  year = {2020},
  journal = {arXiv},
  url = {http://arxiv.org/abs/1912.03049},
  abstract = {In most machine learning algorithms, training data are assumed independent and identically distributed (iid). Otherwise, the algorithms' performances are challenged. A famous phenomenon with non-iid data distribution is known as \$\textbackslash backslash\$say\{catastrophic forgetting\}. Algorithms dealing with it are gathered in the \$\textbackslash backslash\$textit\{Continual Learning\} research field. In this article, we study the \$\textbackslash backslash\$textit\{regularization\} based approaches to continual learning. We show that those approaches can not learn to discriminate classes from different tasks in an elemental continual benchmark: class-incremental setting. We make theoretical reasoning to prove this shortcoming and illustrate it with examples and experiments.},
  keywords = {[fashion],[mnist],/unread,\#nosource,⛔ No DOI found,class incremental,Computer Science - Machine Learning,regularization,Statistics - Machine Learning},
  note = {arXiv: 1912.03049}
}

@phdthesis{lesort2020b,
  title = {Continual {{Learning}}: {{Tackling Catastrophic Forgetting}} in {{Deep Neural Networks}} with {{Replay Processes}}},
  author = {Lesort, Timoth'ee},
  year = {2020},
  journal = {arXiv},
  url = {http://arxiv.org/abs/2007.00487},
  abstract = {Humans learn all their life long. They accumulate knowledge from a sequence of learning experiences and remember the essential concepts without forgetting what they have learned previously. Artificial neural networks struggle to learn similarly. They often rely on data rigorously preprocessed to learn solutions to specific problems such as classification or regression. In particular, they forget their past learning experiences if trained on new ones. Therefore, artificial neural networks are often inept to deal with real-life settings such as an autonomous-robot that has to learn on-line to adapt to new situations and overcome new problems without forgetting its past learning-experiences. Continual learning (CL) is a branch of machine learning addressing this type of problem. Continual algorithms are designed to accumulate and improve knowledge in a curriculum of learning-experiences without forgetting. In this thesis, we propose to explore continual algorithms with replay processes. Replay processes gather together rehearsal methods and generative replay methods. Generative Replay consists of regenerating past learning experiences with a generative model to remember them. Rehearsal consists of saving a core-set of samples from past learning experiences to rehearse them later. The replay processes make possible a compromise between optimizing the current learning objective and the past ones enabling learning without forgetting in sequences of tasks settings. We show that they are very promising methods for continual learning. Notably, they enable the re-evaluation of past data with new knowledge and the confrontation of data from different learning-experiences. We demonstrate their ability to learn continually through unsupervised learning, supervised learning and reinforcement learning tasks.},
  school = {EnstaParis Tech},
  keywords = {[cifar],[framework],[generative],[mnist],[vision],/unread,\#nosource},
  annotation = {\_eprint: 2007.00487},
  note = {This dissertation constitutes a valid summary of the latest effort in the use of generative models for continual learning and vice-versa.}
}

@article{lesort2021,
  ids = {lesort2021b},
  title = {Continual {{Learning}} in {{Deep Networks}}: An {{Analysis}} of the {{Last Layer}}},
  shorttitle = {Continual {{Learning}} in {{Deep Networks}}},
  author = {Lesort, Timoth{\'e}e and George, Thomas and Rish, Irina},
  year = {2021},
  journal = {arXiv},
  eprint = {2106.01834},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2106.01834},
  urldate = {2021-06-04},
  abstract = {We study how different output layer types of a deep neural network learn and forget in continual learning settings. We describe the three factors affecting catastrophic forgetting in the output layer: (1) weights modifications, (2) interferences, and (3) projection drift. Our goal is to provide more insights into how different types of output layers can address (1) and (2). We also propose potential solutions and evaluate them on several benchmarks. We show that the best-performing output layer type depends on the data distribution drifts or the amount of data available. In particular, in some cases where a standard linear layer would fail, it is sufficient to change the parametrization and get significantly better performance while still training with SGD. Our results and analysis shed light on the dynamics of the output layer in continual learning scenarios and help select the best-suited output layer for a given scenario.},
  archiveprefix = {arXiv},
  keywords = {/unread,\#nosource,⛔ No DOI found,Computer Science - Artificial Intelligence,Computer Science - Machine Learning}
}

@article{lesort2021a,
  title = {Understanding {{Continual Learning Settings}} with {{Data Distribution Drift Analysis}}},
  author = {Lesort, Timoth{\'e}e and Caccia, Massimo and Rish, Irina},
  year = {2021},
  journal = {arXiv},
  eprint = {2104.01678},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2104.01678},
  urldate = {2021-04-11},
  abstract = {Classical machine learning algorithms often assume that the data are drawn i.i.d. from a stationary probability distribution. Recently, continual learning emerged as a rapidly growing area of machine learning where this assumption is relaxed, namely, where the data distribution is non-stationary, i.e., changes over time. However, data distribution drifts may interfere with the learning process and erase previously learned knowledge; thus, continual learning algorithms must include specialized mechanisms to deal with such distribution drifts. A distribution drift may change the class labels distribution, the input distribution, or both. Moreover, distribution drifts might be abrupt or gradual. In this paper, we aim to identify and categorize different types of data distribution drifts and potential assumptions about them, to better characterize various continual-learning scenarios. Moreover, we propose to use the distribution drift framework to provide more precise definitions of several terms commonly used in the continual learning field.},
  archiveprefix = {arXiv},
  keywords = {/unread,\#nosource,⛔ No DOI found,Computer Science - Artificial Intelligence,Computer Science - Machine Learning}
}

@inproceedings{li2016,
  title = {Learning without {{Forgetting}}},
  booktitle = {European {{Conference}} on {{Computer Vision}}},
  author = {Li, Zhizhong and Hoiem, Derek},
  year = {2016},
  series = {Springer},
  pages = {614--629},
  url = {http://arxiv.org/abs/1606.09282},
  abstract = {When building a unified vision system or gradually adding new capabilities to a system, the usual assumption is that training data for all tasks is always available. However, as the number of tasks grows, storing and retraining on such data becomes infeasible. A new problem arises where we add new capabilities to a Convolutional Neural Network (CNN), but the training data for its existing capabilities are unavailable. We propose our Learning without Forgetting method, which uses only new task data to train the network while preserving the original capabilities. Our method performs favorably compared to commonly used feature extraction and fine-tuning adaption techniques and performs similarly to multitask learning that uses original task data we assume unavailable. A more surprising observation is that Learning without Forgetting may be able to replace fine-tuning with similar old and new task datasets for improved new task performance.},
  langid = {english},
  keywords = {[imagenet],/unread,\#nosource,⛔ No DOI found,Computer Science - Computer Vision and Pattern Rec,Computer Science - Machine Learning,Statistics - Machine Learning},
  note = {Comment: Conference version appears in ECCV 2016; updated with journal version arXiv: 1606.09282}
}

@article{li2019,
  title = {Continual {{Learning Using Bayesian Neural Networks}}},
  author = {Li, HongLin and Barnaghi, Payam and Enshaeifar, Shirin and Ganz, Frieder},
  year = {2019},
  journal = {arXiv},
  url = {http://arxiv.org/abs/1910.04112},
  abstract = {Continual learning models allow to learn and adapt to new changes and tasks over time. However, in continual and sequential learning scenarios in which the models are trained using different data with various distributions, neural networks tend to forget the previously learned knowledge. This phenomenon is often referred to as catastrophic forgetting. The catastrophic forgetting is an inevitable problem in continual learning models for dynamic environments. To address this issue, we propose a method, called Continual Bayesian Learning Networks (CBLN), which enables the networks to allocate additional resources to adapt to new tasks without forgetting the previously learned tasks. Using a Bayesian Neural Network, CBLN maintains a mixture of Gaussian posterior distributions that are associated with different tasks. The proposed method tries to optimise the number of resources that are needed to learn each task and avoids an exponential increase in the number of resources that are involved in learning multiple tasks. The proposed method does not need to access the past training data and can choose suitable weights to classify the data points during the test time automatically based on an uncertainty criterion. We have evaluated our method on the MNIST and UCR time-series datasets. The evaluation results show that our method can address the catastrophic forgetting problem at a promising rate compared to the state-of-the-art models.},
  keywords = {[bayes],[mnist],/unread,\#nosource,⛔ No DOI found,Bayesian neural networks,continual learning,in-cremental learning,Index Terms-Catastrophic forgetting,uncertainty},
  annotation = {\_eprint: 1910.04112}
}

@article{li2019a,
  title = {Learn to Grow: {{A}} Continual Structure Learning Framework for Overcoming Catastrophic Forgetting},
  author = {Li, Xilai and Zhou, Yingbo and Wu, Tianfu and Socher, Richard and Xiong, Caiming},
  year = {2019},
  journal = {arXiv},
  url = {https://arxiv.org/pdf/1904.00310.pdf},
  abstract = {Addressing catastrophic forgetting is one of the key challenges in continual learning where machine learning systems are trained with sequential or streaming tasks. Despite recent remarkable progress in state-of-the-art deep learning, deep neural networks (DNNs) are still plagued with the catastrophic forgetting problem. This paper presents a conceptually simple yet general and effective framework for handling catastrophic forgetting in continual learning with DNNs. The proposed method consists of two components: a neural structure optimization component and a parameter learning and/or fine-tuning component. By separating the explicit neural structure learning and the parameter estimation, not only is the proposed method capable of evolving neural structures in an intuitively meaningful way, but also shows strong capabilities of alleviating catastrophic forgetting in experiments. Furthermore, the proposed method outperforms all other baselines on the permuted MNIST dataset, the split CIFAR100 dataset and the Visual Domain Decathlon dataset in continual learning setting},
  keywords = {[cifar],[mnist],/unread,\#nosource,⛔ No DOI found},
  annotation = {\_eprint: 1904.00310}
}

@inproceedings{li2020,
  title = {Compositional {{Language Continual Learning}}},
  booktitle = {Eighth {{International Conference}} on {{Learning Representations}}},
  author = {Li, Yuanpeng and Zhao, Liang and Church, Kenneth and Elhoseiny, Mohamed},
  year = {2020},
  url = {https://iclr.cc/virtual_2020/poster_rklnDgHtDS.html},
  urldate = {2021-01-01},
  abstract = {Motivated by the human's ability to continually learn and gain knowledge over time, several research efforts have been pushing the limits of machines to constantly learn while alleviating catastrophic forgetting. Most of the existing methods have been focusing on continual learning of label prediction tasks, which have fixed input and output sizes. In this paper, we propose a new scenario of continual learning which handles sequence-to-sequence tasks common in language learning. We further propose an approach to use label prediction continual learning algorithm for sequence-to-sequence continual learning by leveraging compositionality. Experimental results show that the proposed method has significant improvement over state-of-the-art methods. It enables knowledge transfer and prevents catastrophic forgetting, resulting in more than 85\% accuracy up to 100 stages, compared with less than 50\% accuracy for baselines in instruction learning task. It also shows significant improvement in machine translation task. This is the first work to combine continual learning and compositionality for language learning, and we hope this work will make machines more helpful in various tasks.},
  langid = {english},
  keywords = {[nlp],[rnn],/unread,\#nosource}
}

@article{li2020a,
  title = {Energy-{{Based Models}} for {{Continual Learning}}},
  author = {Li, Shuang and Du, Yilun and {van de Ven}, Gido M. and Torralba, Antonio and Mordatch, Igor},
  year = {2020},
  journal = {arXiv},
  url = {http://arxiv.org/abs/2011.12216},
  abstract = {We motivate Energy-Based Models (EBMs) as a promising model class for continual learning problems. Instead of tackling continual learning via the use of external memory, growing models, or regularization, EBMs have a natural way to support a dynamically-growing number of tasks or classes that causes less interference with previously learned information. We find that EBMs outperform the baseline methods by a large margin on several continual learning benchmarks. We also show that EBMs are adaptable to a more general continual learning setting where the data distribution changes without the notion of explicitly delineated tasks. These observations point towards EBMs as a class of models naturally inclined towards the continual learning regime.},
  keywords = {[cifar],[experimental],[mnist],/unread,\#nosource,⛔ No DOI found},
  annotation = {\_eprint: 2011.12216},
  note = {The paper introduces Energy-Based models for classification in single incremental task + new classes (i.e. class incremental) scenarios. The model does not require task labels at test time, nor task boundaries at training time. It does not make use of replay.}
}

@article{li2020b,
  title = {Continual {{Learning Using Task Conditional Neural Networks}}},
  author = {Li, Honglin and Barnaghi, Payam and Enshaeifar, Shirin and Ganz, Frieder},
  year = {2020},
  journal = {arXiv},
  url = {http://arxiv.org/abs/2005.05080},
  abstract = {Conventional deep learning models have limited capacity in learning multiple tasks sequentially. The issue of forgetting the previously learned tasks in continual learning is known as catastrophic forgetting or interference. When the input data or the goal of learning change, a continual model will learn and adapt to the new status. However, the model will not remember or recognise any revisits to the previous states. This causes performance reduction and re-training curves in dealing with periodic or irregularly reoccurring changes in the data or goals. The changes in goals or data are referred to as new tasks in a continual learning model. Most of the continual learning methods have a task-known setup in which the task identities are known in advance to the learning model. We propose Task Conditional Neural Networks (TCNN) that does not require to known the reoccurring tasks in advance. We evaluate our model on standard datasets using MNIST and CIFAR10, and also a real-world dataset that we have collected in a remote healthcare monitoring study (i.e. TIHM dataset). The proposed model outperforms the state-of-the-art solutions in continual learning and adapting to new tasks that are not defined in advance.},
  keywords = {[cifar],[mnist],/unread,\#nosource,⛔ No DOI found},
  annotation = {\_eprint: 2005.05080}
}

@article{li2022,
  title = {Provable and {{Efficient Continual Representation Learning}}},
  author = {Li, Yingcong and Li, Mingchen and Asif, M. Salman and Oymak, Samet},
  year = {2022},
  journal = {arXiv},
  eprint = {2203.02026},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2203.02026},
  urldate = {2022-05-03},
  abstract = {In continual learning (CL), the goal is to design models that can learn a sequence of tasks without catastrophic forgetting. While there is a rich set of techniques for CL, relatively little understanding exists on how representations built by previous tasks benefit new tasks that are added to the network. To address this, we study the problem of continual representation learning (CRL) where we learn an evolving representation as new tasks arrive. Focusing on zero-forgetting methods where tasks are embedded in subnetworks (e.g., PackNet), we first provide experiments demonstrating CRL can significantly boost sample efficiency when learning new tasks. To explain this, we establish theoretical guarantees for CRL by providing sample complexity and generalization error bounds for new tasks by formalizing the statistical benefits of previously-learned representations. Our analysis and experiments also highlight the importance of the order in which we learn the tasks. Specifically, we show that CL benefits if the initial tasks have large sample size and high "representation diversity". Diversity ensures that adding new tasks incurs small representation mismatch and can be learned with few samples while training only few additional nonzero weights. Finally, we ask whether one can ensure each task subnetwork to be efficient during inference time while retaining the benefits of representation learning. To this end, we propose an inference-efficient variation of PackNet called Efficient Sparse PackNet (ESPN) which employs joint channel \& weight pruning. ESPN embeds tasks in channel-sparse subnets requiring up to 80\% less FLOPs to compute while approximately retaining accuracy and is very competitive with a variety of baselines. In summary, this work takes a step towards data and compute-efficient CL with a representation learning perspective. GitHub page: https://github.com/ucr-optml/CtRL},
  archiveprefix = {arXiv},
  keywords = {/unread,⛔ No DOI found,Computer Science - Machine Learning},
  annotation = {ZSCC:00000}
}

@article{liu2017,
  title = {Lifelong Machine Learning: A Paradigm for Continuous Learning},
  shorttitle = {Lifelong Machine Learning},
  author = {Liu, Bing},
  year = {2017},
  journal = {Frontiers of Computer Science},
  volume = {11},
  number = {3},
  pages = {359--361},
  issn = {2095-2236},
  doi = {10/gnq33r},
  url = {https://doi.org/10.1007/s11704-016-6903-6},
  urldate = {2021-09-14},
  langid = {english},
  keywords = {/unread,\#nosource}
}

@inproceedings{liu2018,
  title = {Rotate Your {{Networks}}: {{Better Weight Consolidation}} and {{Less Catastrophic Forgetting}}},
  shorttitle = {Rotate Your {{Networks}}},
  booktitle = {2018 24th {{International Conference}} on {{Pattern Recognition}} ({{ICPR}})},
  author = {Liu, Xialei and Masana, Marc and Herranz, Luis and {Van de Weijer}, Joost and Lopez, Antonio M. and Bagdanov, Andrew D},
  year = {2018},
  pages = {2262--2268},
  publisher = {{IEEE}},
  doi = {10/gnq326},
  url = {https://ieeexplore.ieee.org/document/8545895/},
  abstract = {In this paper we propose an approach to avoiding catastrophic forgetting in sequential task learning scenarios. Our technique is based on a network reparameterization that approximately diagonalizes the Fisher Information Matrix of the network parameters. This reparameterization takes the form of a factorized rotation of parameter space which, when used in conjunction with Elastic Weight Consolidation (which assumes a diagonal Fisher Information Matrix), leads to significantly better performance on lifelong learning of sequential tasks. Experimental results on the MNIST, CIFAR-100, CUB-200 and Stanford-40 datasets demonstrate that we significantly improve the results of standard elastic weight consolidation, and that we obtain competitive results when compared to the state-of-the-art in lifelong learning without forgetting.},
  isbn = {978-1-5386-3788-3},
  keywords = {[cifar],[mnist],/unread,\#nosource,Computer vision,Data models,ewc,fisher,Fisher Information Matrix,image classification,learning (artificial intelligence),matrix algebra,network parameters,network reparameterization,Neural networks,sequential tasks,standard elastic weight consolidation,Standards,Stanford-40 datasets,Task analysis,Training,Training data},
  note = {ISSN: 1051-4651}
}

@inproceedings{liu2019,
  title = {Continual {{Learning}} for {{Sentence Representations Using Conceptors}}},
  booktitle = {{{NAACL}}},
  author = {Liu, Tianlin and Ungar, Lyle and Sedoc, Jo{\~a}o},
  year = {2019},
  doi = {10/gnq33s},
  url = {http://arxiv.org/abs/1904.09187},
  abstract = {Distributed representations of sentences have become ubiquitous in natural language processing tasks. In this paper, we consider a continual learning scenario for sentence representations: Given a sequence of corpora, we aim to optimize the sentence encoder with respect to the new corpus while maintaining its accuracy on the old corpora. To address this problem, we propose to initialize sentence encoders with the help of corpus-independent features, and then sequentially update sentence encoders using Boolean operations of conceptor matrices to learn corpus-dependent features. We evaluate our approach on semantic textual similarity tasks and show that our proposed sentence encoder can continually learn features from new corpora while retaining its competence on previously encountered corpora.},
  keywords = {[nlp],/unread,\#nosource},
  annotation = {\_eprint: 1904.09187}
}

@article{liu2020,
  title = {Mnemonics {{Training}}: {{Multi-Class Incremental Learning}} without {{Forgetting}}},
  author = {Liu, Yaoyao and Liu, An-An and Su, Yuting and Schiele, Bernt and Sun, Qianru},
  year = {2020},
  journal = {arXiv},
  url = {http://arxiv.org/abs/2002.10211},
  abstract = {Multi-Class Incremental Learning (MCIL) aims to learn new concepts by incrementally updating a model trained on previous concepts. However, there is an inherent trade-off to effectively learning new concepts without catastrophic forgetting of previous ones. To alleviate this issue, it has been proposed to keep around a few examples of the previous concepts but the effectiveness of this approach heavily depends on the representativeness of these examples. This paper proposes a novel and automatic framework we call mnemonics, where we parameterize exemplars and make them optimizable in an end-to-end manner. We train the framework through bilevel optimizations, i.e., model-level and exemplar-level. We conduct extensive experiments on three MCIL benchmarks, CIFAR-100, ImageNet-Subset and ImageNet, and show that using mnemonics exemplars can surpass the state-of-the-art by a large margin. Interestingly and quite intriguingly, the mnemonics exemplars tend to be on the boundaries between different classes.},
  keywords = {[cifar],[imagenet],/unread,\#nosource,⛔ No DOI found},
  annotation = {\_eprint: 2002.10211}
}

@article{liu2020a,
  title = {Continual {{Universal Object Detection}}},
  author = {Liu, Xialei and Yang, Hao and Ravichandran, Avinash and Bhotika, Rahul and Soatto, Stefano},
  year = {2020},
  journal = {arXiv},
  url = {http://arxiv.org/abs/2002.05347},
  abstract = {Object detection has improved significantly in recent years on multiple challenging benchmarks. However, most existing detectors are still domain-specific, where the models are trained and tested on a single domain. When adapting these detectors to new domains, they often suffer from catastrophic forgetting of previous knowledge. In this paper, we propose a continual object detector that can learn sequentially from different domains without forgetting. First, we explore learning the object detector continually in different scenarios across various domains and categories. Learning from the analysis, we propose attentive feature distillation leveraging both bottom-up and top-down attentions to mitigate forgetting. It takes advantage of attention to ignore the noisy background information and feature distillation to provide strong supervision. Finally, for the most challenging scenarios, we propose an adaptive exemplar sampling method to leverage exemplars from previous tasks for less forgetting effectively. The experimental results show the excellent performance of our proposed method in three different scenarios across seven different object detection datasets.},
  keywords = {/unread,\#nosource,⛔ No DOI found},
  annotation = {\_eprint: 2002.05347}
}

@inproceedings{lomonaco2017,
  title = {{{CORe50}}: A {{New Dataset}} and {{Benchmark}} for {{Continuous Object Recognition}}},
  booktitle = {Proceedings of the 1st {{Annual Conference}} on {{Robot Learning}}},
  author = {Lomonaco, Vincenzo and Maltoni, Davide},
  editor = {Levine, Sergey and Vanhoucke, Vincent and Goldberg, Ken},
  year = {2017},
  series = {Proceedings of {{Machine Learning Research}}},
  volume = {78},
  pages = {17--26},
  publisher = {{PMLR}},
  url = {http://proceedings.mlr.press/v78/lomonaco17a.html},
  abstract = {Continuous/Lifelong learning of high-dimensional data streams is a challenging research problem. In fact, fully retraining models each time new data become available is infeasible, due to computational and storage issues, while na\"ive incremental strategies have been shown to suffer from catastrophic forgetting. In the context of real-world object recognition applications (e.g., robotic vision), where continuous learning is crucial, very few datasets and benchmarks are available to evaluate and compare emerging techniques. In this work we propose a new dataset and benchmark CORe50, specifically designed for continuous object recognition, and introduce baseline approaches for different continuous learning scenarios.},
  keywords = {[vision],/unread,\#nosource}
}

@phdthesis{lomonaco2019,
  title = {{Continual Learning with Deep Architectures}},
  author = {Lomonaco, Vincenzo},
  year = {2019},
  journal = {University of Bologna},
  doi = {10.6092/unibo/amsdottorato/9073},
  url = {http://amsdottorato.unibo.it/9073/},
  abstract = {Humans have the extraordinary ability to learn continually from experience. Not only we can apply previously learned knowledge and skills to new situations, we can also use these as the foundation for later learning. One of the grand goals of Artificial Intelligence (AI) is building an artificial ``continual learning'' agent that constructs a sophisticated understanding of the world from its own experience through the autonomous incremental development of ever more complex knowledge and skills. However, despite early speculations and few pioneering works, very little research and effort has been devoted to address this vision. Current AI systems greatly suffer from the exposure to new data or environments which even slightly differ from the ones for which they have been trained for. Moreover, the learning process is usually constrained on fixed datasets within narrow and isolated tasks which may hardly lead to the emergence of more complex and autonomous intelligent behaviors. In essence, continual learning and adaptation capabilities, while more than often thought as fundamental pillars of every intelligent agent, have been mostly left out of the main AI research focus. In this dissertation, we study the application of these ideas in light of the more recent advances in machine learning research and in the context of deep architectures for AI. We propose a comprehensive and unifying framework for continual learning, new metrics, benchmarks and algorithms, as well as providing substantial experimental evaluations in different supervised, unsupervised and reinforcement learning tasks.},
  langid = {italian},
  school = {alma},
  keywords = {[core50],[framework],/unread,\#nosource}
}

@inproceedings{lomonaco2020,
  title = {Rehearsal-{{Free Continual Learning}} over {{Small Non-I}}.{{I}}.{{D}}. {{Batches}}},
  booktitle = {{{CVPR Workshop}} on {{Continual Learning}} for {{Computer Vision}}},
  author = {Lomonaco, Vincenzo and Maltoni, Davide and Pellegrini, Lorenzo},
  year = {2020},
  pages = {246--247},
  url = {https://openaccess.thecvf.com/content_CVPRW_2020/html/w15/Lomonaco_Rehearsal-Free_Continual_Learning_Over_Small_Non-I.I.D._Batches_CVPRW_2020_paper.html},
  abstract = {Robotic vision is a field where continual learning can play a significant role. An embodied agent operating in a complex environment subject to frequent and unpredictable changes is required to learn and adapt continuously. In the context of object recognition, for example, a robot should be able to learn (without forgetting) objects of never before seen classes as well as improving its recognition capabilities as new instances of already known classes are discovered. Ideally, continual learning should be triggered by the availability of short videos of single objects and performed on-line on on-board hardware with fine-grained updates. In this paper, we introduce a novel continual learning protocol based on the CORe50 benchmark and propose two rehearsal-free continual learning techniques, CWR* and AR1*, that can learn effectively even in the challenging case of nearly 400 small non-i.i.d. incremental batches. In particular, our experiments show that AR1* can outperform other state-of-the-art rehearsal-free techniques by more than 15\% accuracy in some cases, with a very light and constant computational and memory overhead across training batches.},
  keywords = {[core50],/unread,\#nosource,⛔ No DOI found}
}

@inproceedings{lomonaco2020a,
  title = {Continual {{Reinforcement Learning}} in {{3D Non-Stationary Environments}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}}},
  author = {Lomonaco, Vincenzo and Desai, Karan and Culurciello, Eugenio and Maltoni, Davide},
  year = {2020},
  pages = {248--249},
  url = {https://openaccess.thecvf.com/content_CVPRW_2020/html/w15/Lomonaco_Continual_Reinforcement_Learning_in_3D_Non-Stationary_Environments_CVPRW_2020_paper.html},
  abstract = {High-dimensional always-changing environments constitute a hard challenge for current reinforcement learning techniques. Artificial agents, nowadays, are often trained off-line in very static and controlled conditions in simulation such that training observations can be thought as sampled i.i.d. from the entire observations space. However, in real world settings, the environment is often non-stationary and subject to unpredictable, frequent changes. In this paper we propose and openly release CRLMaze, a new benchmark for learning continually through reinforcement in a complex 3D non-stationary task based on ViZDoom and subject to several environmental changes. Then, we introduce an end-to-end model-free continual reinforcement learning strategy showing competitive results with respect to four different baselines and not requiring any access to additional supervised signals, previously encountered environmental conditions or observations.},
  keywords = {/unread,\#nosource,⛔ No DOI found}
}

@inproceedings{lomonaco2021,
  title = {Avalanche: An {{End-to-End Library}} for {{Continual Learning}}},
  shorttitle = {Avalanche},
  booktitle = {{{CLVision Workshop}} at {{CVPR}}},
  author = {Lomonaco, Vincenzo and Pellegrini, Lorenzo and Cossu, Andrea and Carta, Antonio and Graffieti, Gabriele and Hayes, Tyler L. and De Lange, Matthias and Masana, Marc and Pomponi, Jary and {van de Ven}, Gido and Mundt, Martin and She, Qi and Cooper, Keiland and Forest, Jeremy and Belouadah, Eden and Calderara, Simone and Parisi, German I. and Cuzzolin, Fabio and Tolias, Andreas and Scardapane, Simone and Antiga, Luca and Amhad, Subutai and Popescu, Adrian and Kanan, Christopher and {van de Weijer}, Joost and Tuytelaars, Tinne and Bacciu, Davide and Maltoni, Davide},
  year = {2021},
  eprint = {2104.00405},
  eprinttype = {arxiv},
  doi = {10/gnk4t8},
  url = {http://arxiv.org/abs/2104.00405},
  urldate = {2021-04-19},
  abstract = {Learning continually from non-stationary data streams is a long-standing goal and a challenging problem in machine learning. Recently, we have witnessed a renewed and fast-growing interest in continual learning, especially within the deep learning community. However, algorithmic solutions are often difficult to re-implement, evaluate and port across different settings, where even results on standard benchmarks are hard to reproduce. In this work, we propose Avalanche, an open-source end-to-end library for continual learning research based on PyTorch. Avalanche is designed to provide a shared and collaborative codebase for fast prototyping, training, and reproducible evaluation of continual learning algorithms.},
  archiveprefix = {arXiv},
  copyright = {All rights reserved},
  keywords = {/unread,\#nosource,Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  note = {Comment: Official Website: https://avalanche.continualai.org}
}

@inproceedings{lopez-paz2017,
  title = {Gradient {{Episodic Memory}} for {{Continual Learning}}},
  booktitle = {{{NIPS}}},
  author = {{Lopez-Paz}, David and Ranzato, Marc'Aurelio},
  year = {2017},
  url = {https://arxiv.org/abs/1706.08840},
  abstract = {One major obstacle towards AI is the poor ability of models to solve new problems quicker, and without forgetting previously acquired knowledge. To better understand this issue, we study the problem of continual learning, where the model observes, once and one by one, examples concerning a sequence of tasks. First, we propose a set of metrics to evaluate models learning over a continuum of data. These metrics characterize models not only by their test accuracy, but also in terms of their ability to transfer knowledge across tasks. Second, we propose a model for continual learning, called Gradient Episodic Memory (GEM) that alleviates forgetting, while allowing beneficial transfer of knowledge to previous tasks. Our experiments on variants of the MNIST and CIFAR-100 datasets demonstrate the strong performance of GEM when compared to the state-of-the-art.},
  keywords = {[cifar],[mnist],/unread,\#nosource,⛔ No DOI found,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,gem},
  note = {Comment: Published at NIPS 2017 arXiv: 1706.08840}
}

@article{losing2018,
  title = {Incremental On-Line Learning: {{A}} Review and Comparison of State of the Art Algorithms},
  author = {Losing, Viktor and Hammer, Barbara and Wersing, Heiko},
  year = {2018},
  journal = {Neurocomputing},
  volume = {275},
  pages = {1261--1274},
  publisher = {{Elsevier B.V.}},
  issn = {18728286},
  doi = {10/gcs6kh},
  url = {https://doi.org/10.1016/j.neucom.2017.06.084},
  abstract = {Recently, incremental and on-line learning gained more attention especially in the context of big data and learning from data streams, conflicting with the traditional assumption of complete data availability. Even though a variety of different methods are available, it often remains unclear which of them is suitable for a specific task and how they perform in comparison to each other. We analyze the key properties of eight popular incremental methods representing different algorithm classes. Thereby, we evaluate them with regards to their on-line classification error as well as to their behavior in the limit. Further, we discuss the often neglected issue of hyperparameter optimization specifically for each method and test how robustly it can be done based on a small set of examples. Our extensive evaluation on data sets with different characteristics gives an overview of the performance with respect to accuracy, convergence speed as well as model complexity, facilitating the choice of the best method for a given application.},
  keywords = {/unread,\#nosource,Data streams,Hyperparameter optimization,Incremental learning,Model selection,On-line learning}
}

@inproceedings{luders2016,
  title = {Continual Learning through Evolvable Neural Turing Machines},
  booktitle = {{{NIPS}} 2016 {{Workshop}} on {{Continual Learning}} and {{Deep Networks}}},
  author = {Luders, Benno and Schlager, Mikkel and Risi, Sebastian},
  year = {2016},
  url = {https://core.ac.uk/reader/84859350},
  abstract = {Continual learning, i.e. the ability to sequentially learn tasks without catastrophicforgetting of previously learned ones, is an important open challenge in machinelearning. In this paper we take a step in this direction by showing that the recentlyproposedEvolving Neural Turing Machine(ENTM) approach is able to performone-shot learningin a reinforcement learning task without catastrophic forgettingof previously stored associations.},
  keywords = {/unread,\#nosource,⛔ No DOI found}
}

@inproceedings{luders2017,
  title = {Continual and {{One-Shot Learning Through Neural Networks}} with {{Dynamic External Memory}}},
  booktitle = {Applications of {{Evolutionary Computation}}},
  author = {L{\"u}ders, Benno and Schl{\"a}ger, Mikkel and Korach, Aleksandra and Risi, Sebastian},
  editor = {Squillero, Giovanni and Sim, Kevin},
  year = {2017},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {886--901},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10/gnq33j},
  url = {https://link.springer.com/chapter/10.1007/978-3-319-55849-3_57},
  abstract = {Training neural networks to quickly learn new skills without forgetting previously learned skills is an important open challenge in machine learning. A common problem for adaptive networks that can learn during their lifetime is that the weights encoding a particular task are often overridden when a new task is learned. This paper takes a step in overcoming this limitation by building on the recently proposed Evolving Neural Turing Machine (ENTM) approach. In the ENTM, neural networks are augmented with an external memory component that they can write to and read from, which allows them to store associations quickly and over long periods of time. The results in this paper demonstrate that the ENTM is able to perform one-shot learning in reinforcement learning tasks without catastrophic forgetting of previously stored associations. Additionally, we introduce a new ENTM default jump mechanism that makes it easier to find unused memory location and therefor facilitates the evolution of continual learning networks. Our results suggest that augmenting evolving networks with an external memory component is not only a viable mechanism for adaptive behaviors in neuroevolution but also allows these networks to perform continual and one-shot learning at the same time.},
  isbn = {978-3-319-55849-3},
  langid = {english},
  keywords = {/unread,\#nosource,Adaptive neural networks,Continual learning,Memory,Neural Turing Machine,Neuroevolution,Plasticity}
}

@inproceedings{luders2017a,
  title = {Continual and {{One-Shot Learning Through Neural Networks}} with {{Dynamic External Memory}}},
  booktitle = {Applications of {{Evolutionary Computation}}},
  author = {L{\"u}ders, Benno and Schl{\"a}ger, Mikkel and Korach, Aleksandra and Risi, Sebastian},
  editor = {Squillero, Giovanni and Sim, Kevin},
  year = {2017},
  pages = {886--901},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10/gnq33j},
  url = {https://link.springer.com/chapter/10.1007/978-3-319-55849-3_57},
  abstract = {Training neural networks to quickly learn new skills without forgetting previously learned skills is an important open challenge in machine learning. A common problem for adaptive networks that can learn during their lifetime is that the weights encoding a particular task are often overridden when a new task is learned. This paper takes a step in overcoming this limitation by building on the recently proposed Evolving Neural Turing Machine (ENTM) approach. In the ENTM, neural networks are augmented with an external memory component that they can write to and read from, which allows them to store associations quickly and over long periods of time. The results in this paper demonstrate that the ENTM is able to perform one-shot learning in reinforcement learning tasks without catastrophic forgetting of previously stored associations. Additionally, we introduce a new ENTM default jump mechanism that makes it easier to find unused memory location and therefor facilitates the evolution of continual learning networks. Our results suggest that augmenting evolving networks with an external memory component is not only a viable mechanism for adaptive behaviors in neuroevolution but also allows these networks to perform continual and one-shot learning at the same time.},
  isbn = {978-3-319-55849-3},
  langid = {english},
  keywords = {/unread}
}

@inproceedings{madaan2021,
  title = {Rethinking the {{Representational Continuity}}: {{Towards Unsupervised Continual Learning}}},
  shorttitle = {Rethinking the {{Representational Continuity}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Madaan, Divyam and Yoon, Jaehong and Li, Yuanchun and Liu, Yunxin and Hwang, Sung Ju},
  year = {2021},
  url = {https://openreview.net/forum?id=9Hrka5PA7LW},
  urldate = {2022-02-22},
  abstract = {Continual learning (CL) aims to learn a sequence of tasks without forgetting the previously acquired knowledge. However, recent CL advances are restricted to supervised continual learning (SCL)...},
  langid = {english},
  keywords = {/unread}
}

@article{madasu2020,
  title = {Sequential {{Domain Adaptation}} through {{Elastic Weight Consolidation}} for {{Sentiment Analysis}}},
  author = {Madasu, Avinash and Rao, Vijjini Anvesh},
  year = {2020},
  journal = {arXiv},
  eprint = {2007.01189},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2007.01189},
  urldate = {2021-01-08},
  abstract = {Elastic Weight Consolidation (EWC) is a technique used in overcoming catastrophic forgetting between successive tasks trained on a neural network. We use this phenomenon of information sharing between tasks for domain adaptation. Training data for tasks such as sentiment analysis (SA) may not be fairly represented across multiple domains. Domain Adaptation (DA) aims to build algorithms that leverage information from source domains to facilitate performance on an unseen target domain. We propose a model-independent framework - Sequential Domain Adaptation (SDA). SDA draws on EWC for training on successive source domains to move towards a general domain solution, thereby solving the problem of domain adaptation. We test SDA on convolutional, recurrent, and attention-based architectures. Our experiments show that the proposed framework enables simple architectures such as CNNs to outperform complex state-of-the-art models in domain adaptation of SA. In addition, we observe that the effectiveness of a harder first Anti-Curriculum ordering of source domains leads to maximum performance.},
  archiveprefix = {arXiv},
  keywords = {[nlp],[rnn],/unread,\#nosource,⛔ No DOI found,Computer Science - Computation and Language},
  note = {Comment: Accepted at 25th International Conference on Pattern Recognition, January 2021, Milan, Italy}
}

@article{madrid2019,
  title = {Towards {{AutoML}} in the Presence of {{Drift}}: First Results},
  author = {Madrid, Jorge G. and Escalante, Hugo Jair and Morales, Eduardo F. and Tu, Wei-Wei and Yu, Yang and {Sun-Hosoya}, Lisheng and Guyon, Isabelle and Sebag, Michele},
  year = {2019},
  journal = {arXiv},
  url = {http://arxiv.org/abs/1907.10772},
  abstract = {Research progress in AutoML has lead to state of the art solutions that can cope quite wellwith supervised learning task, e.g., classification with AutoSklearn. However, so far thesesystems do not take into account the changing nature of evolving data over time (i.e., theystill assume i.i.d. data); even when this sort of domains are increasingly available in realapplications (e.g., spam filtering, user preferences, etc.). We describe a first attempt to de-velop an AutoML solution for scenarios in which data distribution changes relatively slowlyover time and in which the problem is approached in a lifelong learning setting. We extendAuto-Sklearn with sound and intuitive mechanisms that allow it to cope with this sort ofproblems. The extended Auto-Sklearn is combined with concept drift detection techniquesthat allow it to automatically determine when the initial models have to be adapted. Wereport experimental results in benchmark data from AutoML competitions that adhere tothis scenario. Results demonstrate the effectiveness of the proposed methodology.},
  keywords = {/unread,\#nosource,⛔ No DOI found},
  annotation = {\_eprint: 1907.10772}
}

@inproceedings{mallya2018,
  title = {Piggyback: {{Adapting}} a {{Single Network}} to {{Multiple Tasks}} by {{Learning}} to {{Mask Weights}}},
  booktitle = {{{ECCV}}},
  author = {Mallya, Arun and Davis, Dillon and Lazebnik, Svetlana},
  year = {2018},
  pages = {72--88},
  publisher = {{Springer Verlag}},
  issn = {16113349},
  doi = {10/gqjr2k},
  url = {https://doi.org/10.1007/978-3-030-01225-0_5},
  abstract = {This work presents a method for adapting a single, fixed deep neural network to multiple tasks without affecting performance on already learned tasks. By building upon ideas from network quantization and pruning, we learn binary masks that ``piggyback'' on an existing network, or are applied to unmodified weights of that network to provide good performance on a new task. These masks are learned in an end-to-end differentiable fashion, and incur a low overhead of 1 bit per network parameter, per task. Even though the underlying network is fixed, the ability to mask individual weights allows for the learning of a large number of filters. We show performance comparable to dedicated fine-tuned networks for a variety of classification tasks, including those with large domain shifts from the initial task (ImageNet), and a variety of network architectures. Our performance is agnostic to task ordering and we do not suffer from catastrophic forgetting or competition between tasks.},
  isbn = {978-3-030-01224-3},
  keywords = {[imagenet],/unread,\#nosource,Binary networks,Incremental learning},
  annotation = {\_eprint: 1801.06519}
}

@inproceedings{maltoni2016,
  title = {Semi-Supervised Tuning from Temporal Coherence},
  booktitle = {2016 23rd {{International Conference}} on {{Pattern Recognition}} ({{ICPR}})},
  author = {Maltoni, Davide and Lomonaco, Vincenzo},
  year = {2016},
  pages = {2509--2514},
  doi = {10/gnq33p},
  url = {https://ieeexplore.ieee.org/document/7900013},
  abstract = {Recent works demonstrated the usefulness of temporal coherence to regularize supervised training or to learn invariant features with deep architectures. In particular, enforcing a smooth output change while presenting temporally-closed frames from video sequences, proved to be an effective strategy. In this paper we prove the efficacy of temporal coherence for semi-supervised incremental tuning. We show that a deep architecture, just mildly trained in a supervised manner, can progressively improve its classification accuracy, if exposed to video sequences of unlabeled data. The extent to which, in some cases, a semi-supervised tuning allows to improve classification accuracy (approaching the supervised one) is somewhat surprising. A number of control experiments pointed out the fundamental role of temporal coherence.},
  keywords = {/unread,\#nosource,Cameras,Coherence,deep learning,Feature extraction,incremental tuning,Optimization,self-training,temporal coherence,Training,Tuning,Video sequences}
}

@article{maltoni2019,
  title = {Continuous {{Learning}} in {{Single-Incremental-Task Scenarios}}},
  author = {Maltoni, Davide and Lomonaco, Vincenzo},
  year = {2019},
  journal = {Neural Networks},
  volume = {116},
  pages = {56--73},
  doi = {10/gk79q3},
  url = {http://arxiv.org/abs/1806.08568},
  abstract = {It was recently shown that architectural, regularization and rehearsal strategies can be used to train deep models sequentially on a number of disjoint tasks without forgetting previously acquired knowledge. However, these strategies are still unsatisfactory if the tasks are not disjoint but constitute a single incremental task (e.g., class-incremental learning). In this paper we point out the differences between multi-task and single-incremental-task scenarios and show that well-known approaches such as LWF, EWC and SI are not ideal for incremental task scenarios. A new approach, denoted as AR1, combining architectural and regularization strategies is then specifically proposed. AR1 overhead (in terms of memory and computation) is very small thus making it suitable for online learning. When tested on CORe50 and iCIFAR-100, AR1 outperformed existing regularization strategies by a good margin.},
  langid = {english},
  keywords = {[core50],[framework],/unread,\#nosource,Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Rec,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computi,Continuous learning,Deep learning,ewc,Incremental class learning,incremental task,Lifelong learning,Object recognition,review,Single-incremental-task,Statistics - Machine Learning},
  note = {Comment: 26 pages, 13 figures; v3: major revision (e.g. added Sec. 4.4), several typos and minor mistakes corrected arXiv: 1806.08568}
}

@article{mancini2018,
  title = {Adding {{New Tasks}} to a {{Single Network}} with {{Weight Transformations}} Using {{Binary Masks}}},
  author = {Mancini, Massimiliano and Ricci, Elisa and Caputo, Barbara and Bul{\`o}, Samuel Rota},
  year = {2018},
  journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  volume = {11130 LNCS},
  pages = {180--189},
  issn = {16113349},
  doi = {10/gnq323},
  url = {http://arxiv.org/abs/1805.11119},
  abstract = {Visual recognition algorithms are required today to exhibit adaptive abilities. Given a deep model trained on a specific, given task, it would be highly desirable to be able to adapt incrementally to new tasks, preserving scalability as the number of new tasks increases, while at the same time avoiding catastrophic forgetting issues. Recent work has shown that masking the internal weights of a given original conv-net through learned binary variables is a promising strategy. We build upon this intuition and take into account more elaborated affine transformations of the convolutional weights that include learned binary masks. We show that with our generalization it is possible to achieve significantly higher levels of adaptation to new tasks, enabling the approach to compete with fine tuning strategies by requiring slightly more than 1 bit per network parameter per additional task. Experiments on two popular benchmarks showcase the power of our approach, that achieves the new state of the art on the Visual Decathlon Challenge.},
  isbn = {9783030110116},
  keywords = {[sparsity],[vision],/unread,\#nosource,Incremental learning,Multi-task learning},
  annotation = {\_eprint: 1805.11119}
}

@article{mankowitz2018,
  title = {Unicorn: {{Continual Learning}} with a {{Universal}}, {{Off-policy Agent}}},
  author = {Mankowitz, Daniel J and {\v Z}{\'i}dek, Augustin and Barreto, Andr{\'e} and Horgan, Dan and Hessel, Matteo and Quan, John and Oh, Junhyuk and {van Hasselt}, Hado and Silver, David and Schaul, Tom},
  year = {2018},
  journal = {arXiv},
  pages = {1--17},
  url = {http://arxiv.org/abs/1802.08294},
  abstract = {Some real-world domains are best characterized as a single task, but for others this perspective is limiting. Instead, some tasks continually grow in complexity, in tandem with the agent's competence. In continual learning, also referred to as lifelong learning, there are no explicit task boundaries or curricula. As learning agents have become more powerful, continual learning remains one of the frontiers that has resisted quick progress. To test continual learning capabilities we consider a challenging 3D domain with an implicit sequence of tasks and sparse rewards. We propose a novel agent architecture called Unicorn, which demonstrates strong continual learning and outperforms several baseline agents on the proposed domain. The agent achieves this by jointly representing and learning multiple policies efficiently, using a parallel off-policy learning setup.},
  keywords = {/unread,\#nosource,⛔ No DOI found},
  annotation = {\_eprint: 1802.08294}
}

@article{marsland2002,
  title = {A Self-Organising Network That Grows When Required},
  author = {Marsland, Stephen and Shapiro, Jonathan and Nehmzow, Ulrich},
  year = {2002},
  journal = {Neural Networks},
  volume = {15},
  number = {8-9},
  pages = {1041--1058},
  publisher = {{Pergamon}},
  issn = {08936080},
  doi = {10/b8w297},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0893608002000783},
  abstract = {The ability to grow extra nodes is a potentially useful facility for a self-organising neural network. A network that can add nodes into its map space can approximate the input space more accurately, and often more parsimoniously, than a network with predefined structure and size, such as the Self-Organising Map. In addition, a growing network can deal with dynamic input distributions. Most of the growing networks that have been proposed in the literature add new nodes to support the node that has accumulated the highest error during previous iterations or to support topological structures. This usually means that new nodes are added only when the number of iterations is an integer multiple of some pre-defined constant, {$\lambda$}. This paper suggests a way in which the learning algorithm can add nodes whenever the network in its current state does not sufficiently match the input. In this way the network grows very quickly when new data is presented, but stops growing once the network has matched the data. This is particularly important when we consider dynamic data sets, where the distribution of inputs can change to a new regime after some time. We also demonstrate the preservation of neighbourhood relations in the data by the network. The new network is compared to an existing growing network, the Growing Neural Gas (GNG), on a artificial dataset, showing how the network deals with a change in input distribution after some time. Finally, the new network is applied to several novelty detection tasks and is compared with both the GNG and an unsupervised form of the Reduced Coulomb Energy network on a robotic inspection task and with a Support Vector Machine on two benchmark novelty detection tasks. \textcopyright{} 2002 Elsevier Science Ltd. All rights reserved.},
  keywords = {[som],/unread,\#nosource,Dimensionality reduction,Growing networks,Inspection,Mobile robotics,Novelty detection,Self-organisation,Topology preservation,Unsupervised learning}
}

@inproceedings{mazumder2019,
  title = {Lifelong and {{Interactive Learning}} of {{Factual Knowledge}} in {{Dialogues}}},
  booktitle = {Proceedings of the 20th {{Annual SIGdial Meeting}} on {{Discourse}} and {{Dialogue}}},
  author = {Mazumder, Sahisnu and Liu, Bing and Wang, Shuai and Ma, Nianzu},
  year = {2019},
  pages = {21--31},
  publisher = {{Association for Computational Linguistics}},
  address = {{Stroudsburg, PA, USA}},
  doi = {10/gnq33d},
  url = {http://arxiv.org/abs/1907.13295 https://www.aclweb.org/anthology/W19-5903},
  abstract = {Dialogue systems are increasingly using knowledge bases (KBs) storing real-world facts to help generate quality responses. However, as the KBs are inherently incomplete and remain fixed during conversation, it limits dialogue systems' ability to answer questions and to handle questions involving entities or relations that are not in the KB. In this paper, we make an attempt to propose an engine for Continuous and Interactive Learning of Knowledge (CILK) for dialogue systems to give them the ability to continuously and interactively learn and infer new knowledge during conversations. With more knowledge accumulated over time, they will be able to learn better and answer more questions. Our empirical evaluation shows that CILK is promising.},
  keywords = {[nlp],/unread,\#nosource},
  annotation = {\_eprint: 1907.13295}
}

@article{mehta2020,
  title = {Bayesian {{Nonparametric Weight Factorization}} for {{Continual Learning}}},
  author = {Mehta, Nikhil and Liang, Kevin J and Carin, Lawrence},
  year = {2020},
  journal = {arXiv},
  pages = {1--17},
  url = {http://arxiv.org/abs/2004.10098},
  abstract = {Naively trained neural networks tend to experience catastrophic forgetting in sequential task settings, where data from previous tasks are unavailable. A number of methods, using various model expansion strategies, have been proposed recently as possible solutions. However, determining how much to expand the model is left to the practitioner, and typically a constant schedule is chosen for simplicity, regardless of how complex the incoming task is. Instead, we propose a principled Bayesian nonparametric approach based on the Indian Buffet Process (IBP) prior, letting the data determine how much to expand the model complexity. We pair this with a factorization of the neural network's weight matrices. Such an approach allows us to scale the number of factors of each weight matrix to the complexity of the task, while the IBP prior imposes weight factor sparsity and encourages factor reuse, promoting positive knowledge transfer between tasks. We demonstrate the effectiveness of our method on a number of continual learning benchmarks and analyze how weight factors are allocated and reused throughout the training.},
  keywords = {[bayes],[cifar],[mnist],[sparsity],/unread,\#nosource,⛔ No DOI found},
  annotation = {\_eprint: 2004.10098}
}

@inproceedings{mendez2018,
  title = {Lifelong {{Inverse Reinforcement Learning}}},
  booktitle = {{{NeurIPS}}},
  author = {Mendez, Jorge A and Shivkumar, Shashank and Eaton, Eric},
  year = {2018},
  pages = {4502--4513},
  url = {http://papers.nips.cc/paper/7702-lifelong-inverse-reinforcement-learning.pdf},
  abstract = {Methods for learning from demonstration (LfD) have shown success in acquiring behavior policies by imitating a user. However, even for a single task, LfD may require numerous demonstrations. For versatile agents that must learn many tasks via demonstration, this process would substantially burden the user if each task were learned in isolation. To address this challenge, we introduce the novel problem of lifelong learning from demonstration, which allows the agent to continually build upon knowledge learned from previously demonstrated tasks to accelerate the learning of new tasks, reducing the amount of demonstrations required. As one solution to this problem, we propose the first lifelong learning approach to inverse reinforcement learning, which learns consecutive tasks via demonstration, continually transferring knowledge between tasks to improve performance.},
  keywords = {/unread,\#nosource,⛔ No DOI found}
}

@misc{mendez2022,
  title = {How to {{Reuse}} and {{Compose Knowledge}} for a {{Lifetime}} of {{Tasks}}: {{A Survey}} on {{Continual Learning}} and {{Functional Composition}}},
  shorttitle = {How to {{Reuse}} and {{Compose Knowledge}} for a {{Lifetime}} of {{Tasks}}},
  author = {Mendez, Jorge A. and Eaton, Eric},
  year = {2022},
  number = {arXiv:2207.07730},
  eprint = {2207.07730},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2207.07730},
  urldate = {2022-07-19},
  abstract = {A major goal of artificial intelligence (AI) is to create an agent capable of acquiring a general understanding of the world. Such an agent would require the ability to continually accumulate and build upon its knowledge as it encounters new experiences. Lifelong or continual learning addresses this setting, whereby an agent faces a continual stream of problems and must strive to capture the knowledge necessary for solving each new task it encounters. If the agent is capable of accumulating knowledge in some form of compositional representation, it could then selectively reuse and combine relevant pieces of knowledge to construct novel solutions. Despite the intuitive appeal of this simple idea, the literatures on lifelong learning and compositional learning have proceeded largely separately. In an effort to promote developments that bridge between the two fields, this article surveys their respective research landscapes and discusses existing and future connections between them.},
  archiveprefix = {arXiv},
  keywords = {/unread,Computer Science - Artificial Intelligence,Computer Science - Machine Learning}
}

@article{mermillod2013,
  title = {The Stability-Plasticity Dilemma: Investigating the Continuum from Catastrophic Forgetting to Age-Limited Learning Effects},
  author = {Mermillod, Martial and Bugaiska, Aur{\'e}lia and Bonin, Patrick},
  year = {2013},
  journal = {Frontiers in Psychology},
  volume = {4},
  number = {August},
  pages = {504},
  issn = {1664-1078},
  doi = {10/gf27gv},
  url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3732997%7B%5C&%7Dtool=pmcentrez%7B%5C&%7Drendertype=abstract http://journal.frontiersin.org/article/10.3389/fpsyg.2013.00504/abstract},
  pmid = {23935590},
  keywords = {/unread,\#nosource,Mermillod2013a}
}

@article{mi2021,
  title = {Representation {{Memorization}} for {{Fast Learning New Knowledge}} without {{Forgetting}}},
  author = {Mi, Fei and Lin, Tao and Faltings, Boi},
  year = {2021},
  journal = {arXiv},
  eprint = {2108.12596},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2108.12596},
  urldate = {2021-08-31},
  abstract = {The ability to quickly learn new knowledge (e.g. new classes or data distributions) is a big step towards human-level intelligence. In this paper, we consider scenarios that require learning new classes or data distributions quickly and incrementally over time, as it often occurs in real-world dynamic environments. We propose "Memory-based Hebbian Parameter Adaptation" (Hebb) to tackle the two major challenges (i.e., catastrophic forgetting and sample efficiency) towards this goal in a unified framework. To mitigate catastrophic forgetting, Hebb augments a regular neural classifier with a continuously updated memory module to store representations of previous data. To improve sample efficiency, we propose a parameter adaptation method based on the well-known Hebbian theory, which directly "wires" the output network's parameters with similar representations retrieved from the memory. We empirically verify the superior performance of Hebb through extensive experiments on a wide range of learning tasks (image classification, language model) and learning scenarios (continual, incremental, online). We demonstrate that Hebb effectively mitigates catastrophic forgetting, and it indeed learns new knowledge better and faster than the current state-of-the-art.},
  archiveprefix = {arXiv},
  keywords = {[hebbian];[rnn],/unread,\#nosource,⛔ No DOI found,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing}
}

@inproceedings{michieli2019,
  title = {Incremental Learning Techniques for Semantic Segmentation},
  booktitle = {Proceedings - 2019 {{International Conference}} on {{Computer Vision Workshop}}, {{ICCVW}} 2019},
  author = {Michieli, Umberto and Zanuttigh, Pietro},
  year = {2019},
  pages = {3205--3212},
  publisher = {{Institute of Electrical and Electronics Engineers Inc.}},
  doi = {10/gnq327},
  url = {http://arxiv.org/abs/1907.13372},
  abstract = {Deep learning architectures exhibit a critical drop of performance due to catastrophic forgetting when they are required to incrementally learn new tasks. Contemporary incremental learning frameworks focus on image classification and object detection while in this work we formally introduce the incremental learning problem for semantic segmentation in which a pixel-wise labeling is considered. To tackle this task we propose to distill the knowledge of the previous model to retain the information about previously learned classes, whilst updating the current model to learn the new ones. We propose various approaches working both on the output logits and on intermediate features. In opposition to some recent frameworks, we do not store any image from previously learned classes and only the last model is needed to preserve high accuracy on these classes. The experimental evaluation on the Pascal VOC2012 dataset shows the effectiveness of the proposed approaches.},
  isbn = {978-1-72815-023-9},
  keywords = {/unread,\#nosource,Catastrophic forgetting,Incremental learning,Knowledge distillation,Knowledge transfer,Semantic segmentation},
  annotation = {\_eprint: 1907.13372}
}

@article{miconi2016,
  title = {Backpropagation of {{Hebbian}} Plasticity for Continual Learning},
  author = {Miconi, Thomas},
  year = {2016},
  journal = {NIPS Workshop - Continual Learning},
  pages = {5},
  url = {https://c38663e3-a-62cb3a1a-s-sites.googlegroups.com/site/cldlnips2016/CLDL-2016_paper_2.pdf?attachauth=ANoY7cpkpkdHxt2kA42TazATZVrBcNkcKZBbB_QkYQ2MQDe-Hz-inAnoBcb2Rl-6VCBWzWbjKjULT3tkSAtt1hdk66nh4Gy28ObAg7jKgLXNMzPTOYyB_roYB1nPaDNNkfQQhJJGXUdSexlxXDBUU0S},
  langid = {english},
  keywords = {/unread,\#nosource,⛔ No DOI found,hebbian,workshop}
}

@inproceedings{miconi2018,
  title = {Differentiable Plasticity: Training Plastic Neural Networks with Backpropagation},
  shorttitle = {Differentiable Plasticity},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Miconi, Thomas and Stanley, Kenneth and Clune, Jeff},
  year = {2018},
  pages = {3559--3568},
  url = {http://proceedings.mlr.press/v80/miconi18a.html},
  abstract = {How can we build agents that keep learning from experience, quickly and efficiently, after their initial training? Here we take inspiration from the main mechanism of learning in biological brains:...},
  langid = {english},
  keywords = {/unread,\#nosource,plasticity,recurrent}
}

@inproceedings{miconi2019,
  title = {Backpropamine: Training Self-Modifying Neural Networks with Differentiable Neuromodulated Plasticity},
  booktitle = {{{ICLR}}},
  author = {Miconi, Thomas and Rawal, Aditya and Clune, Jeff and Stanley, Kenneth O},
  year = {2019},
  url = {https://openreview.net/pdf?id=r1lrAiA5Ym},
  abstract = {The impressive lifelong learning in animal brains is primarily enabled by plastic changes in synaptic connectivity. Importantly, these changes are not passive, but are actively controlled by neuromodulation, which is itself under the control of the brain. The resulting self-modifying abilities of the brain play an important role in learning and adaptation, and are a major basis for biological reinforcement learning. Here we show for the first time that artificial neural networks with such neuromodulated plasticity can be trained with gradient descent. Extending previous work on differentiable Hebbian plasticity, we propose a differentiable formulation for the neuromodulation of plasticity. We show that neuromodulated plasticity improves the performance of neural networks on both reinforcement learning and supervised learning tasks. In one task, neuromodulated plastic LSTMs with millions of parameters outperform standard LSTMs on a benchmark language modeling task (controlling for the number of parameters). We conclude that differentiable neuromodulation of plasticity offers a powerful new framework for training neural networks.},
  keywords = {/unread,\#nosource,fashion,mnist,spiking}
}

@article{mirzadeh2020,
  title = {Understanding the {{Role}} of {{Training Regimes}} in {{Continual Learning}}},
  author = {Mirzadeh, Seyed Iman and Farajtabar, Mehrdad and Pascanu, Razvan and Ghasemzadeh, Hassan},
  year = {2020},
  journal = {arXiv},
  eprint = {2006.06958},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2006.06958},
  urldate = {2021-09-15},
  abstract = {Catastrophic forgetting affects the training of neural networks, limiting their ability to learn multiple tasks sequentially. From the perspective of the well established plasticity-stability dilemma, neural networks tend to be overly plastic, lacking the stability necessary to prevent the forgetting of previous knowledge, which means that as learning progresses, networks tend to forget previously seen tasks. This phenomenon coined in the continual learning literature, has attracted much attention lately, and several families of approaches have been proposed with different degrees of success. However, there has been limited prior work extensively analyzing the impact that different training regimes -- learning rate, batch size, regularization method-- can have on forgetting. In this work, we depart from the typical approach of altering the learning algorithm to improve stability. Instead, we hypothesize that the geometrical properties of the local minima found for each task play an important role in the overall degree of forgetting. In particular, we study the effect of dropout, learning rate decay, and batch size, on forming training regimes that widen the tasks' local minima and consequently, on helping it not to forget catastrophically. Our study provides practical insights to improve stability via simple yet effective techniques that outperform alternative baselines.},
  archiveprefix = {arXiv},
  keywords = {/unread,\#nosource,⛔ No DOI found,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning}
}

@article{mirzadeh2020a,
  title = {Linear {{Mode Connectivity}} in {{Multitask}} and {{Continual Learning}}},
  author = {Mirzadeh, Seyed Iman and Farajtabar, Mehrdad and Gorur, Dilan and Pascanu, Razvan and Ghasemzadeh, Hassan},
  year = {2020},
  journal = {arXiv},
  url = {https://arxiv.org/abs/2010.04495},
  abstract = {Continual (sequential) training and multitask (simultaneous) training are often attempting to solve the same overall objective: to find a solution that performs well on all considered tasks. The main difference is in the training regimes, where continual learning can only have access to one task at a time, which for neural networks typically leads to catastrophic forgetting. That is, the solution found for a subsequent task does not perform well on the previous ones anymore. However, the relationship between the different minima that the two training regimes arrive at is not well understood. What sets them apart? Is there a local structure that could explain the difference in performance achieved by the two different schemes? Motivated by recent work showing that different minima of the same task are typically connected by very simple curves of low error, we investigate whether multitask and continual solutions are similarly connected. We empirically find that indeed such connectivity can be reliably achieved and, more interestingly, it can be done by a linear path, conditioned on having the same initialization for both. We thoroughly analyze this observation and discuss its significance for the continual learning process. Furthermore, we exploit this finding to propose an effective algorithm that constrains the sequentially learned minima to behave as the multitask solution. We show that our method outperforms several state of the art continual learning algorithms on various vision benchmarks.},
  keywords = {[cifar],[experimental],[mnist],/unread,\#nosource,⛔ No DOI found},
  annotation = {\_eprint: 2010.04495},
  note = {The authors observe how minima of CL and Multitask algorithms lie in a linear subspace (when sharing initialization). They use this argumento to build a CL strategy which forces minima to stay on the same subspace, using also small replay memories.
\par
The authors observe how minima of CL and Multitask algorithms lie in a linear subspace (when sharing initialization). They use this argumento to build a CL strategy which forces minima to stay on the same subspace, using also small replay memories.}
}

@article{mirzadeh2021,
  title = {Wide {{Neural Networks Forget Less Catastrophically}}},
  author = {Mirzadeh, Seyed Iman and Chaudhry, Arslan and Hu, Huiyi and Pascanu, Razvan and Gorur, Dilan and Farajtabar, Mehrdad},
  year = {2021},
  journal = {arXiv},
  eprint = {2110.11526},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2110.11526},
  urldate = {2021-10-25},
  abstract = {A growing body of research in continual learning is devoted to overcoming the "Catastrophic Forgetting" of neural networks by designing new algorithms that are more robust to the distribution shifts. While the recent progress in continual learning literature is encouraging, our understanding of what properties of neural networks contribute to catastrophic forgetting is still limited. To address this, instead of focusing on continual learning algorithms, in this work, we focus on the model itself and study the impact of "width" of the neural network architecture on catastrophic forgetting, and show that width has a surprisingly significant effect on forgetting. To explain this effect, we study the learning dynamics of the network from various perspectives such as gradient norm and sparsity, orthogonalization, and lazy training regime. We provide potential explanations that are consistent with the empirical results across different architectures and continual learning benchmarks.},
  archiveprefix = {arXiv},
  keywords = {/unread,\#nosource,⛔ No DOI found,Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  note = {Comment: preprint}
}

@article{mirzadeh2022,
  title = {Architecture {{Matters}} in {{Continual Learning}}},
  author = {Mirzadeh, Seyed Iman and Chaudhry, Arslan and Yin, Dong and Nguyen, Timothy and Pascanu, Razvan and Gorur, Dilan and Farajtabar, Mehrdad},
  year = {2022},
  journal = {arXiv},
  eprint = {2202.00275},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2202.00275},
  urldate = {2022-02-02},
  abstract = {A large body of research in continual learning is devoted to overcoming the catastrophic forgetting of neural networks by designing new algorithms that are robust to the distribution shifts. However, the majority of these works are strictly focused on the "algorithmic" part of continual learning for a "fixed neural network architecture", and the implications of using different architectures are mostly neglected. Even the few existing continual learning methods that modify the model assume a fixed architecture and aim to develop an algorithm that efficiently uses the model throughout the learning experience. However, in this work, we show that the choice of architecture can significantly impact the continual learning performance, and different architectures lead to different trade-offs between the ability to remember previous tasks and learning new ones. Moreover, we study the impact of various architectural decisions, and our findings entail best practices and recommendations that can improve the continual learning performance.},
  archiveprefix = {arXiv},
  keywords = {/unread,⛔ No DOI found,Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  annotation = {ZSCC: 0000000},
  note = {Comment: preprint}
}

@inproceedings{mitchell1993,
  title = {Explanation-{{Based Neural Network Learning}} for {{Robot Control}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 5},
  author = {Mitchell, Tom M and Thrun, Sebastian B},
  year = {1993},
  url = {https://papers.nips.cc/paper/614-explanation-based-neural-network-learning-for-robot-control.pdf},
  abstract = {How can artificial neural nets generalize better from fewer examples? In order to generalize successfully, neural network learning methods typically require large training data sets. We introduce a neural network learning method that generalizes rationally from many fewer data points, relying instead on prior knowledge encoded in previously learned neural networks. For example, in robot control learning tasks reported here, previously learned networks that model the effects of robot actions are used to guide subsequent learning of robot control functions. For each observed training example of the target function (e.g. the robot control policy), the learner explains the observed example in terms of its prior knowledge, then analyzes this explanation to infer additional information about the shape, or slope, of the target function. This shape knowledge is used to bias generalization when learning the target function. Results are presented applying this approach to a simulated robot task based on reinforcement learning.},
  keywords = {/unread,\#nosource,⛔ No DOI found}
}

@article{mitchell2015,
  title = {Never-{{Ending Learning}}},
  author = {Mitchell, Tom and Cohen, William W and Hruschka, E and Talukdar, Partha P and Yang, B and Betteridge, Justin and Carlson, Andrew and Dalvi, B and Gardner, Matt and Kisiel, Bryan and Krishnamurthy, J and Lao, Ni and Mazaitis, K and Mohamed, T and Nakashole, N and Platanios, E and Ritter, A and Samadi, M and Settles, B and Wang, R and Wijaya, D and Gupta, A and Chen, X and Saparov, A and Greaves, M and Welling, J},
  year = {2015},
  journal = {Communications of the Acm},
  volume = {61},
  number = {1},
  pages = {2302--2310},
  issn = {0001-0782},
  doi = {10/gdjg3d},
  url = {https://dl.acm.org/doi/10.1145/3191513},
  abstract = {Whereas people learn many different types of knowledge from diverse experiences over many years, and become better learners over time, most current machine learning systems are much more narrow, learning just a single func-tion or data model based on statistical analysis of a single data set. We suggest that people learn better than comput-ers precisely because of this difference, and we suggest a key direction for machine learning research is to develop software architectures that enable intelligent agents to also learn many types of knowledge, continuously over many years, and to become better learners over time. In this paper we defi ne more precisely this never-ending learning paradigm for machine learning, and we present one case study: the Never-Ending Language Learner (NELL), which achieves a number of the desired properties of a never-end-ing learner. NELL has been learning to read the Web 24hrs/ day since January 2010, and so far has acquired a knowledge base with 120mn diverse, confi dence-weighted beliefs (e.g., servedWith(tea,biscuits)), while learning thousands of interrelated functions that continually improve its reading competence over time. NELL has also learned to reason over its knowledge base to infer new beliefs it has not yet read from those it has, and NELL is inventing new relational pred-icates to extend the ontology it uses to represent beliefs. We describe the design of NELL, experimental results illustrat-ing its behavior, and discuss both its successes and short-comings as a case study in never-ending learning. NELL can be tracked online at http://rtw.ml.cmu.edu, and followed on Twitter at @CMUNELL. 1. INTRODUCTION Machine learning is a highly successful branch of Artifi cial Intelligence (AI), and is now widely used for tasks from spam fi ltering, to speech recognition, to credit card fraud detec-tion, to face recognition. Despite these successes, the ways in which computers learn today remain surprisingly narrow when compared to human learning. This paper explores an alternative paradigm for machine learning that more closely models the diversity, competence and cumulative nature of human learning. We call this alternative paradigm never-ending learning. To illustrate, note that in each of the above machine learning applications, the computer learns only a single function to perform a single task in isolation, usually from human labeled training examples of inputs and outputs of that function. In spam fi ltering, for instance, training examples consist of specifi c emails and spam or not-spam labels for each. This style of learning is often called super-vised function approximation, because the abstract learning problem is to approximate some unknown function f : X \textrightarrow{} Y},
  isbn = {9781577357018},
  pmid = {15003161},
  keywords = {/unread,\#nosource,NLP and Machine Learning Track},
  annotation = {\_eprint: arXiv:1011.1669v3}
}

@article{mundt2019,
  title = {Unified {{Probabilistic Deep Continual Learning}} through {{Generative Replay}} and {{Open Set Recognition}}},
  author = {Mundt, Martin and Majumder, Sagnik and Pliushch, Iuliia and Hong, Yong Won and Ramesh, Visvanathan},
  year = {2019},
  journal = {arXiv},
  url = {http://arxiv.org/abs/1905.12019},
  abstract = {We introduce a probabilistic approach to unify open set recognition with the prevention of catastrophic forgetting in deep continual learning, based on variational Bayesian inference. Our single model combines a joint probabilistic encoder with a generative model and a linear classifier that get shared across sequentially arriving tasks. In order to successfully distinguish unseen unknown data from trained known tasks, we propose to bound the class specific approximate posterior by fitting regions of high density on the basis of correctly classified data points. These bounds are further used to significantly alleviate catastrophic forgetting by avoiding samples from low density areas in generative replay. Our approach requires neither storing of old, nor upfront knowledge of future data, and is empirically validated on visual and audio tasks in class incremental, as well as cross-dataset scenarios across modalities.},
  keywords = {[audio],[bayes],[fashion],[framework],[generative],[mnist],[vision],/unread,\#nosource,⛔ No DOI found},
  annotation = {\_eprint: 1905.12019}
}

@article{mundt2020,
  title = {A {{Wholistic View}} of {{Continual Learning}} with {{Deep Neural Networks}}: {{Forgotten Lessons}} and the {{Bridge}} to {{Active}} and {{Open World Learning}}},
  author = {Mundt, Martin and Hong, Yong Won and Pliushch, Iuliia and Ramesh, Visvanathan},
  year = {2020},
  journal = {arXiv},
  pages = {32},
  url = {http://arxiv.org/abs/2009.01797},
  abstract = {Current deep learning research is dominated by benchmark evaluation. A method is regarded as favorable if it empirically performs well on the dedicated test set. This mentality is seamlessly reflected in the resurfacing area of continual learning, where consecutively arriving sets of benchmark data are investigated. The core challenge is framed as protecting previously acquired representations from being catastrophically forgotten due to the iterative parameter updates. However, comparison of individual methods is nevertheless treated in isolation from real world application and typically judged by monitoring accumulated test set performance. The closed world assumption remains predominant. It is assumed that during deployment a model is guaranteed to encounter data that stems from the same distribution as used for training. This poses a massive challenge as neural networks are well known to provide overconfident false predictions on unknown instances and break down in the face of corrupted data. In this work we argue that notable lessons from open set recognition, the identification of statistically deviating data outside of the observed dataset, and the adjacent field of active learning, where data is incrementally queried such that the expected performance gain is maximized, are frequently overlooked in the deep learning era. Based on these forgotten lessons, we propose a consolidated view to bridge continual learning, active learning and open set recognition in deep neural networks. Our results show that this not only benefits each individual paradigm, but highlights the natural synergies in a common framework. We empirically demonstrate improvements when alleviating catastrophic forgetting, querying data in active learning, selecting task orders, while exhibiting robust open world application where previously proposed methods fail.},
  keywords = {[bayes],[framework],/unread,\#nosource,⛔ No DOI found},
  annotation = {\_eprint: 2009.01797}
}

@inproceedings{mundt2021,
  title = {Neural {{Architecture Search}} of {{Deep Priors}}: {{Towards Continual Learning Without Catastrophic Interference}}},
  shorttitle = {Neural {{Architecture Search}} of {{Deep Priors}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Mundt, Martin and Pliushch, Iuliia and Ramesh, Visvanathan},
  year = {2021},
  pages = {3523--3532},
  url = {https://openaccess.thecvf.com/content/CVPR2021W/CLVision/html/Mundt_Neural_Architecture_Search_of_Deep_Priors_Towards_Continual_Learning_Without_CVPRW_2021_paper.html},
  urldate = {2021-10-09},
  langid = {english},
  keywords = {/unread,\#nosource}
}

@inproceedings{mundt2021a,
  title = {{{CLEVA-Compass}}: {{A Continual Learning Evaluation Assessment Compass}} to {{Promote Research Transparency}} and {{Comparability}}},
  shorttitle = {{{CLEVA-Compass}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Mundt, Martin and Lang, Steven and Delfosse, Quentin and Kersting, Kristian},
  year = {2021},
  url = {https://openreview.net/forum?id=rHMaBYbkkRJ},
  urldate = {2022-03-24},
  abstract = {What is the state of the art in continual machine learning? Although a natural question for predominant static benchmarks, the notion to train systems in a lifelong manner entails a plethora of...},
  langid = {english},
  keywords = {/unread}
}

@article{nagabandi2019,
  title = {Deep Online Learning via Meta-Learning: {{Continual}} Adaptation for Model-Based {{RL}}},
  author = {Nagabandi, Anusha and Finn, Chelsea and Levine, Sergey},
  year = {2019},
  journal = {7th International Conference on Learning Representations, ICLR 2019},
  url = {https://arxiv.org/abs/1812.07671},
  abstract = {Humans and animals can learn complex predictive models that allow them to accurately and reliably reason about real-world phenomena, and they can adapt such models extremely quickly in the face of unexpected changes. Deep neural network models allow us to represent very complex functions, but lack this capacity for rapid online adaptation. The goal in this paper is to develop a method for continual online learning from an incoming stream of data, using deep neural network models. We formulate an online learning procedure that uses stochastic gradient descent to update model parameters, and an expectation maximization algorithm with a Chinese restaurant process prior to develop and maintain a mixture of models to handle non-stationary task distributions. This allows for all models to be adapted as necessary, with new models instantiated for task changes and old models recalled when previously seen tasks are encountered again. Furthermore, we observe that meta-learning can be used to meta-train a model such that this direct online adaptation with SGD is effective, which is otherwise not the case for large function approximators. In this work, we apply our meta-learning for online learning (MOLe) approach to model-based reinforcement learning, where adapting the predictive model is critical for control; we demonstrate that MOLe outperforms alternative prior methods, and enables effective continuous adaptation in non-stationary task distributions such as varying terrains, motor failures, and unexpected disturbances. Videos available at: https://sites.google.com/Berkeley.edu/onlineviameta.},
  keywords = {/unread,\#nosource,⛔ No DOI found},
  annotation = {\_eprint: 1812.07671}
}

@inproceedings{nekoei2021,
  title = {Continuous {{Coordination As}} a {{Realistic Scenario}} for {{Lifelong Learning}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Nekoei, Hadi and Badrinaaraayanan, Akilesh and Courville, Aaron and Chandar, Sarath},
  year = {2021},
  pages = {8016--8024},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {http://proceedings.mlr.press/v139/nekoei21a.html},
  urldate = {2021-07-13},
  abstract = {Current deep reinforcement learning (RL) algorithms are still highly task-specific and lack the ability to generalize to new environments. Lifelong learning (LLL), however, aims at solving multiple...},
  langid = {english},
  keywords = {/unread,\#nosource}
}

@inproceedings{NEURIPS2020_c2964caa,
  title = {{{RATT}}: {{Recurrent}} Attention to Transient Tasks for Continual Image Captioning},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Del Chiaro, Riccardo and Twardowski, Bart{\l}omiej and Bagdanov, Andrew and {van de Weijer}, Joost},
  editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.},
  year = {2020},
  volume = {33},
  pages = {16736--16748},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2020/file/c2964caac096f26db222cb325aa267cb-Paper.pdf},
  keywords = {[nlp],/unread,\#nosource,⛔ No DOI found}
}

@inproceedings{nguyen2018,
  title = {Variational {{Continual Learning}}},
  booktitle = {{{ICLR}}},
  author = {Nguyen, Cuong V and Li, Yingzhen and Bui, Thang D and Turner, Richard E},
  year = {2018},
  url = {https://openreview.net/forum?id=BkQqq0gRb},
  abstract = {This paper develops variational continual learning (VCL), a simple but general framework for continual learning that fuses online variational inference (VI) and recent advances in Monte Carlo VI for neural networks. The framework can successfully train both deep discriminative models and deep generative models in complex continual learning settings where existing tasks evolve over time and entirely new tasks emerge. Experimental results show that VCL outperforms state-of-the-art continual learning methods on a variety of tasks, avoiding catastrophic forgetting in a fully automatic way.},
  keywords = {[bayes],/unread,\#nosource}
}

@inproceedings{nguyen2019,
  title = {Continual {{Rare-Class Recognition}} with {{Emerging Novel Subclasses}}},
  booktitle = {{{ECML}}},
  author = {Nguyen, Hung and Wang, Xuejian and Akoglu, Leman},
  year = {2019},
  url = {http://arxiv.org/abs/1906.12218},
  abstract = {Given a labeled dataset that contains a rare (or minority) class of of-interest instances, as well as a large class of instances that are not of interest, how can we learn to recognize future of-interest instances over a continuous stream? We introduce RaRecognize, which (i) estimates a general decision boundary between the rare and the majority class, (ii) learns to recognize individual rare subclasses that exist within the training data, as well as (iii) flags instances from previously unseen rare subclasses as newly emerging. The learner in (i) is general in the sense that by construction it is dissimilar to the specialized learners in (ii), thus distinguishes minority from the majority without overly tuning to what is seen in the training data. Thanks to this generality, RaRecognize ignores all future instances that it labels as majority and recognizes the recurrent as well as emerging rare subclasses only. This saves effort at test time as well as ensures that the model size grows moderately over time as it only maintains specialized minority learners. Through extensive experiments, we show that RaRecognize outperforms state-of-the art baselines on three real-world datasets that contain corporate-risk and disaster documents as rare classes.},
  keywords = {[nlp],/unread,\#nosource},
  annotation = {\_eprint: 1906.12218}
}

@article{nguyen2019a,
  title = {Toward {{Understanding Catastrophic Forgetting}} in {{Continual Learning}}},
  author = {Nguyen, Cuong V and Achille, Alessandro and Lam, Michael and Hassner, Tal and Mahadevan, Vijay and Soatto, Stefano},
  year = {2019},
  journal = {arXiv},
  url = {http://arxiv.org/abs/1908.01091},
  abstract = {We study the relationship between catastrophic forgetting and properties of task sequences. In particular, given a sequence of tasks, we would like to understand which properties of this sequence influence the error rates of continual learning algorithms trained on the sequence. To this end, we propose a new procedure that makes use of recent developments in task space modeling as well as correlation analysis to specify and analyze the properties we are interested in. As an application, we apply our procedure to study two properties of a task sequence: (1) total complexity and (2) sequential heterogeneity. We show that error rates are strongly and positively correlated to a task sequence's total complexity for some state-of-the-art algorithms. We also show that, surprisingly, the error rates have no or even negative correlations in some cases to sequential heterogeneity. Our findings suggest directions for improving continual learning benchmarks and methods.},
  keywords = {[cifar],[mnist],/unread,\#nosource,⛔ No DOI found},
  annotation = {\_eprint: 1908.01091}
}

@article{nguyen2020,
  title = {Dissecting {{Catastrophic Forgetting}} in {{Continual Learning}} by {{Deep Visualization}}},
  author = {Nguyen, Giang and Chen, Shuan and Do, Thao and Jun, Tae Joon and Choi, Ho-Jin and Kim, Daeyoung},
  year = {2020},
  journal = {arXiv},
  url = {http://arxiv.org/abs/2001.01578},
  abstract = {Interpreting the behaviors of Deep Neural Networks (usually considered as a black box) is critical especially when they are now being widely adopted over diverse aspects of human life. Taking the advancements from Explainable Artificial Intelligent, this paper proposes a novel technique called Auto DeepVis to dissect catastrophic forgetting in continual learning. A new method to deal with catastrophic forgetting named critical freezing is also introduced upon investigating the dilemma by Auto DeepVis. Experiments on a captioning model meticulously present how catastrophic forgetting happens, particularly showing which components are forgetting or changing. The effectiveness of our technique is then assessed; and more precisely, critical freezing claims the best performance on both previous and coming tasks over baselines, proving the capability of the investigation. Our techniques could not only be supplementary to existing solutions for completely eradicating catastrophic forgetting for life-long learning but also explainable.},
  keywords = {[vision],/unread,\#nosource,⛔ No DOI found},
  annotation = {\_eprint: 2001.01578}
}

@article{ororbia2019,
  title = {Continual {{Learning}} of {{Recurrent Neural Networks}} by {{Locally Aligning Distributed Representations}}},
  author = {Ororbia, Alexander and Mali, Ankur and Giles, C Lee and Kifer, Daniel},
  year = {2019},
  journal = {arXiv},
  url = {http://arxiv.org/abs/1810.07411},
  abstract = {Temporal models based on recurrent neural networks have proven to be quite powerful in a wide variety of applications. However, training these models often relies on back-propagation through time, which entails unfolding the network over many time steps, making the process of conducting credit assignment considerably more challenging. Furthermore, the nature of back-propagation itself does not permit the use of non-differentiable activation functions and is inherently sequential, making parallelization of the underlying training process difficult. Here, we propose the Parallel Temporal Neural Coding Network (P-TNCN), a biologically inspired model trained by the learning algorithm we call Local Representation Alignment. It aims to resolve the difficulties and problems that plague recurrent networks trained by back-propagation through time. The architecture requires neither unrolling in time nor the derivatives of its internal activation functions. We compare our model and learning procedure to other back-propagation through time alternatives (which also tend to be computationally expensive), including real-time recurrent learning, echo state networks, and unbiased online recurrent optimization. We show that it outperforms these on sequence modeling benchmarks such as Bouncing MNIST, a new benchmark we denote as Bouncing NotMNIST, and Penn Treebank. Notably, our approach can in some instances outperform full back-propagation through time as well as variants such as sparse attentive back-tracking. Significantly, the hidden unit correction phase of P-TNCN allows it to adapt to new datasets even if its synaptic weights are held fixed (zero-shot adaptation) and facilitates retention of prior generative knowledge when faced with a task sequence. We present results that show the P-TNCN's ability to conduct zero-shot adaptation and online continual sequence modeling.},
  keywords = {[mnist],[rnn],[spiking],/unread,\#nosource,⛔ No DOI found,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computi,credi assignment},
  note = {Comment: Important revisions made throughout (additional items/results added, including a complexity analysis) arXiv: 1810.07411}
}

@article{ororbia2019a,
  title = {Lifelong {{Neural Predictive Coding}}: {{Sparsity Yields Less Forgetting}} When {{Learning Cumulatively}}},
  author = {Ororbia, Alexander and Mali, Ankur and Kifer, Daniel and Giles, C Lee},
  year = {2019},
  journal = {arXiv},
  pages = {1--11},
  url = {http://arxiv.org/abs/1905.10696},
  abstract = {In lifelong learning systems, especially those based on artificial neural networks, one of the biggest obstacles is the severe inability to retain old knowledge as new information is encountered. This phenomenon is known as catastrophic forgetting. In this paper, we present a new connectionist model, the Sequential Neural Coding Network, and its learning procedure, grounded in the neurocognitive theory of predictive coding. The architecture experiences significantly less forgetting as compared to standard neural models and outperforms a variety of previously proposed remedies and methods when trained across multiple task datasets in a stream-like fashion. The promising performance demonstrated in our experiments offers motivation that directly incorporating mechanisms prominent in real neuronal systems, such as competition, sparse activation patterns, and iterative input processing, can create viable pathways for tackling the challenge of lifelong machine learning.},
  keywords = {[fashion],[mnist],[sparsity],/unread,\#nosource,⛔ No DOI found},
  annotation = {\_eprint: 1905.10696}
}

@article{ororbia2020,
  title = {Spiking {{Neural Predictive Coding}} for {{Continual Learning}} from {{Data Streams}}},
  author = {Ororbia, Alexander},
  year = {2020},
  journal = {arXiv},
  url = {http://arxiv.org/abs/1908.08655},
  abstract = {For energy-efficient computation in specialized neuromorphic hardware, we present the Spiking Neural Coding Network, an instantiation of a family of artificial neural models strongly motivated by the theory of predictive coding. The model, in essence, works by operating in a never-ending process of "guess-and-check", where neurons predict the activity values of one another and then immediately adjust their own activities to make better future predictions. The interactive, iterative nature of our neural system fits well into the continuous time formulation of data sensory stream prediction and, as we show, the model's structure yields a simple, local synaptic update rule, which could be used to complement or replace online spike-timing dependent plasticity. In this article, we experiment with an instantiation of our model that consists of leaky integrate-and-fire units. However, the general framework within which our model is situated can naturally incorporate more complex, formal neurons such as the Hodgkin-Huxley model. Our experimental results in pattern recognition demonstrate the potential of the proposed model when binary spike trains are the primary paradigm for inter-neuron communication. Notably, our model is competitive in terms of classification performance, can conduct online semi-supervised learning, naturally experiences less forgetting when learning from a sequence of tasks, and is more computationally economical and biologically-plausible than popular artificial neural networks.},
  keywords = {[spiking],/unread,\#nosource,⛔ No DOI found,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computi,Quantitative Biology - Neurons and Cognition},
  note = {Comment: Revised version of manuscript \textendash{} includes updated experimental results arXiv: 1908.08655}
}

@article{ororbia2021,
  title = {Continual {{Competitive Memory}}: {{A Neural System}} for {{Online Task-Free Lifelong Learning}}},
  shorttitle = {Continual {{Competitive Memory}}},
  author = {Ororbia, Alexander G.},
  year = {2021},
  journal = {arXiv},
  eprint = {2106.13300},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2106.13300},
  urldate = {2021-06-30},
  abstract = {In this article, we propose a novel form of unsupervised learning, continual competitive memory (CCM), as well as a computational framework to unify related neural models that operate under the principles of competition. The resulting neural system is shown to offer an effective approach for combating catastrophic forgetting in online continual classification problems. We demonstrate that the proposed CCM system not only outperforms other competitive learning neural models but also yields performance that is competitive with several modern, state-of-the-art lifelong learning approaches on benchmarks such as Split MNIST and Split NotMNIST. CCM yields a promising path forward for acquiring representations that are robust to interference from data streams, especially when the task is unknown to the model and must be inferred without external guidance.},
  archiveprefix = {arXiv},
  keywords = {/unread,\#nosource,⛔ No DOI found,Computer Science - Machine Learning}
}

@article{ostapenko2022,
  title = {Foundational {{Models}} for {{Continual Learning}}: {{An Empirical Study}} of {{Latent Replay}}},
  shorttitle = {Foundational {{Models}} for {{Continual Learning}}},
  author = {Ostapenko, Oleksiy and Lesort, Timothee and Rodr{\'i}guez, Pau and Arefin, Md Rifat and Douillard, Arthur and Rish, Irina and Charlin, Laurent},
  year = {2022},
  journal = {arXiv},
  eprint = {2205.00329},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2205.00329},
  urldate = {2022-05-05},
  abstract = {Rapid development of large-scale pre-training has resulted in foundation models that can act as effective feature extractors on a variety of downstream tasks and domains. Motivated by this, we study the efficacy of pre-trained vision models as a foundation for downstream continual learning (CL) scenarios. Our goal is twofold. First, we want to understand the compute-accuracy trade-off between CL in the raw-data space and in the latent space of pre-trained encoders. Second, we investigate how the characteristics of the encoder, the pre-training algorithm and data, as well as of the resulting latent space affect CL performance. For this, we compare the efficacy of various pre-trained models in large-scale benchmarking scenarios with a vanilla replay setting applied in the latent and in the raw-data space. Notably, this study shows how transfer, forgetting, task similarity and learning are dependent on the input data characteristics and not necessarily on the CL algorithms. First, we show that under some circumstances reasonable CL performance can readily be achieved with a non-parametric classifier at negligible compute. We then show how models pre-trained on broader data result in better performance for various replay sizes. We explain this with representational similarity and transfer properties of these representations. Finally, we show the effectiveness of self-supervised pre-training for downstream domains that are out-of-distribution as compared to the pre-training domain. We point out and validate several research directions that can further increase the efficacy of latent CL including representation ensembling. The diverse set of datasets used in this study can serve as a compute-efficient playground for further CL research. The codebase is available under https://github.com/oleksost/latent\_CL.},
  archiveprefix = {arXiv},
  keywords = {/unread,⛔ No DOI found,Computer Science - Artificial Intelligence,Computer Science - Machine Learning}
}

@article{ozgun2020,
  title = {Importance {{Driven Continual Learning}} for {{Segmentation Across Domains}}},
  author = {{\"O}zg{\"u}n, Sinan {\"O}zg{\"u}r and Rickmann, Anne-Marie and Roy, Abhijit Guha and Wachinger, Christian},
  year = {2020},
  journal = {arXiv},
  pages = {1--10},
  url = {http://arxiv.org/abs/2005.00079},
  abstract = {The ability of neural networks to continuously learn and adapt to new tasks while retaining prior knowledge is crucial for many applications. However, current neural networks tend to forget previously learned tasks when trained on new ones, i.e., they suffer from Catastrophic Forgetting (CF). The objective of Continual Learning (CL) is to alleviate this problem, which is particularly relevant for medical applications, where it may not be feasible to store and access previously used sensitive patient data. In this work, we propose a Continual Learning approach for brain segmentation, where a single network is consecutively trained on samples from different domains. We build upon an importance driven approach and adapt it for medical image segmentation. Particularly, we introduce learning rate regularization to prevent the loss of the network's knowledge. Our results demonstrate that directly restricting the adaptation of important network parameters clearly reduces Catastrophic Forgetting for segmentation across domains.},
  keywords = {[vision],/unread,\#nosource,⛔ No DOI found},
  annotation = {\_eprint: 2005.00079}
}

@article{pan2019,
  title = {Leaky {{Tiling Activations}}: {{A Simple Approach}} to {{Learning Sparse Representations Online}}},
  author = {Pan, Yangchen and Banman, Kirby and White, Martha},
  year = {2019},
  journal = {arXiv},
  url = {http://arxiv.org/abs/1911.08068},
  abstract = {Interference is a known problem when learning in online settings, such as continual learning or reinforcement learning. Interference occurs when updates, to improve performance for some inputs, degrades performance for others. Recent work has shown that sparse representations\textemdash where only a small percentage of units are active\textemdash can significantly reduce interference. Those works, however, relied on relatively complex regularization or meta-learning approaches, that have only been used offline in a pre-training phase. In our approach, we design an activation function that naturally produces sparse representations, and so is much more amenable to online training. The idea relies on the simple approach of binning, but overcomes the two key limitations of binning: zero gradients for the flat regions almost everywhere, and lost precision\textemdash reduced discrimination\textemdash due to coarse aggregation. We introduce a Leaky Tiling Activation (LTA) that provides non-negligible gradients and produces overlap between bins that improves discrimination. We empirically investigate both value-based and policy gradient reinforcement learning algorithms that use neural networks with LTAs, in classic discrete-action control environments and Mujoco continuous-action environments. We show that, with LTAs, learning is faster, with more stable policies, without needing target networks.},
  keywords = {[sparsity],/unread,\#nosource,⛔ No DOI found},
  annotation = {\_eprint: 1911.08068}
}

@article{parisi2018,
  title = {Lifelong {{Learning}} of {{Spatiotemporal Representations With Dual-Memory Recurrent Self-Organization}}},
  author = {Parisi, German I and Tani, Jun and Weber, Cornelius and Wermter, Stefan},
  year = {2018},
  journal = {Frontiers in Neurorobotics},
  volume = {12},
  issn = {1662-5218},
  doi = {10/gfx4kz},
  url = {https://www.frontiersin.org/articles/10.3389/fnbot.2018.00078/full},
  abstract = {Artificial autonomous agents and robots interacting in complex environments are required to continually acquire and fine-tune knowledge over sustained periods of time. The ability to learn from continuous streams of information is referred to as lifelong learning and represents a long-standing challenge for neural network models due to catastrophic forgetting in which novel sensory experience interferes with existing representations and leads to abrupt decreases in the performance on previously acquired knowledge. Computational models of lifelong learning typically alleviate catastrophic forgetting in experimental scenarios with given datasets of static images and limited complexity, thereby differing significantly from the conditions artificial agents are exposed to. In more natural settings, sequential information may become progressively available over time and access to previous experience may be restricted. Therefore, specialized neural network mechanisms are required that adapt to novel sequential experience while preventing disruptive interference with existing representations. In this paper, we propose a dual-memory self-organizing architecture for lifelong learning scenarios. The architecture comprises two growing recurrent networks with the complementary tasks of learning object instances (episodic memory) and categories (semantic memory). Both growing networks can expand in response to novel sensory experience: the episodic memory learns fine-grained spatiotemporal representations of object instances in an unsupervised fashion while the semantic memory uses task-relevant signals to regulate structural plasticity levels and develop more compact representations from episodic experience. For the consolidation of knowledge in the absence of external sensory input, the episodic memory periodically replays trajectories of neural reactivations. We evaluate the proposed model on the CORe50 benchmark dataset for continuous object recognition, showing that we significantly outperform current methods of lifelong learning in three different incremental learning scenarios.},
  langid = {english},
  keywords = {[core50],[dual],[rnn],[som],/unread,\#nosource,CLS,Incremental Learning,Lifelong learning,Memory,object recognition systems,Self-organizing Network}
}

@article{parisi2019,
  title = {Continual Lifelong Learning with Neural Networks: {{A}} Review},
  shorttitle = {Continual Lifelong Learning with Neural Networks},
  author = {Parisi, German I and Kemker, Ronald and Part, Jose L and Kanan, Christopher and Wermter, Stefan},
  year = {2019},
  journal = {Neural Networks},
  volume = {113},
  pages = {54--71},
  issn = {0893-6080},
  doi = {10/gfvx7d},
  url = {http://www.sciencedirect.com/science/article/pii/S0893608019300231},
  abstract = {Humans and animals have the ability to continually acquire, fine-tune, and transfer knowledge and skills throughout their lifespan. This ability, referred to as lifelong learning, is mediated by a rich set of neurocognitive mechanisms that together contribute to the development and specialization of our sensorimotor skills as well as to long-term memory consolidation and retrieval. Consequently, lifelong learning capabilities are crucial for computational learning systems and autonomous agents interacting in the real world and processing continuous streams of information. However, lifelong learning remains a long-standing challenge for machine learning and neural network models since the continual acquisition of incrementally available information from non-stationary data distributions generally leads to catastrophic forgetting or interference. This limitation represents a major drawback for state-of-the-art deep neural network models that typically learn representations from stationary batches of training data, thus without accounting for situations in which information becomes incrementally available over time. In this review, we critically summarize the main challenges linked to lifelong learning for artificial learning systems and compare existing neural network approaches that alleviate, to different extents, catastrophic forgetting. Although significant advances have been made in domain-specific learning with neural networks, extensive research efforts are required for the development of robust lifelong learning on autonomous agents and robots. We discuss well-established and emerging research motivated by lifelong learning factors in biological systems such as structural plasticity, memory replay, curriculum and transfer learning, intrinsic motivation, and multisensory integration.},
  langid = {english},
  keywords = {[framework],/unread,\#nosource,Catastrophic forgetting,Continual Learning,Developmental systems,Lifelong Learning,Memory consolidation},
  note = {A general survey of the most important and used CL techniques, with a characterisation of the catastrophic forgetting phenomenon.}
}

@article{parisi2020,
  title = {Online {{Continual Learning}} on {{Sequences}}},
  author = {Parisi, German I and Lomonaco, Vincenzo},
  year = {2020},
  journal = {Studies in Computational Intelligence},
  doi = {10/gnq33q},
  url = {http://arxiv.org/abs/2003.09114},
  abstract = {Online continual learning (OCL) refers to the ability of a system to learn over time from a continuous stream of data without having to revisit previously encountered training samples. Learning continually in a single data pass is crucial for agents and robots operating in changing environments and required to acquire, fine-tune, and transfer increasingly complex representations from non-i.i.d. input distributions. Machine learning models that address OCL must alleviate \$\textbackslash backslash\$textit\{catastrophic forgetting\} in which hidden representations are disrupted or completely overwritten when learning from streams of novel input. In this chapter, we summarize and discuss recent deep learning models that address OCL on sequential input through the use (and combination) of synaptic regularization, structural plasticity, and experience replay. Different implementations of replay have been proposed that alleviate catastrophic forgetting in connectionists architectures via the re-occurrence of (latent representations of) input sequences and that functionally resemble mechanisms of hippocampal replay in the mammalian brain. Empirical evidence shows that architectures endowed with experience replay typically outperform architectures without in (online) incremental learning tasks.},
  keywords = {[framework],/unread,\#nosource,Computer Science - Computer Vision and Pattern Rec,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computi},
  note = {Comment: L. Oneto et al. (eds.), Recent Trends in Learning From Data, Studies in Computational Intelligence 896 arXiv: 2003.09114}
}

@inproceedings{parshotam2020,
  title = {Continual {{Learning}} of {{Object Instances}}},
  booktitle = {{{CVPR}} 2020: {{Workshop}} on {{Continual Learning}} in {{Computer Vision}}},
  author = {Parshotam, Kishan and Kilickaya, Mert},
  year = {2020},
  doi = {10/gm8f65},
  url = {http://arxiv.org/abs/2004.10862},
  abstract = {We propose continual instance learning - a method that applies the concept of continual learning to the task of distinguishing instances of the same object category. We specifically focus on the car object, and incrementally learn to distinguish car instances from each other with metric learning. We begin our paper by evaluating current techniques. Establishing that catastrophic forgetting is evident in existing methods, we then propose two remedies. Firstly, we regularise metric learning via Normalised Cross-Entropy. Secondly, we augment existing models with synthetic data transfer. Our extensive experiments on three large-scale datasets, using two different architectures for five different continual learning methods, reveal that Normalised cross-entropy and synthetic transfer leads to less forgetting in existing techniques.},
  keywords = {[vision],/unread,\#nosource},
  annotation = {\_eprint: 2004.10862}
}

@inproceedings{pfulb2018,
  title = {A Comprehensive, Application-Oriented Study of Catastrophic Forgetting in {{DNNs}}},
  booktitle = {{{ICLR}}},
  author = {Pf{\"u}lb, B and Gepperth, A},
  year = {2018},
  url = {https://openreview.net/pdf?id=BkloRs0qK7},
  abstract = {We present a large-scale empirical study of catastrophic forgetting (CF) in modern Deep Neural Network (DNN) models that perform sequential (or: incremen-tal) learning. A new experimental protocol is proposed that enforces typical constraints encountered in application scenarios. As the investigation is empirical, we evaluate CF behavior on the hitherto largest number of visual classification datasets, from each of which we construct a representative number of Sequential Learning Tasks (SLTs) in close alignment to previous works on CF. Our results clearly indicate that there is no model that avoids CF for all investigated datasets and SLTs under application conditions. We conclude with a discussion of potential solutions and workarounds to CF, notably for the EWC and IMM models.},
  keywords = {[fashion],[mnist],/unread,\#nosource}
}

@article{philps2019,
  title = {Making {{Good}} on {{LSTMs}}' {{Unfulfilled Promise}}},
  author = {Philps, Daniel and d'Avila Garcez, Artur and Weyde, Tillman},
  year = {2019},
  journal = {arXiv},
  eprint = {1911.04489},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1911.04489},
  urldate = {2021-01-08},
  abstract = {LSTMs promise much to financial time-series analysis, temporal and cross-sectional inference, but we find that they do not deliver in a real-world financial management task. We examine an alternative called Continual Learning (CL), a memory-augmented approach, which can provide transparent explanations, i.e. which memory did what and when. This work has implications for many financial applications including credit, time-varying fairness in decision making and more. We make three important new observations. Firstly, as well as being more explainable, time-series CL approaches outperform LSTMs as well as a simple sliding window learner using feed-forward neural networks (FFNN). Secondly, we show that CL based on a sliding window learner (FFNN) is more effective than CL based on a sequential learner (LSTM). Thirdly, we examine how real-world, time-series noise impacts several similarity approaches used in CL memory addressing. We provide these insights using an approach called Continual Learning Augmentation (CLA) tested on a complex real-world problem, emerging market equities investment decision making. CLA provides a test-bed as it can be based on different types of time-series learners, allowing testing of LSTM and FFNN learners side by side. CLA is also used to test several distance approaches used in a memory recall-gate: Euclidean distance (ED), dynamic time warping (DTW), auto-encoders (AE) and a novel hybrid approach, warp-AE. We find that ED under-performs DTW and AE but warp-AE shows the best overall performance in a real-world financial task.},
  archiveprefix = {arXiv},
  keywords = {[rnn],/unread,\#nosource,⛔ No DOI found,Computer Science - Machine Learning,Quantitative Finance - Computational Finance,Quantitative Finance - Portfolio Management,Statistics - Machine Learning},
  note = {Comment: 33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada. arXiv admin note: text overlap with arXiv:1812.02340}
}

@article{pique2022,
  title = {Controlling {{Soft Robotic Arms Using Continual Learning}}},
  author = {Piqu{\'e}, Francesco and Kalidindi, Hari Teja and Fruzzetti, Lorenzo and Laschi, Cecilia and Menciassi, Arianna and Falotico, Egidio},
  year = {2022},
  journal = {IEEE Robotics and Automation Letters},
  volume = {7},
  number = {2},
  pages = {5469--5476},
  issn = {2377-3766},
  doi = {10/gpt3cq},
  url = {https://ieeexplore.ieee.org/document/9730039},
  abstract = {Learning-based modeling and control of soft robots is advantageous due to neural network's ability to capture complex dynamical effects with low computational cost. Continual Learning techniques add further value to these methods by allowing networks to learn from continuously available data without incurring into catastrophic forgetting. In the context of soft robotic control, such capability can be exploited to design controllers able to continuously adapt to changes in robot dynamics, frequently due to material degradation or external interactions. This should be done without forgetting the control under normal working conditions which can be recovered as soon as the external interactions return to normal. In this letter elastic weight consolidation is used to continuously re-tune a neural network-based controller while changing the external loading of a soft robot. We demonstrate experimentally on a soft robot arm that this method outperforms plain stochastic gradient descent in tracking tasks, in the context of a continuously changing loading condition. We also show that the proposed control architecture can improve its performances when exposed to loading conditions already experienced. This letter represents a first step towards the introduction of continual learning methods in the soft robot control field.},
  keywords = {/unread,and learning for soft robots,Computational modeling,control,learning and adaptive systems,Loading,Mathematical models,Modeling,Robots,soft robot applications,Soft robotics,Task analysis,Training},
  annotation = {ZSCC:00005}
}

@article{pomponi2020,
  title = {Efficient Continual Learning in Neural Networks with Embedding Regularization},
  author = {Pomponi, Jary and Scardapane, Simone and Lomonaco, Vincenzo and Uncini, Aurelio},
  year = {2020},
  journal = {Neurocomputing},
  issn = {0925-2312},
  doi = {10/gnq325},
  url = {http://www.sciencedirect.com/science/article/pii/S092523122030151X},
  abstract = {Continual learning of deep neural networks is a key requirement for scaling them up to more complex applicative scenarios and for achieving real lifelong learning of these architectures. Previous approaches to the problem have considered either the progressive increase in the size of the networks, or have tried to regularize the network behavior to equalize it with respect to previously observed tasks. In the latter case, it is essential to understand what type of information best represents this past behavior. Common techniques include regularizing the past outputs, gradients, or individual weights. In this work, we propose a new, relatively simple and efficient method to perform continual learning by regularizing instead the network internal embeddings. To make the approach scalable, we also propose a dynamic sampling strategy to reduce the memory footprint of the required external storage. We show that our method performs favorably with respect to state-of-the-art approaches in the literature, while requiring significantly less space in memory and computational time. In addition, inspired by to recent works, we evaluate the impact of selecting a more flexible model for the activation functions inside the network, evaluating the impact of catastrophic forgetting on the activation functions themselves.},
  langid = {english},
  keywords = {[cifar],[mnist],/unread,\#nosource,Catastrophic forgetting,Continual learning,Embedding,Regularization,Trainable activation functions}
}

@article{pomponi2020a,
  title = {Efficient Continual Learning in Neural Networks with Embedding Regularization},
  author = {Pomponi, Jary and Scardapane, Simone and Lomonaco, Vincenzo and Uncini, Aurelio},
  year = {2020},
  journal = {Neurocomputing},
  volume = {397},
  pages = {139--148},
  issn = {09252312},
  doi = {10/gnq325},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S092523122030151X},
  urldate = {2022-01-26},
  abstract = {Continual learning of deep neural networks is a key requirement for scaling them up to more complex applicative scenarios and for achieving real lifelong learning of these architectures. Previous approaches to the problem have considered either the progressive increase in the size of the networks, or have tried to regularize the network behavior to equalize it with respect to previously observed tasks. In the latter case, it is essential to understand what type of information best represents this past behavior. Common techniques include regularizing the past outputs, gradients, or individual weights. In this work, we propose a new, relatively simple and efficient method to perform continual learning by regularizing instead the network internal embeddings. To make the approach scalable, we also propose a dynamic sampling strategy to reduce the memory footprint of the required external storage. We show that our method performs favorably with respect to state-of-the-art approaches in the literature, while requiring significantly less space in memory and computational time. In addition, inspired by to recent works, we evaluate the impact of selecting a more flexible model for the activation functions inside the network, evaluating the impact of catastrophic forgetting on the activation functions themselves.},
  langid = {english},
  keywords = {/unread},
  annotation = {ZSCC: 0000022}
}

@article{pomponi2021,
  title = {Structured {{Ensembles}}: {{An}} Approach to Reduce the Memory Footprint of Ensemble Methods},
  shorttitle = {Structured {{Ensembles}}},
  author = {Pomponi, Jary and Scardapane, Simone and Uncini, Aurelio},
  year = {2021},
  journal = {Neural Networks},
  volume = {144},
  pages = {407--418},
  issn = {08936080},
  doi = {10/gn9qm3},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0893608021003579},
  urldate = {2022-01-26},
  abstract = {In this paper, we propose a novel ensembling technique for deep neural networks, which is able to drastically reduce the required memory compared to alternative approaches. In particular, we propose to extract multiple sub-networks from a single, untrained neural network by solving an end-to-end optimization task combining differentiable scaling over the original architecture, with multiple regularization terms favouring the diversity of the ensemble. Since our proposal aims to detect and extract sub-structures, we call it Structured Ensemble. On a large experimental evaluation, we show that our method can achieve higher or comparable accuracy to competing methods while requiring significantly less storage. In addition, we evaluate our ensembles in terms of predictive calibration and uncertainty, showing they compare favourably with the state-of-the-art. Finally, we draw a link with the continual learning literature, and we propose a modification of our framework to handle continuous streams of tasks with a sub-linear memory cost. We compare with a number of alternative strategies to mitigate catastrophic forgetting, highlighting advantages in terms of average accuracy and memory.},
  langid = {english},
  keywords = {/unread},
  annotation = {ZSCC: 0000000}
}

@inproceedings{prabhu2020,
  title = {{{GDumb}}: {{A Simple Approach}} That {{Questions Our Progress}} in {{Continual Learning}}},
  shorttitle = {{{GDumb}}},
  booktitle = {Computer {{Vision}} \textendash{} {{ECCV}} 2020},
  author = {Prabhu, Ameya and Torr, Philip H. S. and Dokania, Puneet K.},
  editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
  year = {2020},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {524--540},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10/ghnnbk},
  url = {https://link.springer.com/chapter/10.1007%2F978-3-030-58536-5_31},
  abstract = {We discuss a general formulation for the Continual Learning (CL) problem for classification\textemdash a learning task where a stream provides samples to a learner and the goal of the learner, depending on the samples it receives, is to continually upgrade its knowledge about the old classes and learn new ones. Our formulation takes inspiration from the open-set recognition problem where test scenarios do not necessarily belong to the training distribution. We also discuss various quirks and assumptions encoded in recently proposed approaches for CL. We argue that some oversimplify the problem to an extent that leaves it with very little practical importance, and makes it extremely easy to perform well on. To validate this, we propose GDumb that (1) greedily stores samples in memory as they come and; (2) at test time, trains a model from scratch using samples only in the memory. We show that even though GDumb is not specifically designed for CL problems, it obtains state-of-the-art accuracies (often with large margins) in almost all the experiments when compared to a multitude of recently proposed algorithms. Surprisingly, it outperforms approaches in CL formulations for which they were specifically designed. This, we believe, raises concerns regarding our progress in CL for classification. Overall, we hope our formulation, characterizations and discussions will help in designing realistically useful CL algorithms, and GDumb will serve as a strong contender for the same.},
  isbn = {978-3-030-58536-5},
  langid = {english},
  keywords = {/unread,\#nosource}
}

@inproceedings{rajasegaran2019,
  title = {Random {{Path Selection}} for {{Incremental Learning}}},
  booktitle = {{{NeurIPS}}},
  author = {Rajasegaran, Jathushan and Hayat, Munawar and Fahad, Salman Khan and Khan, Shahbaz and Shao, Ling},
  year = {2019},
  pages = {12669--12679},
  url = {http://papers.nips.cc/paper/9429-random-path-selection-for-continual-learning.pdf},
  abstract = {Incremental lifelong learning is a main challenge towards the long-standing goal of Artificial General Intelligence. In real-life settings, learning tasks arrive in a sequence and machine learning models must continually learn to increment already acquired knowledge. Existing incremental learning approaches, fall well below the state-of-the-art cumulative models that use all training classes at once. In this paper, we propose a random path selection algorithm, called RPS-Net, that progressively chooses optimal paths for the new tasks while encouraging parameter sharing. Since the reuse of previous paths enables forward knowledge transfer, our approach requires a considerably lower computational overhead. As an added novelty, the proposed model integrates knowledge distillation and retrospection along with the path selection strategy to overcome catastrophic forgetting. In order to maintain an equilibrium between previous and newly acquired knowledge, we propose a simple controller to dynamically balance the model plasticity. Through extensive experiments, we demonstrate that the proposed method surpasses the state-of-the-art performance on incremental learning and by utilizing parallel computation this method can run in constant time with nearly the same efficiency as a conventional deep convolutional neural network.},
  keywords = {[cifar],[imagenet],[mnist],/unread,\#nosource,⛔ No DOI found}
}

@inproceedings{rajasegaran2020,
  title = {{{iTAML}}: {{An Incremental Task-Agnostic Meta-learning Approach}}},
  booktitle = {{{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Rajasegaran, Jathushan and Khan, Salman and Hayat, Munawar and Khan, Fahad Shahbaz and Shah, Mubarak},
  year = {2020},
  pages = {13588---13597},
  url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Rajasegaran_iTAML_An_Incremental_Task-Agnostic_Meta-learning_Approach_CVPR_2020_paper.html},
  abstract = {Humans can continuously learn new knowledge as their experience grows. In contrast, previous learning in deep neural networks can quickly fade out when they are trained on a new task. In this paper, we hypothesize this problem can be avoided by learning a set of generalized parameters, that are neither specific to old nor new tasks. In this pursuit, we introduce a novel meta-learning approach that seeks to maintain an equilibrium between all the encountered tasks. This is ensured by a new meta-update rule which avoids catastrophic forgetting. In comparison to previous meta-learning techniques, our approach is task-agnostic. When presented with a continuum of data, our model automatically identifies the task and quickly adapts to it with just a single update. We perform extensive experiments on five datasets in a class-incremental setting, leading to significant improvements over the state of the art methods (e.g., a 21.3\% boost on CIFAR100 with 10 incremental tasks). Specifically, on large-scale datasets that generally prove difficult cases for incremental learning, our approach delivers absolute gains as high as 19.1\% and 7.4\% on ImageNet and MS-Celeb datasets, respectively.},
  keywords = {[cifar],[imagenet],/unread,\#nosource,⛔ No DOI found}
}

@article{ramapuram2017,
  title = {Lifelong {{Generative Modeling}}},
  author = {Ramapuram, Jason and Gregorova, Magda and Kalousis, Alexandros},
  year = {2017},
  journal = {arXiv},
  number = {2010},
  pages = {1--14},
  url = {http://arxiv.org/abs/1705.09847},
  abstract = {Lifelong learning is the problem of learning multiple consecutive tasks in an online manner and is essential towards the development of intelligent machines that can adapt to their surroundings. In this work we focus on learning a lifelong approach to generative modeling whereby we continuously incorporate newly observed distributions into our model representation. We utilize two models, aptly named the student and the teacher, in order to aggregate information about all past distributions without the preservation of any of the past data or previous models. The teacher is utilized as a form of compressed memory in order to allow for the student model to learn over the past as well as present data. We demonstrate why a naive approach to lifelong generative modeling fails and introduce a regularizer with which we demonstrate learning across a long range of distributions.},
  keywords = {[fashion],[generative],[mnist],/unread,\#nosource,⛔ No DOI found},
  annotation = {\_eprint: 1705.09847}
}

@inproceedings{ramasesh2021,
  title = {Anatomy of {{Catastrophic Forgetting}}: {{Hidden Representations}} and {{Task Semantics}}},
  shorttitle = {Anatomy of {{Catastrophic Forgetting}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Ramasesh, Vinay Venkatesh and Dyer, Ethan and Raghu, Maithra},
  year = {2021},
  url = {https://openreview.net/forum?id=LhY8QdUGSuw},
  urldate = {2022-06-30},
  abstract = {Catastrophic forgetting is a recurring challenge to developing versatile deep learning models. Despite its ubiquity, there is limited understanding of its connections to neural network (hidden)...},
  langid = {english},
  keywords = {/unread}
}

@inproceedings{rao2019,
  title = {Continual {{Unsupervised Representation Learning}}},
  booktitle = {{{NeurIPS}}},
  author = {Rao, Dushyant and Visin, Francesco and Rusu, Andrei A and Teh, Yee Whye and Pascanu, Razvan and Hadsell, Raia},
  year = {2019},
  url = {https://papers.nips.cc/paper/8981-continual-unsupervised-representation-learning.pdf},
  abstract = {Continual learning aims to improve the ability of modern learning systems to deal with non-stationary distributions, typically by attempting to learn a series of tasks sequentially. Prior art in the field has largely considered supervised or reinforcement learning tasks, and often assumes full knowledge of task labels and boundaries. In this work, we propose an approach (CURL) to tackle a more general problem that we will refer to as unsupervised continual learning. The focus is on learning representations without any knowledge about task identity, and we explore scenarios when there are abrupt changes between tasks, smooth transitions from one task to another, or even when the data is shuffled. The proposed approach performs task inference directly within the model, is able to dynamically expand to capture new concepts over its lifetime, and incorporates additional rehearsal-based techniques to deal with catastrophic forgetting. We demonstrate the efficacy of CURL in an unsupervised learning setting with MNIST and Omniglot, where the lack of labels ensures no information is leaked about the task. Further, we demonstrate strong performance compared to prior art in an i.i.d setting, or when adapting the technique to supervised tasks such as incremental class learning.},
  keywords = {[mnist],[omniglot],/unread,\#nosource,⛔ No DOI found}
}

@article{ratcliff1990,
  title = {Connectionist Models of Recognition Memory: Constraints Imposed by Learning and Forgetting Functions.},
  author = {Ratcliff, R},
  year = {1990},
  journal = {Psychological review},
  volume = {97},
  number = {2},
  pages = {285--308},
  issn = {0033-295X},
  url = {http://www.ncbi.nlm.nih.gov/pubmed/2186426},
  abstract = {Multilayer connectionist models of memory based on the encoder model using the backpropagation learning rule are evaluated. The models are applied to standard recognition memory procedures in which items are studied sequentially and then tested for retention. Sequential learning in these models leads to 2 major problems. First, well-learned information is forgotten rapidly as new information is learned. Second, discrimination between studied items and new items either decreases or is nonmonotonic as a function of learning. To address these problems, manipulations of the network within the multilayer model and several variants of the multilayer model were examined, including a model with prelearned memory and a context model, but none solved the problems. The problems discussed provide limitations on connectionist models applied to human memory and in tasks where information to be learned is not all available during learning.},
  pmid = {2186426},
  keywords = {/unread,\#nosource}
}

@article{ratcliff1990a,
  title = {Connectionist Models of Recognition Memory: Constraints Imposed by Learning and Forgetting Functions},
  shorttitle = {Connectionist Models of Recognition Memory},
  author = {Ratcliff, R.},
  year = {1990},
  journal = {Psychological Review},
  volume = {97},
  number = {2},
  pages = {285--308},
  issn = {0033-295X},
  doi = {10/ftqr8v},
  url = {https://europepmc.org/article/MED/2186426},
  abstract = {Multilayer connectionist models of memory based on the encoder model using the backpropagation learning rule are evaluated. The models are applied to standard recognition memory procedures in which items are studied sequentially and then tested for retention. Sequential learning in these models leads to 2 major problems. First, well-learned information is forgotten rapidly as new information is learned. Second, discrimination between studied items and new items either decreases or is nonmonotonic as a function of learning. To address these problems, manipulations of the network within the multilayer model and several variants of the multilayer model were examined, including a model with prelearned memory and a context model, but none solved the problems. The problems discussed provide limitations on connectionist models applied to human memory and in tasks where information to be learned is not all available during learning.},
  langid = {english},
  pmid = {2186426},
  keywords = {/unread,Attention,Humans,Memory,Mental Recall,Retention; Psychology,Serial Learning},
  annotation = {ZSCC: 0000893}
}

@inproceedings{rebuffi2017,
  title = {{{iCaRL}}: {{Incremental Classifier}} and {{Representation Learning}}},
  booktitle = {The {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Rebuffi, Sylvestre-Alvise and Kolesnikov, Alexander and Sperl, Georg and Lampert, Christoph H},
  year = {2017},
  url = {http://openaccess.thecvf.com/content_cvpr_2017/papers/Rebuffi_iCaRL_Incremental_Classifier_CVPR_2017_paper.pdf},
  keywords = {[cifar],/unread,\#nosource},
  note = {Official Code\\
\href{https://github.com/srebuffi/iCaRL}{https://github.com/srebuffi/iCaRL}}
}

@article{ren2020,
  title = {Wandering within a World: {{Online}} Contextualized Few-Shot Learning},
  author = {Ren, Mengye and Iuzzolino, Michael L and Mozer, Michael C and Zemel, Richard S},
  year = {2020},
  journal = {arXiv},
  url = {https://arxiv.org/abs/2007.04546},
  abstract = {We aim to bridge the gap between typical human and machine-learning environments by extending the standard framework of few-shot learning to an online, continual setting. In this setting, episodes do not have separate training and testing phases, and instead models are evaluated online while learning novel classes. As in real world, where the presence of spatiotemporal context helps us retrieve learned skills in the past, our online few-shot learning setting also features an underlying context that changes throughout time. Object classes are correlated within a context and inferring the correct context can lead to better performance. Building upon this setting, we propose a new few-shot learning dataset based on large scale indoor imagery that mimics the visual experience of an agent wandering within a world. Furthermore, we convert popular few-shot learning approaches into online versions and we also propose a new model named contextual prototypical memory that can make use of spatiotemporal contextual information from the recent past.},
  keywords = {[omniglot],/unread,\#nosource,⛔ No DOI found},
  annotation = {\_eprint: 2007.04546}
}

@misc{renn2022,
  title = {The {{Multiple Subnetwork Hypothesis}}: {{Enabling Multidomain Learning}} by {{Isolating Task-Specific Subnetworks}} in {{Feedforward Neural Networks}}},
  shorttitle = {The {{Multiple Subnetwork Hypothesis}}},
  author = {Renn, Jacob and Sotnek, Ian and Harvey, Benjamin and Caffo, Brian},
  year = {2022},
  number = {arXiv:2207.08821},
  eprint = {2207.08821},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2207.08821},
  urldate = {2022-07-21},
  abstract = {Neural networks have seen an explosion of usage and research in the past decade, particularly within the domains of computer vision and natural language processing. However, only recently have advancements in neural networks yielded performance improvements beyond narrow applications and translated to expanded multitask models capable of generalizing across multiple data types and modalities. Simultaneously, it has been shown that neural networks are overparameterized to a high degree, and pruning techniques have proved capable of significantly reducing the number of active weights within the network while largely preserving performance. In this work, we identify a methodology and network representational structure which allows a pruned network to employ previously unused weights to learn subsequent tasks. We employ these methodologies on well-known benchmarking datasets for testing purposes and show that networks trained using our approaches are able to learn multiple tasks, which may be related or unrelated, in parallel or in sequence without sacrificing performance on any task or exhibiting catastrophic forgetting.},
  archiveprefix = {arXiv},
  keywords = {[sparsity],/unread,Computer Science - Artificial Intelligence,Computer Science - Machine Learning}
}

@inproceedings{riemer2019,
  title = {Learning to Learn without Forgetting by Maximizing Transfer and Minimizing Interference},
  booktitle = {{{ICLR}}},
  author = {Riemer, Matthew and Cases, Ignacio and Ajemian, Robert and Liu, Miao and Rish, Irina and Tu, Yuhai and Tesauro, Gerald},
  year = {2019},
  url = {https://openreview.net/pdf?id=B1gTShAct7},
  abstract = {Lack of performance when it comes to continual learning over non-stationary distributions of data remains a major challenge in scaling neural network learning to more human realistic settings. In this work we propose a new conceptualization of the continual learning problem in terms of a temporally symmetric trade-off between transfer and interference that can be optimized by enforcing gradient alignment across examples. We then propose a new algorithm, Meta-Experience Replay (MER), that directly exploits this view by combining experience replay with optimization based meta-learning. This method learns parameters that make interference based on future gradients less likely and transfer based on future gradients more likely. 1 We conduct experiments across continual lifelong supervised learning benchmarks and non-stationary reinforcement learning environments demonstrating that our approach consistently outperforms recently proposed baselines for continual learning. Our experiments show that the gap between the performance of MER and baseline algorithms grows both as the environment gets more non-stationary and as the fraction of the total experiences stored gets smaller.},
  keywords = {[mnist],/unread,\#nosource}
}

@phdthesis{ring1994,
  title = {Continual {{Learning}} in {{Reinforcement Environments}}},
  author = {Ring, Mark},
  year = {1994},
  journal = {University of Texas},
  volume = {1},
  url = {https://www.cs.utexas.edu/ ring/Ring-dissertation.pdf},
  abstract = {Continual learning is the constant development of complex behaviors with no final end in mind. It is the process of learning ever more complicated skills by building on those skills already developed. In order for learning at one stage of development to serve as the foundation for later learning, a continual-learning agent should learn hierarchically. CHILD, an agent capable of Continual, Hierarchical, Incremental Learning and Development is proposed, described, tested, and evaluated in this dissertation. CHILD accumulates useful behaviors in reinforcement environments by using the Temporal Transition Hierarchies learning algorithm, also derived in the dissertation. This constructive algorithm generates a hierarchical, higher-order neural network that can be used for predicting context-dependent temporal sequences and can learn sequential-task benchmarks more than two orders of magnitude faster than competing neural network systems. Consequently, CHILD can quickly solve complicated non-Markovian reinforcement-learning tasks and can then transfer its skills to similar but even more complicated tasks, learning these faster still. This continual-learning approach is made possible by the unique properties of Temporal Transition Hierarchies, which allow existing skills to be amended and augmented in precisely the same way that they were constructed in the first place.},
  school = {University of Texas},
  keywords = {[framework],/unread,\#nosource}
}

@article{ring1997,
  title = {{{CHILD}}: {{A First Step Towards Continual Learning}}},
  shorttitle = {{{CHILD}}},
  author = {Ring, Mark B},
  year = {1997},
  journal = {Machine Learning},
  volume = {28},
  number = {1},
  pages = {77--104},
  issn = {1573-0565},
  doi = {10/dzpvrj},
  url = {https://doi.org/10.1023/A:1007331723572},
  abstract = {Continual learning is the constant development of increasingly complex behaviors; the process of building more complicated skills on top of those already developed. A continual-learning agent should therefore learn incrementally and hierarchically. This paper describes CHILD, an agent capable of Continual, Hierarchical, Incremental Learning and Development. CHILD can quickly solve complicated non-Markovian reinforcement-learning tasks and can then transfer its skills to similar but even more complicated tasks, learning these faster still.},
  langid = {english},
  keywords = {/unread,\#nosource,cl,continual learner,Continual learning,definition,hierarchical neural networks,reinforcement learning,sequence learning,transfer}
}

@article{ritter2018,
  title = {Online {{Structured Laplace Approximations For Overcoming Catastrophic Forgetting}}},
  author = {Ritter, Hippolyt and Botev, Aleksandar and Barber, David},
  year = {2018},
  journal = {arXiv},
  url = {http://arxiv.org/abs/1805.07810},
  abstract = {We introduce the Kronecker factored online Laplace approximation for overcoming catastrophic forgetting in neural networks. The method is grounded in a Bayesian online learning framework, where we recursively approximate the posterior after every task with a Gaussian, leading to a quadratic penalty on changes to the weights. The Laplace approximation requires calculating the Hessian around a mode, which is typically intractable for modern architectures. In order to make our method scalable, we leverage recent block-diagonal Kronecker factored approximations to the curvature. Our algorithm achieves over 90\{\$\textbackslash backslash\$\%\} test accuracy across a sequence of 50 instantiations of the permuted MNIST dataset, substantially outperforming related methods for overcoming catastrophic forgetting.},
  keywords = {[bayes],[mnist],/unread,\#nosource,⛔ No DOI found},
  annotation = {\_eprint: 1805.07810}
}

@inproceedings{roady2020,
  title = {Stream-51: {{Streaming Classification}} and {{Novelty Detection From Videos}}},
  shorttitle = {Stream-51},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}}},
  author = {Roady, Ryne and Hayes, Tyler L. and Vaidya, Hitesh and Kanan, Christopher},
  year = {2020},
  pages = {228--229},
  url = {https://openaccess.thecvf.com/content_CVPRW_2020/html/w15/Roady_Stream-51_Streaming_Classification_and_Novelty_Detection_From_Videos_CVPRW_2020_paper.html},
  urldate = {2021-11-29},
  keywords = {/unread,\#nosource}
}

@article{roady2020a,
  title = {Open {{Set Classification}} for {{Deep Learning}} in {{Large-Scale}} and {{Continual Learning Models}}},
  author = {Roady, Ryne},
  year = {2020},
  journal = {Theses},
  url = {https://scholarworks.rit.edu/theses/10592},
  keywords = {/unread,⛔ No DOI found}
}

@article{robins1995,
  title = {Catastrophic {{Forgetting}}; {{Catastrophic Interference}}; {{Stability}}; {{Plasticity}}; {{Rehearsal}}.},
  author = {Robins, Anthony},
  year = {1995},
  journal = {Connection Science},
  volume = {7},
  number = {2},
  pages = {123--146},
  issn = {0954-0091, 1360-0494},
  doi = {10/cz395d},
  url = {http://www.tandfonline.com/doi/abs/10.1080/09540099550039318},
  abstract = {This paper reviews the problem of catastrophic forgetting (the loss or disruption of previously learned information when new information is learned) in neural networks, and explores rehearsal mechanisms (the retraining of some of the previously learned information as the new information is added) as a potential solution. W e replicate some of the experiments described by Ratcliff (1990), including those relating to a simple `recency' based rehearsal regime. We then develop further rehearsal regimes which are more effective than recency rehearsal. In particular, `sweep rehearsal' is very successful at minimizing catastrophic forgetting. One possible limitation of rehearsal in general, however, is that previously learned information may not be available for retraining. W e describe a solution to this problem, `pseudorehearsal' , a method which provides the advantages of rehearsal without actually requiring any access to the previously learned information (the original training population) itself. We then suggest an interpretation of these rehearsal mechanisms in the context of a function approximation based account of neural network learning. Both rehearsal and pseudorehearsal may have practical applications, allowing new information to be integrated into an existing network with minimum disruption of old informa tion.},
  langid = {english},
  keywords = {[dual],/unread,\#nosource},
  note = {An in-depth overview of rehearsal techniques, including pseudo-rehearsal. Useful to combine these ideas in dual systems, in which one system learns a task while the other provides rehearsal of previous patterns.}
}

@inproceedings{rolnick2019,
  title = {Experience {{Replay}} for {{Continual Learning}}},
  booktitle = {{{NeurIPS}}},
  author = {Rolnick, David and Ahuja, Arun and Schwarz, Jonathan and Lillicrap, Timothy P and Wayne, Greg},
  year = {2019},
  pages = {350--360},
  url = {http://papers.nips.cc/paper/8327-experience-replay-for-continual-learning.pdf},
  abstract = {Interacting with a complex world involves continual learning, in which tasks and data distributions change over time. A continual learning system should demonstrate both plasticity (acquisition of new knowledge) and stability (preservation of old knowledge). Catastrophic forgetting is the failure of stability, in which new experience overwrites previous experience. In the brain, replay of past experience is widely believed to reduce forgetting, yet it has been largely overlooked as a solution to forgetting in deep reinforcement learning. Here, we introduce CLEAR, a replay-based method that greatly reduces catastrophic forgetting in multi-task reinforcement learning. CLEAR leverages off-policy learning and behavioral cloning from replay to enhance stability, as well as on-policy learning to preserve plasticity. We show that CLEAR performs better than state-of-the-art deep learning techniques for mitigating forgetting, despite being significantly less complicated and not requiring any knowledge of the individual tasks being learned.},
  keywords = {/unread,\#nosource,⛔ No DOI found}
}

@inproceedings{rosasco2021,
  title = {Distilled {{Replay}}: {{Overcoming Forgetting}} through {{Synthetic Samples}}},
  shorttitle = {Distilled {{Replay}}},
  booktitle = {1st {{International Workshop}} on {{Continual Semi-Supervised Learning}} ({{CSSL}}) at {{IJCAI}}},
  author = {Rosasco, Andrea and Carta, Antonio and Cossu, Andrea and Lomonaco, Vincenzo and Bacciu, Davide},
  year = {2021},
  eprint = {2103.15851},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2103.15851},
  urldate = {2021-08-01},
  abstract = {Replay strategies are Continual Learning techniques which mitigate catastrophic forgetting by keeping a buffer of patterns from previous experiences, which are interleaved with new data during training. The amount of patterns stored in the buffer is a critical parameter which largely influences the final performance and the memory footprint of the approach. This work introduces Distilled Replay, a novel replay strategy for Continual Learning which is able to mitigate forgetting by keeping a very small buffer (1 pattern per class) of highly informative samples. Distilled Replay builds the buffer through a distillation process which compresses a large dataset into a tiny set of informative examples. We show the effectiveness of our Distilled Replay against popular replay-based strategies on four Continual Learning benchmarks.},
  archiveprefix = {arXiv},
  copyright = {All rights reserved},
  keywords = {/unread,\#nosource,⛔ No DOI found,Computer Science - Artificial Intelligence,Computer Science - Machine Learning}
}

@article{rostami2019,
  title = {Complementary {{Learning}} for {{Overcoming Catastrophic Forgetting Using Experience Replay}}},
  author = {Rostami, Mohammad and Kolouri, Soheil and Pilly, Praveen K},
  year = {2019},
  journal = {arXiv},
  url = {http://arxiv.org/abs/1903.04566},
  abstract = {Despite huge success, deep networks are unable to learn effectively in sequential multitask learning settings as they forget the past learned tasks after learning new tasks. Inspired from complementary learning systems theory, we address this challenge by learning a generative model that couples the current task to the past learned tasks through a discriminative embedding space. We learn an abstract level generative distribution in the embedding that allows the generation of data points to represent the experience. We sample from this distribution and utilize experience replay to avoid forgetting and simultaneously accumulate new knowledge to the abstract distribution in order to couple the current task with past experience. We demonstrate theoretically and empirically that our framework learns a distribution in the embedding that is shared across all task and as a result tackles catastrophic forgetting.},
  keywords = {/unread,\#nosource,⛔ No DOI found},
  annotation = {\_eprint: 1903.04566}
}

@article{rostami2019a,
  title = {Complementary {{Learning}} for {{Overcoming Catastrophic Forgetting Using Experience Replay}}},
  author = {Rostami, Mohammad and Kolouri, Soheil and Pilly, Praveen K.},
  year = {2019},
  journal = {arXiv},
  doi = {10/gpt3ct},
  url = {https://arxiv.org/abs/1903.04566v2},
  urldate = {2022-03-22},
  abstract = {Despite huge success, deep networks are unable to learn effectively in sequential multitask learning settings as they forget the past learned tasks after learning new tasks. Inspired from complementary learning systems theory, we address this challenge by learning a generative model that couples the current task to the past learned tasks through a discriminative embedding space. We learn an abstract level generative distribution in the embedding that allows the generation of data points to represent the experience. We sample from this distribution and utilize experience replay to avoid forgetting and simultaneously accumulate new knowledge to the abstract distribution in order to couple the current task with past experience. We demonstrate theoretically and empirically that our framework learns a distribution in the embedding that is shared across all task and as a result tackles catastrophic forgetting.},
  langid = {english},
  keywords = {/unread}
}

@article{rusu2016,
  title = {Progressive {{Neural Networks}}},
  author = {Rusu, Andrei A and Rabinowitz, Neil C and Desjardins, Guillaume and Soyer, Hubert and Kirkpatrick, James and Kavukcuoglu, Koray and Pascanu, Razvan and Hadsell, Raia},
  year = {2016},
  journal = {arXiv},
  url = {http://arxiv.org/abs/1606.04671},
  abstract = {Learning to solve complex sequences of tasks\textemdash while both leveraging transfer and avoiding catastrophic forgetting\textemdash remains a key obstacle to achieving human-level intelligence. The progressive networks approach represents a step forward in this direction: they are immune to forgetting and can leverage prior knowledge via lateral connections to previously learned features. We evaluate this architecture extensively on a wide variety of reinforcement learning tasks (Atari and 3D maze games), and show that it outperforms common baselines based on pretraining and finetuning. Using a novel sensitivity measure, we demonstrate that transfer occurs at both low-level sensory and high-level control layers of the learned policy.},
  langid = {english},
  keywords = {[mnist],/unread,\#nosource,⛔ No DOI found,Computer Science - Machine Learning,lifelong learning,modular,progressive},
  note = {The authors rely on a separate feedforward network (column) for each task the model is trained on. Each column is connected through adaptive connections to all the previous ones. The weights of previous columns are frozen once trained. At inference time, given a known task label, the network choose the appropriate column to produce the output, thus preventing forgetting by design.
\par
The authors rely on a separate feedforward network (column) for each task the model is trained on. Each column is connected through adaptive connections to all the previous ones. The weights of previous columns are frozen once trained. At inference time, given a known task label, the network choose the appropriate column to produce the output, thus preventing forgetting by design.}
}

@inproceedings{ruvolo2013,
  title = {{{ELLA}}: {{An Efficient Lifelong Learning Algorithm}}},
  shorttitle = {{{ELLA}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Ruvolo, Paul and Eaton, Eric},
  year = {2013},
  pages = {507--515},
  publisher = {{PMLR}},
  issn = {1938-7228},
  url = {http://proceedings.mlr.press/v28/ruvolo13.html},
  urldate = {2021-06-27},
  abstract = {The problem of learning multiple consecutive tasks, known as lifelong learning, is of great importance to the creation of intelligent, general-purpose, and flexible machines.  In this paper, we dev...},
  langid = {english},
  keywords = {/unread,\#nosource}
}

@inproceedings{saha2020,
  title = {Gradient {{Projection Memory}} for {{Continual Learning}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Saha, Gobinda and Roy, Kaushik},
  year = {2020},
  url = {https://openreview.net/forum?id=3AOj0RCNC2},
  urldate = {2021-01-17},
  abstract = {The ability to learn continually without forgetting the past tasks is a desired attribute for artificial learning systems. Existing approaches to enable such learning in artificial neural networks...},
  langid = {english},
  keywords = {/unread,\#nosource}
}

@article{saha2020a,
  title = {Structured {{Compression}} and {{Sharing}} of {{Representational Space}} for {{Continual Learning}}},
  author = {Saha, Gobinda and Garg, Isha and Ankit, Aayush and Roy, Kaushik},
  year = {2020},
  journal = {arXiv},
  url = {http://arxiv.org/abs/2001.08650},
  abstract = {Humans are skilled at learning adaptively and efficiently throughout their lives, but learning tasks incrementally causes artificial neural networks to overwrite relevant information learned about older tasks, resulting in 'Catastrophic Forgetting'. Efforts to overcome this phenomenon suffer from poor utilization of resources in many ways, such as through the need to save older data or parametric importance scores, or to grow the network architecture. We propose an algorithm that enables a network to learn continually and efficiently by partitioning the representational space into a Core space, that contains the condensed information from previously learned tasks, and a Residual space, which is akin to a scratch space for learning the current task. The information in the Residual space is then compressed using Principal Component Analysis and added to the Core space, freeing up parameters for the next task. We evaluate our algorithm on P-MNIST, CIFAR-10 and CIFAR-100 datasets. We achieve comparable accuracy to state-of-the-art methods while overcoming the problem of catastrophic forgetting completely. Additionally, we get up to 4.5x improvement in energy efficiency during inference due to the structured nature of the resulting architecture.},
  keywords = {[cifar],[mnist],/unread,\#nosource,⛔ No DOI found},
  annotation = {\_eprint: 2001.08650}
}

@article{saha2021,
  title = {Gradient {{Projection Memory}} for {{Continual Learning}}},
  author = {Saha, Gobinda and Garg, Isha and Roy, Kaushik},
  year = {2021},
  journal = {arXiv:2103.09762 [cs]},
  eprint = {2103.09762},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2103.09762},
  urldate = {2022-05-03},
  abstract = {The ability to learn continually without forgetting the past tasks is a desired attribute for artificial learning systems. Existing approaches to enable such learning in artificial neural networks usually rely on network growth, importance based weight update or replay of old data from the memory. In contrast, we propose a novel approach where a neural network learns new tasks by taking gradient steps in the orthogonal direction to the gradient subspaces deemed important for the past tasks. We find the bases of these subspaces by analyzing network representations (activations) after learning each task with Singular Value Decomposition (SVD) in a single shot manner and store them in the memory as Gradient Projection Memory (GPM). With qualitative and quantitative analyses, we show that such orthogonal gradient descent induces minimum to no interference with the past tasks, thereby mitigates forgetting. We evaluate our algorithm on diverse image classification datasets with short and long sequences of tasks and report better or on-par performance compared to the state-of-the-art approaches.},
  archiveprefix = {arXiv},
  keywords = {/unread,⛔ No DOI found,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  annotation = {ZSCC:00026},
  note = {Comment: Accepted for Oral Presentation at ICLR 2021 https://openreview.net/forum?id=3AOj0RCNC2}
}

@misc{saha2021a,
  title = {Gradient {{Projection Memory}} for {{Continual Learning}}},
  author = {Saha, Gobinda and Garg, Isha and Roy, Kaushik},
  year = {2021},
  number = {arXiv:2103.09762},
  eprint = {2103.09762},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2103.09762},
  urldate = {2022-07-08},
  abstract = {The ability to learn continually without forgetting the past tasks is a desired attribute for artificial learning systems. Existing approaches to enable such learning in artificial neural networks usually rely on network growth, importance based weight update or replay of old data from the memory. In contrast, we propose a novel approach where a neural network learns new tasks by taking gradient steps in the orthogonal direction to the gradient subspaces deemed important for the past tasks. We find the bases of these subspaces by analyzing network representations (activations) after learning each task with Singular Value Decomposition (SVD) in a single shot manner and store them in the memory as Gradient Projection Memory (GPM). With qualitative and quantitative analyses, we show that such orthogonal gradient descent induces minimum to no interference with the past tasks, thereby mitigates forgetting. We evaluate our algorithm on diverse image classification datasets with short and long sequences of tasks and report better or on-par performance compared to the state-of-the-art approaches.},
  archiveprefix = {arXiv},
  keywords = {/unread,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  note = {Comment: Accepted for Oral Presentation at ICLR 2021 https://openreview.net/forum?id=3AOj0RCNC2}
}

@misc{sangermano2022,
  title = {Sample {{Condensation}} in {{Online Continual Learning}}},
  author = {Sangermano, Mattia and Carta, Antonio and Cossu, Andrea and Bacciu, Davide},
  year = {2022},
  number = {arXiv:2206.11849},
  eprint = {2206.11849},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2206.11849},
  urldate = {2022-07-08},
  abstract = {Online Continual learning is a challenging learning scenario where the model must learn from a non-stationary stream of data where each sample is seen only once. The main challenge is to incrementally learn while avoiding catastrophic forgetting, namely the problem of forgetting previously acquired knowledge while learning from new data. A popular solution in these scenario is to use a small memory to retain old data and rehearse them over time. Unfortunately, due to the limited memory size, the quality of the memory will deteriorate over time. In this paper we propose OLCGM, a novel replay-based continual learning strategy that uses knowledge condensation techniques to continuously compress the memory and achieve a better use of its limited size. The sample condensation step compresses old samples, instead of removing them like other replay strategies. As a result, the experiments show that, whenever the memory budget is limited compared to the complexity of the data, OLCGM improves the final accuracy compared to state-of-the-art replay strategies.},
  archiveprefix = {arXiv},
  keywords = {/unread,Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  note = {Comment: Accepted as a conference paper at 2022 International Joint Conference on Neural Networks (IJCNN 2022). Part of 2022 IEEE World Congress on Computational Intelligence (IEEE WCCI 2022)}
}

@incollection{schak2019,
  title = {A {{Study}} on {{Catastrophic Forgetting}} in {{Deep LSTM Networks}}},
  booktitle = {Artificial {{Neural Networks}} and {{Machine Learning}} \textendash{} {{ICANN}} 2019: {{Deep Learning}}},
  author = {Schak, Monika and Gepperth, Alexander},
  editor = {Tetko, Igor V and K{\r{u}}rkov{\'a}, V{\v e}ra and Karpov, Pavel and Theis, Fabian},
  year = {2019},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {714--728},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-30484-3_56},
  url = {http://link.springer.com/10.1007/978-3-030-30484-3_56},
  abstract = {We present a systematic study of Catastrophic Forgetting (CF), i.e., the abrupt loss of previously acquired knowledge, when retraining deep recurrent LSTM networks with new samples. CF has recently received renewed attention in the case of feed-forward DNNs, and this article is the first work that aims to rigorously establish whether deep LSTM networks are afflicted by CF as well, and to what degree. In order to test this fully, training is conducted using a wide variety of high-dimensional image-based sequence classification tasks derived from established visual classification benchmarks (MNIST, Devanagari, FashionMNIST and EMNIST). We find that the CF effect occurs universally, without exception, for deep LSTM-based sequence classifiers, regardless of the construction and provenance of sequences. This leads us to conclude that LSTMs, just like DNNs, are fully affected by CF, and that further research work needs to be conducted in order to determine how to avoid this effect (which is not a goal of this study).},
  isbn = {978-3-030-30484-3},
  langid = {english},
  keywords = {[rnn],/unread,\#nosource,Catastrophic Forgetting,LSTM,sequential}
}

@inproceedings{schlegel2017,
  title = {Stable Predictive Representations with General Value Functions for Continual Learning},
  booktitle = {Continual {{Learning}} and {{Deep Networks}} Workshop at the {{Neural Information Processing System Conference}}},
  author = {Schlegel, Matthew and White, Adam and White, Martha},
  year = {2017},
  url = {https://sites.ualberta.ca/ amw8/cldl.pdf},
  abstract = {The objective of continual learning is to build agents that continually learn about their world, building on prior learning. In this paper, we explore an approach to continual learning based on making and updating many predictions formalized as general value functions (GVFs). The idea behind GVFs is simple: if we can cast the task of representing predictive knowledge as a prediction of future reward, then computationally efficient policy evaluation methods from reinforcement learning can be used to learn a large collection of predictions while the agent interacts with the world. We explore this idea further by analyzing how GVF predictions can be used as predictive features, and introduce two algorithmic techniques to ensure the stability of continual prediction learning. We illustrate these ideas with a small experiment in the cycle world domain.},
  keywords = {/unread,\#nosource,⛔ No DOI found}
}

@inproceedings{schwarz2018,
  title = {Progress \& {{Compress}}: {{A}} Scalable Framework for Continual Learning},
  shorttitle = {Progress \& {{Compress}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Schwarz, Jonathan and Czarnecki, Wojciech and Luketina, Jelena and {Grabska-Barwinska}, Agnieszka and Teh, Yee Whye and Pascanu, Razvan and Hadsell, Raia},
  year = {2018},
  pages = {4528--4537},
  url = {http://proceedings.mlr.press/v80/schwarz18a.html},
  abstract = {We introduce a conceptually simple and scalable framework for continual learning domains where tasks are learned sequentially. Our method is constant in the number of parameters and is designed to ...},
  langid = {english},
  keywords = {[vision],/unread,\#nosource,ewc,normalized ewc,online ewc}
}

@article{seff2017,
  title = {Continual {{Learning}} in {{Generative Adversarial Nets}}},
  author = {Seff, Ari and Beatson, Alex and Suo, Daniel and Liu, Han},
  year = {2017},
  journal = {arXiv},
  pages = {1--9},
  url = {http://arxiv.org/abs/1705.08395},
  abstract = {Developments in deep generative models have allowed for tractable learning of high-dimensional data distributions. While the employed learning procedures typically assume that training data is drawn i.i.d. from the distribution of interest, it may be desirable to model distinct distributions which are observed sequentially, such as when different classes are encountered over time. Although conditional variations of deep generative models permit multiple distributions to be modeled by a single network in a disentangled fashion, they are susceptible to catastrophic forgetting when the distributions are encountered sequentially. In this paper, we adapt recent work in reducing catastrophic forgetting to the task of training generative adversarial networks on a sequence of distinct distributions, enabling continual generative modeling.},
  keywords = {[mnist],/unread,\#nosource,⛔ No DOI found},
  annotation = {\_eprint: 1705.08395}
}

@inproceedings{serra2018,
  title = {Overcoming {{Catastrophic Forgetting}} with {{Hard Attention}} to the {{Task}}},
  booktitle = {{{ICML}}},
  author = {Serr{\`a}, Joan and Sur{\'i}s, D{\'i}dac and Miron, Marius and Karatzoglou, Alexandros},
  year = {2018},
  url = {https://arxiv.org/pdf/1801.01423.pdf},
  abstract = {Catastrophic forgetting occurs when a neural network loses the information learned in a previous task after training on subsequent tasks. This problem remains a hurdle for artificial intelligence systems with sequential learning capabilities. In this paper, we propose a task-based hard attention mechanism that preserves previous tasks' information without affecting the current task's learning. A hard attention mask is learned concurrently to every task, through stochastic gradient descent, and previous masks are exploited to condition such learning. We show that the proposed mechanism is effective for reducing catastrophic forgetting , cutting current rates by 45 to 80\%. We also show that it is robust to different hyperparameter choices, and that it offers a number of monitoring capabilities. The approach features the possibility to control both the stability and compactness of the learned knowledge, which we believe makes it also attractive for online learning or network compression applications.},
  keywords = {[cifar],[fashion],[mnist],/unread,\#nosource,⛔ No DOI found,serra2018a}
}

@article{shao2021,
  title = {The {{Traffic Flow Prediction Method Using}} the {{Incremental Learning-Based CNN-LTSM Model}}: {{The Solution}} of {{Mobile Application}}},
  shorttitle = {The {{Traffic Flow Prediction Method Using}} the {{Incremental Learning-Based CNN-LTSM Model}}},
  author = {Shao, Yanli and Zhao, Yiming and Yu, Feng and Zhu, Huawei and Fang, Jinglong},
  year = {2021},
  journal = {Mobile Information Systems},
  volume = {2021},
  pages = {e5579451},
  publisher = {{Hindawi}},
  issn = {1574-017X},
  doi = {10/gnq33n},
  url = {https://www.hindawi.com/journals/misy/2021/5579451/},
  urldate = {2021-06-05},
  abstract = {With the acceleration of urbanization and the increase in the number of motor vehicles, more and more social problems such as traffic congestion have emerged. Accordingly, efficient and accurate traffic flow prediction has become a research hot spot in the field of intelligent transportation. However, traditional machine learning algorithms cannot further optimize the model with the increase of the data scale, and the deep learning algorithms perform poorly in mobile application or real-time application; how to train and update deep learning models efficiently and accurately is still an urgent problem since they require huge computation resources and time costs. Therefore, an incremental learning-based CNN-LTSM model, IL-TFNet, is proposed for traffic flow prediction in this study. The lightweight convolution neural network-based model architecture is designed to process spatiotemporal and external environment features simultaneously to improve the prediction performance and prediction efficiency of the model. Especially, the K-means clustering algorithm is applied as an uncertainty feature to extract unknown traffic accident information. During the model training, instead of the traditional batch learning algorithm, the incremental learning algorithm is applied to reduce the cost of updating the model and satisfy the requirements of high real-time performance and low computational overhead in short-term traffic prediction. Furthermore, the idea of combining incremental learning with active learning is proposed to fine-tune the prediction model to improve prediction accuracy in special situations. Experiments have proved that compared with other traffic flow prediction models, the IL-TFNet model performs well in short-term traffic flow prediction.},
  langid = {english},
  keywords = {[experimental],/unread,\#nosource}
}

@article{she2019,
  title = {{{OpenLORIS-Object}}: {{A Robotic Vision Dataset}} and {{Benchmark}} for {{Lifelong Deep Learning}}},
  author = {She, Qi and Feng, Fan and Hao, Xinyue and Yang, Qihan and Lan, Chuanlin and Lomonaco, Vincenzo and Shi, Xuesong and Wang, Zhengwei and Guo, Yao and Zhang, Yimin and Qiao, Fei and Chan, Rosa H M},
  year = {2019},
  journal = {arXiv},
  pages = {1--8},
  url = {http://arxiv.org/abs/1911.06487},
  abstract = {The recent breakthroughs in computer vision have benefited from the availability of large representative datasets (e.g. ImageNet and COCO) for training. Yet, robotic vision poses unique challenges for applying visual algorithms developed from these standard computer vision datasets due to their implicit assumption over non-varying distributions for a fixed set of tasks. Fully retraining models each time a new task becomes available is infeasible due to computational, storage and sometimes privacy issues, while na\$\textbackslash backslash\$"\{i\}ve incremental strategies have been shown to suffer from catastrophic forgetting. It is crucial for the robots to operate continuously under open-set and detrimental conditions with adaptive visual perceptual systems, where lifelong learning is a fundamental capability. However, very few datasets and benchmarks are available to evaluate and compare emerging techniques. To fill this gap, we provide a new lifelong robotic vision dataset ("OpenLORIS-Object") collected via RGB-D cameras. The dataset embeds the challenges faced by a robot in the real-life application and provides new benchmarks for validating lifelong object recognition algorithms. Moreover, we have provided a testbed of \$9\$ state-of-the-art lifelong learning algorithms. Each of them involves \$48\$ tasks with \$4\$ evaluation metrics over the OpenLORIS-Object dataset. The results demonstrate that the object recognition task in the ever-changing difficulty environments is far from being solved and the bottlenecks are at the forward/backward transfer designs. Our dataset and benchmark are publicly available at at \$\textbackslash backslash\$href\{https://lifelong-robotic-vision.github.io/dataset/object\}\{\$\textbackslash backslash\$underline\{https://lifelong-robotic-vision.github.io/dataset/object\}\}.},
  keywords = {[vision],/unread,\#nosource,⛔ No DOI found},
  annotation = {\_eprint: 1911.06487}
}

@inproceedings{shen2019,
  title = {A {{Progressive Model}} to {{Enable Continual Learning}} for {{Semantic Slot Filling}}},
  booktitle = {Proceedings of the 2019 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} and the 9th {{International Joint Conference}} on {{Natural Language Processing}}},
  author = {Shen, Yilin and Zeng, Xiangyu and Jin, Hongxia},
  year = {2019},
  pages = {1279--1284},
  publisher = {{Association for Computational Linguistics}},
  url = {https://www.aclweb.org/anthology/D19-1126.pdf},
  abstract = {Semantic slot filling is one of the major tasks in spoken language understanding (SLU). After a slot filling model is trained on pre-collected data, it is crucial to continually improve the model after deployment to learn users' new expressions. As the data amount grows, it becomes infeasible to either store such huge data and repeatedly retrain the model on all data or fine tune the model only on new data without forgetting old expressions. In this paper, we introduce a novel progressive slot filling model, ProgModel. ProgModel consists of a novel context gate that transfers previously learned knowledge to a small size expanded component; and meanwhile enables this new component to be fast trained to learn from new data. As such, ProgModel learns the new knowledge by only using new data at each time and meanwhile preserves the previously learned expressions. Our experiments show that ProgModel needs much less training time and smaller model size to outperform various model fine tuning competitors by up to 4.24\% and 3.03\% on two benchmark datasets.},
  keywords = {[nlp],/unread,\#nosource,⛔ No DOI found}
}

@inproceedings{shi2021,
  title = {Continual {{Learning}} via {{Bit-Level Information Preserving}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Shi, Yujun and Yuan, Li and Chen, Yunpeng and Feng, Jiashi},
  year = {2021},
  pages = {16674--16683},
  url = {https://openaccess.thecvf.com/content/CVPR2021/html/Shi_Continual_Learning_via_Bit-Level_Information_Preserving_CVPR_2021_paper.html},
  urldate = {2021-10-14},
  langid = {english},
  keywords = {/unread,\#nosource}
}

@inproceedings{shin2017,
  title = {Continual {{Learning}} with {{Deep Generative Replay}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 30},
  author = {Shin, Hanul and Lee, Jung Kwon and Kim, Jaehong and Kim, Jiwon},
  editor = {Guyon, I and Luxburg, U V and Bengio, S and Wallach, H and Fergus, R and Vishwanathan, S and Garnett, R},
  year = {2017},
  pages = {2990--2999},
  publisher = {{Curran Associates, Inc.}},
  url = {http://papers.nips.cc/paper/6892-continual-learning-with-deep-generative-replay.pdf},
  keywords = {[mnist],/unread,\#nosource,⛔ No DOI found}
}

@article{shmelkov2017,
  title = {Incremental {{Learning}} of {{Object Detectors}} without {{Catastrophic Forgetting}}},
  author = {Shmelkov, Konstantin and Schmid, Cordelia and Alahari, Karteek},
  year = {2017},
  journal = {Proceedings of the IEEE International Conference on Computer Vision},
  volume = {2017-Octob},
  pages = {3420--3429},
  publisher = {{Institute of Electrical and Electronics Engineers Inc.}},
  issn = {15505499},
  doi = {10/gfx3wr},
  url = {http://arxiv.org/abs/1708.06977},
  abstract = {Despite their success for object detection, convolutional neural networks are ill-equipped for incremental learning, i.e., adapting the original model trained on a set of classes to additionally detect objects of new classes, in the absence of the initial training data. They suffer from "catastrophic forgetting" - an abrupt degradation of performance on the original set of classes, when the training objective is adapted to the new classes. We present a method to address this issue, and learn object detectors incrementally, when neither the original training data nor annotations for the original classes in the new training set are available. The core of our proposed solution is a loss function to balance the interplay between predictions on the new classes and a new distillation loss which minimizes the discrepancy between responses for old classes from the original and the updated networks. This incremental learning can be performed multiple times, for a new set of classes in each step, with a moderate drop in performance compared to the baseline network trained on the ensemble of data. We present object detection results on the PASCAL VOC 2007 and COCO datasets, along with a detailed empirical analysis of the approach.},
  isbn = {9781538610329},
  keywords = {/unread,\#nosource},
  annotation = {\_eprint: 1708.06977}
}

@incollection{shrestha2018,
  title = {{{SLAYER}}: {{Spike Layer Error Reassignment}} in {{Time}}},
  shorttitle = {{{SLAYER}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 31},
  author = {Shrestha, Sumit Bam and Orchard, Garrick},
  editor = {Bengio, S and Wallach, H and Larochelle, H and Grauman, K and {Cesa-Bianchi}, N and Garnett, R},
  year = {2018},
  pages = {1412--1421},
  publisher = {{Curran Associates, Inc.}},
  url = {http://papers.nips.cc/paper/7415-slayer-spike-layer-error-reassignment-in-time.pdf},
  keywords = {/unread,\#nosource}
}

@inproceedings{shu2016,
  title = {Lifelong-{{RL}}: {{Lifelong Relaxation Labeling}} for {{Separating Entities}} and {{Aspects}} in {{Opinion Targets}}.},
  booktitle = {Proceedings of the {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}. {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Shu, Lei and Liu, Bing and Xu, Hu and Kim, Annice},
  year = {2016},
  volume = {2016},
  pages = {225--235},
  issn = {1527-5418},
  doi = {10.1038/nrg3575.Systems},
  url = {http://www.ncbi.nlm.nih.gov/pubmed/29756130 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC5947972},
  abstract = {It is well-known that opinions have targets. Extracting such targets is an important problem of opinion mining because without knowing the target of an opinion, the opinion is of limited use. So far many algorithms have been proposed to extract opinion targets. However, an opinion target can be an entity or an aspect (part or attribute) of an entity. An opinion about an entity is an opinion about the entity as a whole, while an opinion about an aspect is just an opinion about that specific attribute or aspect of an entity. Thus, opinion targets should be separated into entities and aspects before use because they represent very different things about opinions. This paper proposes a novel algorithm, called Lifelong-RL, to solve the problem based on lifelong machine learning and relaxation labeling. Extensive experiments show that the proposed algorithm Lifelong-RL outperforms baseline methods markedly.},
  isbn = {978-1-4939-7371-2},
  pmid = {29756130},
  keywords = {[nlp],/unread,\#nosource,⚠️ Invalid DOI},
  annotation = {\_eprint: 15334406}
}

@inproceedings{slim2022,
  title = {Dataset {{Knowledge Transfer}} for {{Class-Incremental Learning}} without {{Memory}}},
  booktitle = {{{IEEE}}/{{CVF Winter Conference}} on {{Applications}} of {{Computer Vision}}, {{WACV}} 2022, {{Waikoloa}}, {{HI}}, {{USA}}, {{January}} 3-8, 2022},
  author = {Slim, Habib and Belouadah, Eden and Popescu, Adrian and Onchis, Darian M.},
  year = {2022},
  pages = {3311--3320},
  publisher = {{IEEE}},
  doi = {10/gpt3cr},
  url = {https://doi.org/10.1109/WACV51458.2022.00337},
  keywords = {/unread}
}

@article{smith2019,
  title = {Unsupervised {{Progressive Learning}} and the {{STAM Architecture}}},
  author = {Smith, James and Baer, Seth and Taylor, Cameron and Dovrolis, Constantine},
  year = {2019},
  journal = {arXiv},
  url = {http://arxiv.org/abs/1904.02021},
  abstract = {We first pose the Unsupervised Progressive Learning (UPL) problem: an online representation learning problem in which the learner observes a non-stationary and unlabeled data stream, and identifies a growing number of features that persist over time even though the data is not stored or replayed. To solve the UPL problem we propose the Self-Taught Associative Memory (STAM) architecture. Layered hierarchies of STAM modules learn based on a combination of online clustering, novelty detection, forgetting outliers, and storing only prototypical features rather than specific examples. We evaluate STAM representations using classification and clustering tasks. Even though there are no prior approaches that are directly applicable to the UPL problem, we evaluate the STAM architecture in comparison to some unsupervised and self-supervised deep learning approaches adapted in the UPL context.},
  keywords = {[mnist],/unread,\#nosource,⛔ No DOI found},
  annotation = {\_eprint: 1904.02021}
}

@article{sodhani2019,
  title = {Toward {{Training Recurrent Neural Networks}} for {{Lifelong Learning}}},
  author = {Sodhani, Shagun and Chandar, Sarath and Bengio, Yoshua},
  year = {2019},
  journal = {Neural Computation},
  volume = {32},
  number = {1},
  pages = {1--35},
  publisher = {{MIT Press}},
  issn = {0899-7667},
  doi = {10/ggh2mp},
  url = {https://doi.org/10.1162/neco_a_01246},
  urldate = {2021-01-06},
  abstract = {Catastrophic forgetting and capacity saturation are the central challenges of any parametric lifelong learning system. In this work, we study these challenges in the context of sequential supervised learning with an emphasis on recurrent neural networks. To evaluate the models in the lifelong learning setting, we propose a curriculum-based, simple, and intuitive benchmark where the models are trained on tasks with increasing levels of difficulty. To measure the impact of catastrophic forgetting, the model is tested on all the previous tasks as it completes any task. As a step toward developing true lifelong learning systems, we unify gradient episodic memory (a catastrophic forgetting alleviation approach) and Net2Net (a capacity expansion approach). Both models are proposed in the context of feedforward networks, and we evaluate the feasibility of using them for recurrent networks. Evaluation on the proposed benchmark shows that the unified model is more suitable than the constituent models for lifelong learning setting.},
  keywords = {[rnn],/unread,\#nosource}
}

@misc{sodhani2022,
  title = {An {{Introduction}} to {{Lifelong Supervised Learning}}},
  author = {Sodhani, Shagun and Faramarzi, Mojtaba and Mehta, Sanket Vaibhav and Malviya, Pranshu and Abdelsalam, Mohamed and Janarthanan, Janarthanan and Chandar, Sarath},
  year = {2022},
  number = {arXiv:2207.04354},
  eprint = {2207.04354},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2207.04354},
  urldate = {2022-07-13},
  abstract = {This primer is an attempt to provide a detailed summary of the different facets of lifelong learning. We start with Chapter 2 which provides a high-level overview of lifelong learning systems. In this chapter, we discuss prominent scenarios in lifelong learning (Section 2.4), provide 8 Introduction a high-level organization of different lifelong learning approaches (Section 2.5), enumerate the desiderata for an ideal lifelong learning system (Section 2.6), discuss how lifelong learning is related to other learning paradigms (Section 2.7), describe common metrics used to evaluate lifelong learning systems (Section 2.8). This chapter is more useful for readers who are new to lifelong learning and want to get introduced to the field without focusing on specific approaches or benchmarks. The remaining chapters focus on specific aspects (either learning algorithms or benchmarks) and are more useful for readers who are looking for specific approaches or benchmarks. Chapter 3 focuses on regularization-based approaches that do not assume access to any data from previous tasks. Chapter 4 discusses memory-based approaches that typically use a replay buffer or an episodic memory to save subset of data across different tasks. Chapter 5 focuses on different architecture families (and their instantiations) that have been proposed for training lifelong learning systems. Following these different classes of learning algorithms, we discuss the commonly used evaluation benchmarks and metrics for lifelong learning (Chapter 6) and wrap up with a discussion of future challenges and important research directions in Chapter 7.},
  archiveprefix = {arXiv},
  keywords = {/unread,Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  note = {Comment: Lifelong Learning Primer}
}

@article{sokar2021,
  title = {{{SpaceNet}}: {{Make Free Space}} for {{Continual Learning}}},
  shorttitle = {{{SpaceNet}}},
  author = {Sokar, Ghada and Mocanu, Decebal Constantin and Pechenizkiy, Mykola},
  year = {2021},
  journal = {Neurocomputing},
  volume = {439},
  pages = {1--11},
  issn = {0925-2312},
  doi = {10/gn6dxq},
  url = {https://www.sciencedirect.com/science/article/pii/S0925231221001545},
  urldate = {2022-07-25},
  abstract = {The continual learning (CL) paradigm aims to enable neural networks to learn tasks continually in a sequential fashion. The fundamental challenge in this learning paradigm is catastrophic forgetting previously learned tasks when the model is optimized for a new task, especially when their data is not accessible. Current architectural-based methods aim at alleviating the catastrophic forgetting problem but at the expense of expanding the capacity of the model. Regularization-based methods maintain a fixed model capacity; however, previous studies showed the huge performance degradation of these methods when the task identity is not available during inference (e.g. class incremental learning scenario). In this work, we propose a novel architectural-based method referred as SpaceNet11Code available at: https://github.com/GhadaSokar/SpaceNet for class incremental learning scenario where we utilize the available fixed capacity of the model intelligently. SpaceNet trains sparse deep neural networks from scratch in an adaptive way that compresses the sparse connections of each task in a compact number of neurons. The adaptive training of the sparse connections results in sparse representations that reduce the interference between the tasks. Experimental results show the robustness of our proposed method against catastrophic forgetting old tasks and the efficiency of SpaceNet in utilizing the available capacity of the model, leaving space for more tasks to be learned. In particular, when SpaceNet is tested on the well-known benchmarks for CL: split MNIST, split Fashion-MNIST, CIFAR-10/100, and iCIFAR100, it outperforms regularization-based methods by a big performance gap. Moreover, it achieves better performance than architectural-based methods without model expansion and achieves comparable results with rehearsal-based methods, while offering a huge memory reduction.},
  langid = {english},
  keywords = {[cifar],[fashion],[mnist],[sparsity],/unread,Class incremental learning,Continual learning,Deep neural networks,Lifelong learning,Sparse training}
}

@article{soltoggio2018,
  title = {Born to Learn: {{The}} Inspiration, Progress, and Future of Evolved Plastic Artificial Neural Networks},
  shorttitle = {Born to Learn},
  author = {Soltoggio, Andrea and Stanley, Kenneth O. and Risi, Sebastian},
  year = {2018},
  journal = {Neural Networks},
  volume = {108},
  pages = {48--67},
  issn = {0893-6080},
  doi = {10/gfpzhz},
  url = {https://www.sciencedirect.com/science/article/pii/S0893608018302120},
  urldate = {2021-07-20},
  abstract = {Biological neural networks are systems of extraordinary computational capabilities shaped by evolution, development, and lifelong learning. The interplay of these elements leads to the emergence of biological intelligence. Inspired by such intricate natural phenomena, Evolved Plastic Artificial Neural Networks (EPANNs) employ simulated evolution in-silico to breed plastic neural networks with the aim to autonomously design and create learning systems. EPANN experiments evolve networks that include both innate properties and the ability to change and learn in response to experiences in different environments and problem domains. EPANNs' aims include autonomously creating learning systems, bootstrapping learning from scratch, recovering performance in unseen conditions, testing the computational advantages of particular neural components, and deriving hypotheses on the emergence of biological learning. Thus, EPANNs may include a large variety of different neuron types and dynamics, network architectures, plasticity rules, and other factors. While EPANNs have seen considerable progress over the last two decades, current scientific and technological advances in artificial neural networks are setting the conditions for radically new approaches and results. Exploiting the increased availability of computational resources and of simulation environments, the often challenging task of hand-designing learning neural networks could be replaced by more autonomous and creative processes. This paper brings together a variety of inspiring ideas that define the field of EPANNs. The main methods and results are reviewed. Finally, new opportunities and possible developments are presented.},
  langid = {english},
  keywords = {/unread,\#nosource,Artificial neural networks,Evolutionary computation,Lifelong learning,Plasticity}
}

@inproceedings{srivastava2013,
  title = {Compete to {{Compute}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 26},
  author = {Srivastava, Rupesh Kumar and Masci, Jonathan and Kazerounian, Sohrob and Gomez, Faustino and Schmidhuber, J{\"u}rgen},
  year = {2013},
  url = {http://papers.nips.cc/paper/5059-compete-to-compute.pdf},
  abstract = {Local competition among neighboring neurons is common in biological neu-ral networks (NNs). In this paper, we apply the concept to gradient-based, backprop-trained artificial multilayer NNs. NNs with competing linear units tend to outperform those with non-competing nonlinear units, and avoid catastrophic forgetting when training sets change over time.},
  keywords = {[mnist],[sparsity],/unread,\#nosource,⛔ No DOI found}
}

@inproceedings{srivastava2019,
  title = {Adaptive {{Compression-based Lifelong Learning}}},
  booktitle = {{{BMVC}}},
  author = {Srivastava, Shivangi and Berman, Maxim and Blaschko, Matthew B and Tuia, Devis},
  year = {2019},
  url = {http://arxiv.org/abs/1907.09695},
  abstract = {The problem of a deep learning model losing performance on a previously learned task when fine-tuned to a new one is a phenomenon known as Catastrophic forgetting. There are two major ways to mitigate this problem: either preserving activations of the initial network during training with a new task; or restricting the new network activations to remain close to the initial ones. The latter approach falls under the denomination of lifelong learning, where the model is updated in a way that it performs well on both old and new tasks, without having access to the old task's training samples anymore. Recently, approaches like pruning networks for freeing network capacity during sequential learning of tasks have been gaining in popularity. Such approaches allow learning small networks while making redundant parameters available for the next tasks. The common problem encountered with these approaches is that the pruning percentage is hard-coded, irrespective of the number of samples, of the complexity of the learning task and of the number of classes in the dataset. We propose a method based on Bayesian optimization to perform adaptive compression/pruning of the network and show its effectiveness in lifelong learning. Our method learns to perform heavy pruning for small and/or simple datasets while using milder compression rates for large and/or complex data. Experiments on classification and semantic segmentation demonstrate the applicability of learning network compression, where we are able to effectively preserve performances along sequences of tasks of varying complexity.},
  keywords = {[imagenet],[sparsity],/unread,\#nosource},
  annotation = {\_eprint: 1907.09695}
}

@inproceedings{stojanov2019,
  title = {Incremental {{Object Learning From Contiguous Views}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Stojanov, Stefan and Mishra, Samarth and Thai, Ngoc Anh and Dhanda, Nikhil and Humayun, Ahmad and Yu, Chen and Smith, Linda B. and Rehg, James M.},
  year = {2019},
  pages = {8777--8786},
  url = {https://openaccess.thecvf.com/content_CVPR_2019/html/Stojanov_Incremental_Object_Learning_From_Contiguous_Views_CVPR_2019_paper.html},
  urldate = {2021-08-26},
  keywords = {/unread,\#nosource},
  note = {The authors introduced the CRIB benchmark}
}

@inproceedings{sun2020,
  title = {{{LAMOL}}: {{LAnguage MOdeling}} for {{Lifelong Language Learning}}},
  shorttitle = {{{LAMOL}}},
  booktitle = {{{ICLR}}},
  author = {Sun, Fan-Keng and Ho, Cheng-Hao and Lee, Hung-Yi},
  year = {2020},
  url = {https://openreview.net/forum?id=Skgxcn4YDS},
  abstract = {Most research on lifelong learning applies to images or games, but not language. We present LAMOL, a simple yet effective method for lifelong language learning (LLL) based on language...},
  keywords = {[nlp],/unread,\#nosource}
}

@article{swaroop2019,
  title = {Improving and {{Understanding Variational Continual Learning}}},
  author = {Swaroop, Siddharth and Nguyen, Cuong V and Bui, Thang D and Turner, Richard E},
  year = {2019},
  journal = {Continual Learning Workshop NeurIPS},
  pages = {1--17},
  url = {http://arxiv.org/abs/1905.02099},
  abstract = {In the continual learning setting, tasks are encountered sequentially. The goal is to learn whilst i) avoiding catastrophic forgetting, ii) efficiently using model capacity, and iii) employing forward and backward transfer learning. In this paper, we explore how the Variational Continual Learning (VCL) framework achieves these desiderata on two benchmarks in continual learning: split MNIST and permuted MNIST. We first report significantly improved results on what was already a competitive approach. The improvements are achieved by establishing a new best practice approach to mean-field variational Bayesian neural networks. We then look at the solutions in detail. This allows us to obtain an understanding of why VCL performs as it does, and we compare the solution to what an `ideal' continual learning solution might be.},
  keywords = {[bayes],[mnist],/unread,\#nosource,⛔ No DOI found},
  annotation = {\_eprint: 1905.02099}
}

@article{takahashi2020,
  title = {A {{Review}} of {{Off-Line Mode Dataset Shifts}}},
  author = {Takahashi, Carla C. and Braga, Antonio P.},
  year = {2020},
  journal = {IEEE Computational Intelligence Magazine},
  volume = {15},
  number = {3},
  pages = {16--27},
  publisher = {{Institute of Electrical and Electronics Engineers Inc.}},
  issn = {1556-603X},
  doi = {10/gqjr2g},
  url = {https://ieeexplore.ieee.org/document/9141463/},
  abstract = {Dataset shifts are present in many real-world applications, since data generation is not always fully controlled and is subject to noise, degradation, and other natural variations. In machine learning, the lack of regularity in data can degrade performance by breaching error constraints. Different methods have been proposed to solve shifting problems; however, shifts in off-line learning mode are not as well examined. Off-line shifts consist of problems where drifts occur only with unlabeled data. Most methods aimed at dataset shifts consider that new labeled data can be received after training, which is not always the case. Here, a review on dataset shift characteristics and causes is presented as a tool for the analysis and implementation of machine learning methods targeting off-line mode dataset shift problems. In this context, a relationship between statistical learning risk functions and error degradation due to variation in data distribution was straightforwardly derived. Moreover, this paper provides a consistent survey of recent popular machine learning methods that address off-line mode dataset shift problems, focusing on the main characteristics of unlabeled data shifts.},
  keywords = {/unread,\#nosource}
}

@inproceedings{tang2020,
  title = {Graph-{{Based Continual Learning}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Tang, Binh and Matteson, David S.},
  year = {2020},
  url = {https://openreview.net/forum?id=HHSEKOnPvaO},
  urldate = {2021-01-16},
  abstract = {Despite significant advances, continual learning models still suffer from catastrophic forgetting when exposed to incrementally available data from non-stationary distributions. Rehearsal...},
  langid = {english},
  keywords = {/unread,\#nosource}
}

@inproceedings{tao2020,
  title = {Few-{{Shot Class-Incremental Learning}}},
  booktitle = {{{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Tao, X. and X., Hong and Chang, X. and Dong, S. and Wei, X. and Gong, Y.},
  year = {2020},
  doi = {10/ghbbnw},
  url = {https://arxiv.org/abs/2004.10956},
  abstract = {The ability to incrementally learn new classes is crucial to the development of real-world artificial intelligence systems. In this paper, we focus on a challenging but practical few-shot class-incremental learning (FSCIL) problem. FSCIL requires CNN models to incrementally learn new classes from very few labelled samples, without forgetting the previously learned ones. To address this problem, we represent the knowledge using a neural gas (NG) network, which can learn and preserve the topology of the feature manifold formed by different classes. On this basis, we propose the TOpology-Preserving knowledge InCrementer (TOPIC) framework. TOPIC mitigates the forgetting of the old classes by stabilizing NG's topology and improves the representation learning for few-shot new classes by growing and adapting NG to new training samples. Comprehensive experimental results demonstrate that our proposed method significantly outperforms other state-of-the-art class-incremental learning methods on CIFAR100, miniImageNet, and CUB200 datasets.},
  keywords = {[cifar],/unread,\#nosource}
}

@article{teng2019,
  title = {Continual {{Learning}} via {{Online Leverage Score Sampling}}},
  author = {Teng, Dan and Dasgupta, Sakyasingha},
  year = {2019},
  journal = {arXiv},
  url = {http://arxiv.org/abs/1908.00355},
  abstract = {In order to mimic the human ability of continual acquisition and transfer of knowledge across various tasks, a learning system needs the capability for continual learning, effectively utilizing the previously acquired skills. As such, the key challenge is to transfer and generalize the knowledge learned from one task to other tasks, avoiding forgetting and interference of previous knowledge and improving the overall performance. In this paper, within the continual learning paradigm, we introduce a method that effectively forgets the less useful data samples continuously and allows beneficial information to be kept for training of the subsequent tasks, in an online manner. The method uses statistical leverage score information to measure the importance of the data samples in every task and adopts frequent directions approach to enable a continual or life-long learning property. This effectively maintains a constant training size across all tasks. We first provide mathematical intuition for the method and then demonstrate its effectiveness in avoiding catastrophic forgetting and computational efficiency on continual learning of classification tasks when compared with the existing state-of-the-art techniques.},
  keywords = {[cifar],[mnist],/unread,\#nosource,⛔ No DOI found},
  annotation = {\_eprint: 1908.00355}
}

@inproceedings{terekhov2015,
  title = {Knowledge Transfer in Deep Block-Modular Neural Networks},
  booktitle = {Conference on {{Biomimetic}} and {{Biohybrid Systems}}},
  author = {Terekhov, Alexander V. and Montone, Guglielmo and O'Regan, J. Kevin},
  year = {2015},
  volume = {9222},
  pages = {268--279},
  publisher = {{Springer Verlag}},
  issn = {16113349},
  doi = {10/gkz2h8},
  url = {http://lpp.psycho.univ-paris5.fr/feel},
  abstract = {Although deep neural networks (DNNs) have demonstrated impressive results during the last decade, they remain highly specialized tools, which are trained \textendash{} often from scratch \textendash{} to solve each particular task. The human brain, in contrast, significantly re-uses existing capacities when learning to solve new tasks. In the current study we explore a block-modular architecture for DNNs, which allows parts of the existing network to be re-used to solve a new task without a decrease in performance when solving the original task. We show that networks with such architectures can outperform networks trained from scratch, or perform comparably, while having to learn nearly 10 times fewer weights than the networks trained from scratch.},
  isbn = {978-3-319-22978-2},
  keywords = {[vision],/unread,\#nosource,Deep learning,Knowledge transfer,Modular,Neural networks},
  annotation = {\_eprint: 1908.08017}
}

@article{thai2021,
  title = {Does {{Continual Learning}} = {{Catastrophic Forgetting}}?},
  author = {Thai, Anh and Stojanov, Stefan and Rehg, Isaac and Rehg, James M.},
  year = {2021},
  journal = {arXiv},
  eprint = {2101.07295},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2101.07295},
  urldate = {2021-11-14},
  abstract = {Continual learning is known for suffering from catastrophic forgetting, a phenomenon where earlier learned concepts are forgotten at the expense of more recent samples. In this work, we challenge the assumption that continual learning is inevitably associated with catastrophic forgetting by presenting a set of tasks that surprisingly do not suffer from catastrophic forgetting when learned continually. The robustness of these tasks leads to the potential of having a proxy representation learning task for continual classification. We further introduce a novel yet simple algorithm, YASS that achieves state-of-the-art performance in the class-incremental categorization learning task and provide an insight into the benefit of learning the representation continuously. Finally, we present converging evidence on the forgetting dynamics of representation learning in continual models. The codebase, dataset, and pre-trained models released with this article can be found at https://github.com/rehg-lab/CLRec.},
  archiveprefix = {arXiv},
  keywords = {/unread,\#nosource,⛔ No DOI found,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}

@inproceedings{thompson2019,
  title = {Overcoming {{Catastrophic Forgetting During Domain Adaptation}} of {{Neural Machine Translation}}},
  booktitle = {Proceedings of the 2019 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 1 ({{Long}} and {{Short Papers}})},
  author = {Thompson, Brian and Gwinnup, Jeremy and Khayrallah, Huda and Duh, Kevin and Koehn, Philipp},
  year = {2019},
  pages = {2062--2068},
  publisher = {{Association for Computational Linguistics}},
  address = {{Minneapolis, Minnesota}},
  doi = {10/gkjs3f},
  url = {https://www.aclweb.org/anthology/N19-1209},
  urldate = {2021-01-08},
  abstract = {Continued training is an effective method for domain adaptation in neural machine translation. However, in-domain gains from adaptation come at the expense of general-domain performance. In this work, we interpret the drop in general-domain performance as catastrophic forgetting of general-domain knowledge. To mitigate it, we adapt Elastic Weight Consolidation (EWC)\textemdash a machine learning method for learning a new task without forgetting previous tasks. Our method retains the majority of general-domain performance lost in continued training without degrading in-domain performance, outperforming the previous state-of-the-art. We also explore the full range of general-domain performance available when some in-domain degradation is acceptable.},
  keywords = {[nlp],[rnn],/unread,\#nosource}
}

@incollection{thrun1995,
  title = {A {{Lifelong Learning Perspective}} for {{Mobile Robot Control}}},
  booktitle = {Intelligent {{Robots}} and {{Systems}}},
  author = {Thrun, Sebastian},
  editor = {Graefe, Volker},
  year = {1995},
  pages = {201--214},
  publisher = {{Elsevier Science B.V.}},
  address = {{Amsterdam}},
  doi = {10.1016/B978-044482250-5/50015-3},
  url = {http://www.sciencedirect.com/science/article/pii/B9780444822505500153},
  abstract = {Designing robots that learn by themselves to perform complex real-world tasks is a still-open challenge for the field of robotics and artificial intelligence. This chapter presents the robot learning problem as a lifelong problem, in which a robot faces a collection of tasks over its entire lifetime. Such a scenario provides the opportunity to gather general-purpose knowledge that transfers across tasks. The chapter illustrates a learning mechanism, explanation-based neural-network learning, that transfers knowledge between related tasks via neural-network action models. The learning approach is illustrated using a mobile robot, equipped with visual, ultrasonic, and laser sensors. In less than 10 minutes of operation time, the robot is able to learn to navigate to a marked target object in a natural office environment.},
  isbn = {978-0-444-82250-5},
  langid = {english},
  keywords = {/unread,\#nosource}
}

@book{thrun1996,
  title = {Explanation-{{Based Neural Network Learning}}: {{A Lifelong Learning Approach}}},
  author = {Thrun, Sebastian},
  year = {1996},
  publisher = {{Springer}},
  url = {https://www.springer.com/gp/book/9780792397168},
  abstract = {Lifelong learning addresses situations in which a learner faces a series of different learning tasks providing the opportunity for synergy among them. Explanation-based neural network learning (EBNN) is a machine learning algorithm that transfers knowledge across multiple learning tasks. When faced with a new learning task, EBNN exploits domain knowledge accumulated in previous learning tasks to guide generalization in the new one. As a result, EBNN generalizes more accurately from less data than comparable methods. Explanation-Based Neural Network Learning: A Lifelong Learning Approach describes the basic EBNN paradigm and investigates it in the context of supervised learning, reinforcement learning, robotics, and chess. `The paradigm of lifelong learning - using earlier learned knowledge to improve subsequent learning - is a promising direction for a new generation of machine learning algorithms. Given the need for more accurate learning methods, it is difficult to imagine a future for machine learning that does not include this paradigm.' From the Foreword by Tom M. Mitchell.},
  isbn = {978-1-4612-8597-7},
  keywords = {[framework],/unread,\#nosource}
}

@inproceedings{thrun1996a,
  title = {Is {{Learning The}} N-Th {{Thing Any Easier Than Learning The First}}?},
  booktitle = {Advances in {{Neural Information Processing Systems}} 8},
  author = {Thrun, Sebastian},
  editor = {Touretzky, D S and Mozer, M C and Hasselmo, M E},
  year = {1996},
  pages = {640--646},
  publisher = {{MIT Press}},
  url = {http://papers.nips.cc/paper/1034-is-learning-the-n-th-thing-any-easier-than-learning-the-first.pdf},
  keywords = {[vision],/unread,\#nosource,⛔ No DOI found,lifelong,lifelong learning}
}

@article{titsias2019,
  title = {Functional {{Regularisation}} for {{Continual Learning}} Using {{Gaussian Processes}}},
  author = {Titsias, Michalis K and Schwarz, Jonathan and Matthews, Alexander G de G and Pascanu, Razvan and Teh, Yee Whye},
  year = {2019},
  journal = {arXiv},
  url = {http://arxiv.org/abs/1901.11356},
  abstract = {We introduce a novel approach for supervised continual learning based on approximate Bayesian inference over function space rather than the parameters of a deep neural network. We use a Gaussian process obtained by treating the weights of the last layer of a neural network as random and Gaussian distributed. Functional regularisation for continual learning naturally arises by applying the variational sparse GP inference method in a sequential fashion as new tasks are encountered. At each step of the process, a summary is constructed for the current task that consists of (i) inducing inputs and (ii) a posterior distribution over the function values at these inputs. This summary then regularises learning of future tasks, through Kullback-Leibler regularisation terms that appear in the variational lower bound, and reduces the effects of catastrophic forgetting. We fully develop the theory of the method and we demonstrate its effectiveness in classification datasets, such as Split-MNIST, Permuted-MNIST and Omniglot.},
  keywords = {[mnist],[omniglot],/unread,\#nosource,⛔ No DOI found},
  annotation = {\_eprint: 1901.11356}
}

@inproceedings{toneva2019,
  title = {An {{Empirical Study}} of {{Example Forgetting}} during {{Deep Neural Network Learning}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Toneva, Mariya and Sordoni, Alessandro and {des Combes}, Remi Tachet and Trischler, Adam and Bengio, Yoshua and Gordon, Geoffrey J},
  year = {2019},
  url = {https://openreview.net/forum?id=BJlxm30cKm},
  abstract = {Inspired by the phenomenon of catastrophic forgetting, we investigate the learning dynamics of neural networks as they train on single classification tasks. Our goal is to understand whether a...},
  keywords = {[cifar],[mnist],/unread,\#nosource},
  note = {An interesting aspect of this paper is related to the study of unforgettable patterns and how they influence performance in terms of forgetting.}
}

@article{triki2017,
  title = {Encoder {{Based Lifelong Learning}}},
  author = {Triki, Amal Rannen and Aljundi, Rahaf and Blaschko, Mathew B. and Tuytelaars, Tinne},
  year = {2017},
  journal = {Proceedings of the IEEE International Conference on Computer Vision},
  volume = {2017-Octob},
  pages = {1329--1337},
  publisher = {{Institute of Electrical and Electronics Engineers Inc.}},
  issn = {15505499},
  doi = {10/gfx3tp},
  url = {http://arxiv.org/abs/1704.01920 http://dx.doi.org/10.1109/ICCV.2017.148},
  abstract = {This paper introduces a new lifelong learning solution where a single model is trained for a sequence of tasks. The main challenge that vision systems face in this context is catastrophic forgetting: as they tend to adapt to the most recently seen task, they lose performance on the tasks that were learned previously. Our method aims at preserving the knowledge of the previous tasks while learning a new one by using autoencoders. For each task, an under-complete autoencoder is learned, capturing the features that are crucial for its achievement. When a new task is presented to the system, we prevent the reconstructions of the features with these autoencoders from changing, which has the effect of preserving the information on which the previous tasks are mainly relying. At the same time, the features are given space to adjust to the most recent environment as only their projection into a low dimension submanifold is controlled. The proposed system is evaluated on image classification tasks and shows a reduction of forgetting over the state-of-the-art},
  isbn = {9781538610329},
  keywords = {[imagenet],[vision],/unread,\#nosource},
  annotation = {\_eprint: 1704.01920}
}

@article{turner2021,
  title = {Modular {{Dynamic Neural Network}}: {{A Continual Learning Architecture}}},
  shorttitle = {Modular {{Dynamic Neural Network}}},
  author = {Turner, Daniel and Cardoso, Pedro J. S. and Rodrigues, Jo{\~a}o M. F.},
  year = {2021},
  journal = {Applied Sciences},
  volume = {11},
  number = {24},
  pages = {12078},
  issn = {2076-3417},
  doi = {10/gqjrtf},
  url = {https://www.mdpi.com/2076-3417/11/24/12078},
  urldate = {2022-07-08},
  abstract = {Learning to recognize a new object after having learned to recognize other objects may be a simple task for a human, but not for machines. The present go-to approaches for teaching a machine to recognize a set of objects are based on the use of deep neural networks (DNN). So, intuitively, the solution for teaching new objects on the fly to a machine should be DNN. The problem is that the trained DNN weights used to classify the initial set of objects are extremely fragile, meaning that any change to those weights can severely damage the capacity to perform the initial recognitions; this phenomenon is known as catastrophic forgetting (CF). This paper presents a new (DNN) continual learning (CL) architecture that can deal with CF, the modular dynamic neural network (MDNN). The presented architecture consists of two main components: (a) the ResNet50-based feature extraction component as the backbone; and (b) the modular dynamic classification component, which consists of multiple sub-networks and progressively builds itself up in a tree-like structure that rearranges itself as it learns over time in such a way that each sub-network can function independently. The main contribution of the paper is a new architecture that is strongly based on its modular dynamic training feature. This modular structure allows for new classes to be added while only altering specific sub-networks in such a way that previously known classes are not forgotten. Tests on the CORe50 dataset showed results above the state of the art for CL architectures.},
  langid = {english},
  keywords = {/unread}
}

@inproceedings{valkov2018,
  title = {{{HOUDINI}}: {{Lifelong Learning}} as {{Program Synthesis}}},
  booktitle = {{{NeurIPS}}},
  author = {Valkov, Lazar and Chaudhari, Dipak and Srivastava, Akash and Sutton, Charles and Chaudhuri, Swarat},
  year = {2018},
  pages = {8687--8698},
  url = {http://papers.nips.cc/paper/8086-houdini-lifelong-learning-as-program-synthesis.pdf},
  abstract = {We present a neurosymbolic framework for the lifelong learning of algorithmic tasks that mix perception and procedural reasoning. Reusing high-level concepts across domains and learning complex procedures are key challenges in lifelong learning. We show that a program synthesis approach that combines gradient descent with combinatorial search over programs can be a more effective response to these challenges than purely neural methods. Our framework, called HOUDINI, represents neural networks as strongly typed, differentiable functional programs that use symbolic higher-order combinators to compose a library of neural functions. Our learning algorithm consists of: (1) a symbolic program synthesizer that performs a type-directed search over parameterized programs, and decides on the library functions to reuse, and the architectures to combine them, while learning a sequence of tasks; and (2) a neural module that trains these programs using stochas-tic gradient descent. We evaluate HOUDINI on three benchmarks that combine perception with the algorithmic tasks of counting, summing, and shortest-path computation. Our experiments show that HOUDINI transfers high-level concepts more effectively than traditional transfer learning and progressive neural networks, and that the typed representation of networks significantly accelerates the search.},
  keywords = {/unread,\#nosource,⛔ No DOI found}
}

@inproceedings{vandeven2018,
  title = {Three Scenarios for Continual Learning},
  booktitle = {Continual {{Learning Workshop NeurIPS}}},
  author = {{van de Ven}, Gido M and Tolias, Andreas S},
  year = {2018},
  url = {http://arxiv.org/abs/1904.07734},
  abstract = {Standard artificial neural networks suffer from the well-known issue of catastrophic forgetting, making continual or lifelong learning difficult for machine learning. In recent years, numerous methods have been proposed for continual learning, but due to differences in evaluation protocols it is difficult to directly compare their performance. To enable more structured comparisons, we describe three continual learning scenarios based on whether at test time task identity is provided and\textendash in case it is not\textendash whether it must be inferred. Any sequence of well-defined tasks can be performed according to each scenario. Using the split and permuted MNIST task protocols, for each scenario we carry out an extensive comparison of recently proposed continual learning methods. We demonstrate substantial differences between the three scenarios in terms of difficulty and in terms of how efficient different methods are. In particular, when task identity must be inferred (i.e., class incremental learning), we find that regularization-based approaches (e.g., elastic weight consolidation) fail and that replaying representations of previous experiences seems required for solving this scenario.},
  keywords = {[framework],[mnist],/unread,\#nosource,⛔ No DOI found,Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Rec,Computer Science - Machine Learning,Statistics - Machine Learning},
  note = {The authors clearly show how regularization approaches fail to perform well on Class-IL scenarios. They used a MLP for all experiments.}
}

@article{vandeven2018a,
  title = {Generative Replay with Feedback Connections as a General Strategy for Continual Learning},
  author = {{van de Ven}, Gido M. and Tolias, Andreas S.},
  year = {2018},
  journal = {arXiv},
  url = {https://arxiv.org/abs/1809.10635},
  abstract = {A major obstacle to developing artificial intelligence applications capable of true lifelong learning is that artificial neural networks quickly or catastrophically forget previously learned tasks when trained on a new one. Numerous methods for alleviating catastrophic forgetting are currently being proposed, but differences in evaluation protocols make it difficult to directly compare their performance. To enable more meaningful comparisons, here we identified three distinct scenarios for continual learning based on whether task identity is known and, if it is not, whether it needs to be inferred. Performing the split and permuted MNIST task protocols according to each of these scenarios, we found that regularization-based approaches (e.g., elastic weight consolidation) failed when task identity needed to be inferred. In contrast, generative replay combined with distillation (i.e., using class probabilities as "soft targets") achieved superior performance in all three scenarios. Addressing the issue of efficiency, we reduced the computational cost of generative replay by integrating the generative model into the main model by equipping it with generative feedback or backward connections. This Replay-through-Feedback approach substantially shortened training time with no or negligible loss in performance. We believe this to be an important first step towards making the powerful technique of generative replay scalable to real-world continual learning applications.},
  keywords = {[framework],[generative],[mnist],/unread,\#nosource,⛔ No DOI found},
  annotation = {\_eprint: 1809.10635}
}

@inproceedings{vandeven2020,
  title = {Brain-like {{Replay}} for {{Continual Learning}} with {{Artificial Neural Networks}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{Workshop}} on {{Bridging AI}} and {{Cognitive Science}})},
  author = {{van de Ven}, Gido M. and Siegelmann, Hava T. and Tolias, Andreas S.},
  year = {2020},
  url = {https://baicsworkshop.github.io/pdf/BAICS_8.pdf},
  abstract = {Artificial neural networks suffer from catastrophic forgetting. Unlike humans, when these networks are trained on something new, they rapidly forget what was learned before. In the brain, a mechanism thought to be important for protecting memories is the replay of neuronal activity patterns representing those memories. In artificial neural networks, such memory replay has been implemented in the form of `generative replay', which can successfully prevent catastrophic forgetting in a range of toy examples. Scaling up generative replay to problems with more complex inputs, however, turns out to be challenging. We propose a new, more brain-like variant of replay in which internal or hidden representations are replayed that are generated by the network's own, context-modulated feedback connections. In contrast to established continual learning methods, our method achieves acceptable performance on the challenging problem of class-incremental learning on natural images without relying on stored data.},
  keywords = {[cifar],/unread,\#nosource,⛔ No DOI found}
}

@article{vandeven2020a,
  title = {Brain-Inspired Replay for Continual Learning with Artificial Neural Networks},
  author = {{van de Ven}, Gido M. and Siegelmann, Hava T. and Tolias, Andreas S.},
  year = {2020},
  journal = {Nature Communications},
  volume = {11},
  doi = {10/gg8xst},
  url = {https://www.nature.com/articles/s41467-020-17866-2},
  abstract = {Artificial neural networks suffer from catastrophic forgetting. Unlike humans, when these networks are trained on something new, they rapidly forget what was learned before. In the brain, a mechanism thought to be important for protecting memories is the reactivation of neuronal activity patterns representing those memories. In artificial neural networks, such memory replay can be implemented as `generative replay', which can successfully \textendash{} and surprisingly efficiently \textendash{} prevent catastrophic forgetting on toy examples even in a class-incremental learning scenario. However, scaling up generative replay to complicated problems with many tasks or complex inputs is challenging. We propose a new, brain-inspired variant of replay in which internal or hidden representations are replayed that are generated by the network's own, context-modulated feedback connections. Our method achieves state-of-the-art performance on challenging continual learning benchmarks (e.g., class-incremental learning on CIFAR-100) without storing data, and it provides a novel model for replay in the brain.},
  keywords = {[cifar],[framework],[generative],[mnist],/unread,\#nosource},
  note = {The paper shows a generative form of replay in which a VAE, conditioned on the current task, is able to generate pseudosamples and, when used as a classifier, to address new tasks. The idea is that the generative model is inspired by the hyppocampus, which sits hierarchically on top of the cortex (often thought as the classifier). In this way, replay is fed-back by the same model used to predict the class. Forgetting is prevented both on VAE and on the classification component through replay. It also shows that regularization approaches fail in class-incremental setting.}
}

@article{velez2017,
  title = {Diffusion-Based Neuromodulation Can Eliminate Catastrophic Forgetting in Simple Neural Networks},
  author = {Velez, Roby and Clune, Jeff},
  year = {2017},
  journal = {PLoS ONE},
  volume = {12},
  number = {11},
  pages = {1--31},
  issn = {19326203},
  doi = {10/gckrq7},
  url = {http://arxiv.org/abs/1705.07241 http://dx.doi.org/10.1371/journal.pone.0187736},
  abstract = {A long-term goal of AI is to produce agents that can learn a diversity of skills throughout their lifetimes and continuously improve those skills via experience. A longstanding obstacle towards that goal is catastrophic forgetting, which is when learning new information erases previously learned information. Catastrophic forgetting occurs in artificial neural networks (ANNs), which have fueled most recent advances in AI. A recent paper proposed that catastrophic forgetting in ANNs can be reduced by promoting modularity, which can limit forgetting by isolating task information to specific clusters of nodes and connections (functional modules). While the prior work did show that modular ANNs suffered less from catastrophic forgetting, it was not able to produce ANNs that possessed task-specific functional modules, thereby leaving the main theory regarding modularity and forgetting untested. We introduce diffusion-based neuromodulation, which simulates the release of diffusing, neuromodulatory chemicals within an ANN that can modulate (i.e. up or down regulate) learning in a spatial region. On the simple diagnostic problem from the prior work, diffusion-based neuromodulation 1) induces task-specific learning in groups of nodes and connections (task-specific localized learning), which 2) produces functional modules for each subtask, and 3) yields higher performance by eliminating catastrophic forgetting. Overall, our results suggest that diffusion-based neuromodulation promotes task-specific localized learning and functional modularity, which can help solve the challenging, but important problem of catastrophic forgetting.},
  isbn = {1111111111},
  keywords = {/unread,\#nosource},
  annotation = {\_eprint: 1705.07241}
}

@article{veness2020,
  title = {Gated {{Linear Networks}}},
  author = {Veness, Joel and Lattimore, Tor and Budden, David and Bhoopchand, Avishkar and Mattern, Christopher and {Grabska-Barwinska}, Agnieszka and Sezener, Eren and Wang, Jianan and Toth, Peter and Schmitt, Simon and Hutter, Marcus},
  year = {2020},
  journal = {arXiv},
  eprint = {1910.01526},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1910.01526},
  urldate = {2021-04-20},
  abstract = {This paper presents a new family of backpropagation-free neural architectures, Gated Linear Networks (GLNs). What distinguishes GLNs from contemporary neural networks is the distributed and local nature of their credit assignment mechanism; each neuron directly predicts the target, forgoing the ability to learn feature representations in favor of rapid online learning. Individual neurons can model nonlinear functions via the use of data-dependent gating in conjunction with online convex optimization. We show that this architecture gives rise to universal learning capabilities in the limit, with effective model capacity increasing as a function of network size in a manner comparable with deep ReLU networks. Furthermore, we demonstrate that the GLN learning mechanism possesses extraordinary resilience to catastrophic forgetting, performing comparably to a MLP with dropout and Elastic Weight Consolidation on standard benchmarks. These desirable theoretical and empirical properties position GLNs as a complementary technique to contemporary offline deep learning methods.},
  archiveprefix = {arXiv},
  keywords = {/unread,\#nosource,⛔ No DOI found,Computer Science - Information Theory,Computer Science - Machine Learning,Statistics - Machine Learning},
  note = {Comment: arXiv admin note: substantial text overlap with arXiv:1712.01897}
}

@article{veniat2020,
  title = {Efficient {{Continual Learning}} with {{Modular Networks}} and {{Task-Driven Priors}}},
  author = {Veniat, Tom and Denoyer, Ludovic and Ranzato, Marc'Aurelio},
  year = {2020},
  journal = {arXiv},
  eprint = {2012.12631},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2012.12631},
  urldate = {2020-12-29},
  abstract = {Existing literature in Continual Learning (CL) has focused on overcoming catastrophic forgetting, the inability of the learner to recall how to perform tasks observed in the past. There are however other desirable properties of a CL system, such as the ability to transfer knowledge from previous tasks and to scale memory and compute sub-linearly with the number of tasks. Since most current benchmarks focus only on forgetting using short streams of tasks, we first propose a new suite of benchmarks to probe CL algorithms across these new axes. Finally, we introduce a new modular architecture, whose modules represent atomic skills that can be composed to perform a certain task. Learning a task reduces to figuring out which past modules to re-use, and which new modules to instantiate to solve the current task. Our learning algorithm leverages a task-driven prior over the exponential search space of all possible ways to combine modules, enabling efficient learning on long streams of tasks. Our experiments show that this modular architecture and learning algorithm perform competitively on widely used CL benchmarks while yielding superior performance on the more challenging benchmarks we introduce in this work.},
  archiveprefix = {arXiv},
  keywords = {[experimental],/unread,\#nosource,⛔ No DOI found,Computer Science - Machine Learning},
  note = {This paper introduces a new benchmark for CL and evaluate a newly proposed model on different types of task streams. The model is a modular network with a data-driven prior to choose among modules. It uses task labels both at training and test time.}
}

@inproceedings{veniat2021,
  title = {Efficient {{Continual Learning}} with {{Modular Networks}} and {{Task-Driven Priors}}},
  booktitle = {{{ICLR}}},
  author = {Veniat, Tom and Denoyer, Ludovic and Ranzato, Marc'Aurelio},
  year = {2021},
  eprint = {2012.12631},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2012.12631},
  urldate = {2021-04-11},
  abstract = {Existing literature in Continual Learning (CL) has focused on overcoming catastrophic forgetting, the inability of the learner to recall how to perform tasks observed in the past. There are however other desirable properties of a CL system, such as the ability to transfer knowledge from previous tasks and to scale memory and compute sub-linearly with the number of tasks. Since most current benchmarks focus only on forgetting using short streams of tasks, we first propose a new suite of benchmarks to probe CL algorithms across these new axes. Finally, we introduce a new modular architecture, whose modules represent atomic skills that can be composed to perform a certain task. Learning a task reduces to figuring out which past modules to re-use, and which new modules to instantiate to solve the current task. Our learning algorithm leverages a task-driven prior over the exponential search space of all possible ways to combine modules, enabling efficient learning on long streams of tasks. Our experiments show that this modular architecture and learning algorithm perform competitively on widely used CL benchmarks while yielding superior performance on the more challenging benchmarks we introduce in this work.},
  archiveprefix = {arXiv},
  keywords = {/unread,\#nosource,⛔ No DOI found,Computer Science - Machine Learning}
}

@inproceedings{verwimp2021,
  title = {Rehearsal {{Revealed}}: {{The Limits}} and {{Merits}} of {{Revisiting Samples}} in {{Continual Learning}}},
  shorttitle = {Rehearsal {{Revealed}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}}},
  author = {Verwimp, Eli and De Lange, Matthias and Tuytelaars, Tinne},
  year = {2021},
  pages = {9385--9394},
  url = {https://openaccess.thecvf.com/content/ICCV2021/html/Verwimp_Rehearsal_Revealed_The_Limits_and_Merits_of_Revisiting_Samples_in_ICCV_2021_paper.html},
  keywords = {/unread,\#nosource},
  note = {Rehearsal provides good mitigation of forgetting, but often at the expenses of over-fitting on the memory samples, thus reducing generalization.}
}

@article{villa2022,
  title = {{{vCLIMB}}: {{A Novel Video Class Incremental Learning Benchmark}}},
  shorttitle = {{{vCLIMB}}},
  author = {Villa, Andr{\'e}s and Alhamoud, Kumail and Alc{\'a}zar, Juan Le{\'o}n and Heilbron, Fabian Caba and Escorcia, Victor and Ghanem, Bernard},
  year = {2022},
  journal = {arXiv},
  eprint = {2201.09381},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2201.09381},
  urldate = {2022-04-07},
  abstract = {Continual learning (CL) is under-explored in the video domain. The few existing works contain splits with imbalanced class distributions over the tasks, or study the problem in unsuitable datasets. We introduce vCLIMB, a novel video continual learning benchmark. vCLIMB is a standardized test-bed to analyze catastrophic forgetting of deep models in video continual learning. In contrast to previous work, we focus on class incremental continual learning with models trained on a sequence of disjoint tasks, and distribute the number of classes uniformly across the tasks. We perform in-depth evaluations of existing CL methods in vCLIMB, and observe two unique challenges in video data. The selection of instances to store in episodic memory is performed at the frame level. Second, untrimmed training data influences the effectiveness of frame sampling strategies. We address these two challenges by proposing a temporal consistency regularization that can be applied on top of memory-based continual learning methods. Our approach significantly improves the baseline, by up to 24\% on the untrimmed continual learning task.},
  archiveprefix = {arXiv},
  keywords = {/unread,⛔ No DOI found,Computer Science - Computer Vision and Pattern Recognition},
  annotation = {ZSCC:00000},
  note = {Comment: An updated version of our CVPR 2022 paper (oral); v2 adds minor text changes. The code of our benchmark can be found at: https://vclimb.netlify.app/}
}

@inproceedings{vonoswald2020,
  title = {Continual Learning with Hypernetworks},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {{von Oswald}, Johannes and Henning, Christian and Sacramento, Jo{\~a}o and Grewe, Benjamin F},
  year = {2020},
  url = {https://openreview.net/forum?id=SJgwNerKvB},
  abstract = {Artificial neural networks suffer from catastrophic forgetting when they are sequentially trained on multiple tasks. To overcome this problem, we present a novel approach based on task-conditioned...},
  keywords = {[cifar],[mnist],/unread,\#nosource}
}

@article{vuorio2018,
  title = {Meta Continual Learning},
  author = {Vuorio, Risto and Cho, Dong-Yeon and Kim, Daejoong and Kim, Jiwon},
  year = {2018},
  journal = {arXiv},
  url = {https://arxiv.org/abs/1806.06928},
  abstract = {Using neural networks in practical settings would benefit from the ability of the networks to learn new tasks throughout their lifetimes without forgetting the previous tasks. This ability is limited in the current deep neural networks by a problem called catastrophic forgetting, where training on new tasks tends to severely degrade performance on previous tasks. One way to lessen the impact of the forgetting problem is to constrain parameters that are important to previous tasks to stay close to the optimal parameters. Recently, multiple competitive approaches for computing the importance of the parameters with respect to the previous tasks have been presented. In this paper, we propose a learning to optimize algorithm for mitigating catastrophic forgetting. Instead of trying to formulate a new constraint function ourselves, we propose to train another neural network to predict parameter update steps that respect the importance of parameters to the previous tasks. In the proposed meta-training scheme, the update predictor is trained to minimize loss on a combination of current and past tasks. We show experimentally that the proposed approach works in the continual learning setting.},
  keywords = {[mnist],/unread,\#nosource,⛔ No DOI found},
  annotation = {\_eprint: 1806.06928}
}

@article{wang2019,
  title = {Continual {{Learning}} of {{New Sound Classes}} Using {{Generative Replay}}},
  author = {Wang, Zhepei and Subakan, Cem and Tzinis, Efthymios and Smaragdis, Paris and Charlin, Laurent},
  year = {2019},
  journal = {arXiv},
  url = {http://arxiv.org/abs/1906.00654},
  abstract = {Continual learning consists in incrementally training a model on a sequence of datasets and testing on the union of all datasets. In this paper, we examine continual learning for the problem of sound classification, in which we wish to refine already trained models to learn new sound classes. In practice one does not want to maintain all past training data and retrain from scratch, but naively updating a model with new data(sets) results in a degradation of already learned tasks, which is referred to as "catastrophic forgetting." We develop a generative replay procedure for generating training audio spectrogram data, in place of keeping older training datasets. We show that by incrementally refining a classifier with generative replay a generator that is 4\% of the size of all previous training data matches the performance of refining the classifier keeping 20\% of all previous training data. We thus conclude that we can extend a trained sound classifier to learn new classes without having to keep previously used datasets.},
  keywords = {[audio],/unread,\#nosource,⛔ No DOI found,audio,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio,sequence,sequences,Statistics - Machine Learning,time series},
  note = {arXiv: 1906.00654
\par
arXiv: 1906.00654}
}

@article{wang2020,
  title = {Lifelong {{Graph Learning}}},
  author = {Wang, Chen and Qiu, Yuheng and Scherer, Sebastian},
  year = {2020},
  journal = {arXiv},
  url = {http://arxiv.org/abs/2009.00647},
  abstract = {Graph neural networks are powerful models for many graph-structured tasks. In this paper, we aim to solve the problem of lifelong learning for graph neural networks. One of the main challenges is the effect of "catastrophic forgetting" for continuously learning a sequence of tasks, as the nodes can only be present to the model once. Moreover, the number of nodes changes dynamically in lifelong learning and this makes many graph models and sampling strategies inapplicable. To solve these problems, we construct a new graph topology, called the feature graph. It takes features as new nodes and turns nodes into independent graphs. This successfully converts the original problem of node classification to graph classification. In this way, the increasing nodes in lifelong learning can be regarded as increasing training samples, which makes lifelong learning easier. We demonstrate that the feature graph achieves much higher accuracy than the state-of-the-art methods in both data-incremental and class-incremental tasks. We expect that the feature graph will have broad potential applications for graph-structured tasks in lifelong learning.},
  keywords = {[graph],/unread,\#nosource,⛔ No DOI found},
  annotation = {\_eprint: 2009.00647}
}

@article{wang2021,
  title = {Few-{{Shot Continual Learning}}: {{A Brain-Inspired Approach}}},
  author = {Wang, Liyuan and Li, Qian and Zhong, Yi and Zhu, Jun},
  year = {2021},
  journal = {arXiv},
  url = {https://arxiv.org/abs/2104.09034},
  abstract = {It is an important yet challenging setting to continually learn new tasks from a few examples. Although numerous efforts have been devoted to either continual learning or few-shot learning, little work has considered this new setting of few-shot continual learning (FSCL), which needs to minimize the catastrophic forgetting to the old tasks and gradually improve the ability of few-shot generalization. In this paper, we provide a first systematic study on FSCL and present an effective solution with deep neural networks. Our solution is based on the observation that continual learning of a task sequence inevitably interferes few-shot generalization, which makes it highly nontrivial to extend few-shot learning strategies to continual learning scenarios. We draw inspirations from the robust brain system and develop a method that (1) interdependently updates a pair of fast / slow weights for continual learning and few-shot learning to disentangle their divergent objectives, inspired by the biological model of meta-plasticity and fast / slow synapse; and (2) applies a brain-inspired two-step consolidation strategy to learn a task sequence without forgetting in the fast weights while improve generalization without overfitting in the slow weights. Extensive results on various benchmarks show that our method achieves a better performance than joint training of all the tasks ever seen. The ability of few-shot generalization is also substantially improved from incoming tasks and examples.},
  keywords = {/unread,⛔ No DOI found,continual learning}
}

@article{widmer1996,
  title = {Learning in the Presence of Concept Drift and Hidden Contexts},
  author = {Widmer, Gerhard and Kubat, Miroslav},
  year = {1996},
  journal = {Machine Learning},
  volume = {23},
  number = {1},
  pages = {69--101},
  issn = {0885-6125},
  doi = {10/ckfm6q},
  url = {https://doi.org/10.1007/BF00116900 http://link.springer.com/10.1007/BF00116900},
  abstract = {On-line learning in domains where the target concept depends on some hidden context poses serious problems. A changing context can induce changes in the target concepts, producing what is known as concept drift. We describe a family of learning algorithms that flexibly react to concept drift and can take advantage of situations where contexts reappear. The general approach underlying all these algorithms consists of (1) keeping only a window of currently trusted examples and hypotheses; (2) storing concept descriptions and reusing them when a previous context re-appears; and (3) controlling both of these functions by a heuristic that constantly monitors the system's behavior. The paper reports on experiments that test the systems' perfomance under various conditions such as different levels of noise and different extent and rate of concept drift.},
  langid = {english},
  keywords = {/unread,\#nosource}
}

@article{wiewel2019,
  title = {Localizing {{Catastrophic Forgetting}} in {{Neural Networks}}},
  author = {Wiewel, Felix and Yang, Bin},
  year = {2019},
  journal = {arXiv},
  url = {http://arxiv.org/abs/1906.02568},
  abstract = {Artificial neural networks (ANNs) suffer from catastrophic forgetting when trained on a sequence of tasks. While this phenomenon was studied in the past, there is only very limited recent research on this phenomenon. We propose a method for determining the contribution of individual parameters in an ANN to catastrophic forgetting. The method is used to analyze an ANNs response to three different continual learning scenarios.},
  keywords = {[mnist],/unread,\#nosource,⛔ No DOI found},
  annotation = {\_eprint: 1906.02568}
}

@inproceedings{wolf2018,
  title = {Continuous {{Learning}} in a {{Hierarchical Multiscale Neural Network}}},
  booktitle = {{{ACL}}},
  author = {Wolf, Thomas and Chaumond, Julien and Delangue, Clement},
  year = {2018},
  doi = {10/gnq33t},
  url = {http://arxiv-export-lb.library.cornell.edu/abs/1805.05758},
  urldate = {2021-01-08},
  langid = {english},
  keywords = {[rnn],/unread,\#nosource}
}

@article{woltz2000,
  title = {Negative Transfer Errors in Sequential Cognitive Skills: {{Strong-but-wrong}} Sequence Application.},
  shorttitle = {Negative Transfer Errors in Sequential Cognitive s},
  author = {Woltz, Dan J. and Gardner, Michael K. and Bell, Brian G.},
  year = {2000},
  journal = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
  volume = {26},
  number = {3},
  pages = {601--625},
  issn = {1939-1285},
  doi = {10/czkrb3},
  url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0278-7393.26.3.601},
  abstract = {Three experiments investigated the role of processing sequence knowledge in negative transfer within multistep cognitive skills. In Experiments 1 and 2, more training resulted in higher error rates when new processing sequences that resembled familiar ones were introduced in transfer. Transfer error responses were executed with the same speed as correct responses to familiar sequence trials, and the errors appeared to be undetected by the performers. Experiment 3 tested whether the effects of sequence learning were attributable to explicit or implicit knowledge of processing sequences. Evidence favored the implicit learning interpretation. Findings are discussed in relationship to earlier demonstrations of the einstellung effect and to current taxonomic theories of human error.},
  langid = {english},
  keywords = {/unread,\#nosource}
}

@article{wong2016,
  title = {Towards {{Lifelong Self-Supervision}}: {{A Deep Learning Direction}} for {{Robotics}}},
  author = {Wong, Jay M},
  year = {2016},
  journal = {arXiv},
  url = {http://arxiv.org/abs/1611.00201},
  abstract = {Despite outstanding success in vision amongst other domains, many of the recent deep learning approaches have evident drawbacks for robots. This manuscript surveys recent work in the literature that pertain to applying deep learning systems to the robotics domain, either as means of estimation or as a tool to resolve motor commands directly from raw percepts. These recent advances are only a piece to the puzzle. We suggest that deep learning as a tool alone is insufficient in building a unified framework to acquire general intelligence. For this reason, we complement our survey with insights from cognitive development and refer to ideas from classical control theory, producing an integrated direction for a lifelong learning architecture.},
  keywords = {/unread,\#nosource,⛔ No DOI found,autonomy,cognition,deep learning,lifelong learning,robotics},
  annotation = {\_eprint: 1611.00201}
}

@inproceedings{xu2018,
  title = {Reinforced Continual Learning},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Xu, Ju and Zhu, Zhanxing},
  year = {2018},
  pages = {899--908},
  url = {http://papers.nips.cc/paper/7369-reinforced-continual-learning},
  abstract = {Most artificial intelligence models are limited in their ability to solve new tasks faster, without forgetting previously acquired knowledge. The recently emerging paradigm of continual learning aims to solve this issue, in which the model learns various tasks in a sequential fashion. In this work, a novel approach for continual learning is proposed, which searches for the best neural architecture for each coming task via sophisticatedly designed reinforcement learning strategies. We name it as Reinforced Continual Learning. Our method not only has good performance on preventing catastrophic forgetting but also fits new tasks well. The experiments on sequential classification tasks for variants of MNIST and CIFAR-100 datasets demonstrate that the proposed approach outperforms existing continual learning alternatives for deep networks.},
  keywords = {[cifar],[mnist],/unread,\#nosource,⛔ No DOI found}
}

@article{ye2019,
  title = {Class-{{Incremental Learning Based}} on {{Feature Extraction}} of {{CNN With Optimized Softmax}} and {{One-Class Classifiers}}},
  author = {Ye, Xin and Zhu, Qiuyu},
  year = {2019},
  journal = {IEEE Access},
  volume = {7},
  pages = {42024--42031},
  publisher = {{Institute of Electrical and Electronics Engineers Inc.}},
  issn = {2169-3536},
  doi = {10/ghn4n8},
  url = {https://ieeexplore.ieee.org/document/8666123/},
  abstract = {With the development of deep convolutional neural networks in recent years, the network structure has become more and more complicated and varied, and there are very good results in pattern recognition, image classification, scene classification, and target tracking. This end-to-end learning model relies on the initial large dataset. However, many data are gradually obtained in practical situations, which contradict the deep learning of one-time batch learning. There is an urgent need for an incremental learning approach that can continuously learn new knowledge from new data while retaining what has already been learned. This paper proposes an incremental learning algorithm based on convolutional neural network and support vector data description. CNN and AM-Softmax loss function are used to represent and continuously learn image features. Support vector data description is used to construct multiple hyperspheres for new and old classes of images. Class-incremental learning is achieved by the increment of hyperspheres. The experimental results show that the incremental learning method proposed in this paper can effectively extract the latent features of the image and adapt it to the learning situation of the class-increment. The recognition accuracy is close to batch learning.},
  keywords = {[cifar],[mnist],/unread,\#nosource,feature extraction,incremental learning,loss function,One-class classifier}
}

@inproceedings{yoon2018,
  title = {Lifelong {{Learning With Dynamically Expandable Networks}}},
  booktitle = {{{ICLR}}},
  author = {Yoon, Jaehong and Yang, Eunho and Lee, Jeongtae and Hwang, Sung Ju},
  year = {2018},
  pages = {11},
  url = {https://arxiv.org/abs/1708.01547},
  abstract = {We propose a novel deep network architecture for lifelong learning which we refer to as Dynamically Expandable Network (DEN), that can dynamically decide its network capacity as it trains on a sequence of tasks, to learn a compact overlapping knowledge sharing structure among tasks. DEN is efficiently trained in an online manner by performing selective retraining, dynamically expands network capacity upon arrival of each task with only the necessary number of units, and effectively prevents semantic drift by splitting/duplicating units and timestamping them. We validate DEN on multiple public datasets under lifelong learning scenarios, on which it not only significantly outperforms existing lifelong learning methods for deep networks, but also achieves the same level of performance as the batch counterparts with substantially fewer number of parameters. Further, the obtained network fine-tuned on all tasks obtained siginficantly better performance over the batch models, which shows that it can be used to estimate the optimal network structure even when all tasks are available in the first place.},
  langid = {english},
  keywords = {[cifar],[mnist],[sparsity],/unread,\#nosource,⛔ No DOI found,disadvantages,lifelong learning,modular,progressive},
  note = {The authors propose a method to evaluate the importance of each neuron in the network through the use of sparse connections. The network is then expanded based on the neuron importance for each task.}
}

@article{yoon2021,
  title = {Online {{Coreset Selection}} for {{Rehearsal-based Continual Learning}}},
  author = {Yoon, Jaehong and Madaan, Divyam and Yang, Eunho and Hwang, Sung Ju},
  year = {2021},
  journal = {arXiv},
  eprint = {2106.01085},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2106.01085},
  urldate = {2021-06-07},
  abstract = {A dataset is a shred of crucial evidence to describe a task. However, each data point in the dataset does not have the same potential, as some of the data points can be more representative or informative than others. This unequal importance among the data points may have a large impact in rehearsal-based continual learning, where we store a subset of the training examples (coreset) to be replayed later to alleviate catastrophic forgetting. In continual learning, the quality of the samples stored in the coreset directly affects the model's effectiveness and efficiency. The coreset selection problem becomes even more important under realistic settings, such as imbalanced continual learning or noisy data scenarios. To tackle this problem, we propose Online Coreset Selection (OCS), a simple yet effective method that selects the most representative and informative coreset at each iteration and trains them in an online manner. Our proposed method maximizes the model's adaptation to a target dataset while selecting high-affinity samples to past tasks, which directly inhibits catastrophic forgetting. We validate the effectiveness of our coreset selection mechanism over various standard, imbalanced, and noisy datasets against strong continual learning baselines, demonstrating that it improves task adaptation and prevents catastrophic forgetting in a sample-efficient manner.},
  archiveprefix = {arXiv},
  keywords = {/unread,\#nosource,⛔ No DOI found,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}

@inproceedings{zenke2017,
  title = {Continual {{Learning Through Synaptic Intelligence}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Zenke, Friedemann and Poole, Ben and Ganguli, Surya},
  year = {2017},
  pages = {3987--3995},
  url = {http://proceedings.mlr.press/v70/zenke17a.html},
  abstract = {While deep learning has led to remarkable advances across diverse applications, it struggles in domains where the data distribution changes over the course of learning. In stark contrast, biologica...},
  langid = {english},
  keywords = {[cifar],[mnist],/unread,\#nosource,mnist}
}

@inproceedings{zeno2018,
  title = {Task {{Agnostic Continual Learning Using Online Variational Bayes}}},
  booktitle = {{{NeurIPS Bayesian Deep Learning Workshop}}},
  author = {Zeno, Chen and Golan, Itay and Hoffer, Elad and Soudry, Daniel},
  year = {2018},
  url = {http://bayesiandeeplearning.org/2018/papers/58.pdf},
  abstract = {Catastrophic forgetting is the notorious vulnerability of neural networks to the change of the data distribution while learning. This phenomena has long been considered a major obstacle for allowing the use of learning agents in realistic continual learning settings. Although this vulnerability of neural networks is widely investigated, it is currently only mitigated by explicitly reacting to the change of task. We suggest a novel approach for overcoming catastrophic forgetting in neural networks, using an online version of the variational Bayes method. Having a confidence measure of the weights alleviates catastrophic forgetting and, for the first time, succeeds in this even without the knowledge of when the tasks are being switched. 2},
  keywords = {[bayes],[cifar],[mnist],/unread,\#nosource,⛔ No DOI found},
  note = {Bayesian Gradient Descent allows for Task Agnostic Continual Learning. The posterior on the parameters is updated at every batch, without needing task boundaries or task labels at training time. Experiments are performed with multi-head for Split scenarios.}
}

@article{zhai2019,
  title = {Lifelong {{Learning}} for {{Scene Recognition}} in {{Remote Sensing Images}}},
  author = {Zhai, Min and Liu, Huaping and Sun, Fuchun},
  year = {2019},
  journal = {IEEE Geoscience and Remote Sensing Letters},
  volume = {16},
  number = {9},
  pages = {1472--1476},
  publisher = {{IEEE}},
  issn = {1545-598X},
  doi = {10/ggbgt9},
  url = {https://ieeexplore.ieee.org/document/8662768/},
  abstract = {The development of visual sensing technologies has made it possible to obtain some high resolution and to gather many high-resolution satellite images. To make the best use of these images, it is essential to be able to recognize and retrieve their intrinsic scene information. The problem of scene recognition in remote sensing images has recently aroused considerable interest, mainly due to the great success achieved by deep learning methods in generic image classification. Nevertheless, such methods usually require large amounts of labeled data. By contrast, remote sensing images are relatively scarce and expensive to obtain. Moreover, data sets from different aerospace research institutions exhibit large disparities. In order to address these problems, we propose a model based on a meta-learning method with the ability of learning a classifier from just few-shot samples. With the proposed model, the knowledge learned from one data set can be easily adapted to a new data set, which, in turn, would serve in the lifelong few-shot learning. Scene-level image recognition experiments, on public high-resolution remote sensing image data sets, validate our proposed lifelong few-shot learning model.},
  keywords = {[vision],/unread,\#nosource}
}

@article{zhang2019,
  title = {Prototype {{Reminding}} for {{Continual Learning}}},
  author = {Zhang, Mengmi and Wang, Tao and Lim, Joo Hwee and Feng, Jiashi},
  year = {2019},
  journal = {arXiv},
  pages = {1--10},
  url = {http://arxiv.org/abs/1905.09447},
  abstract = {Continual learning is a critical ability of continually acquiring and transferring knowledge without catastrophically forgetting previously learned knowledge. However, enabling continual learning for AI remains a long-standing challenge. In this work, we propose a novel method, Prototype Reminding, that efficiently embeds and recalls previously learnt knowledge to tackle catastrophic forgetting issue. In particular, we consider continual learning in classification tasks. For each classification task, our method learns a metric space containing a set of prototypes where embedding of the samples from the same class cluster around prototypes and class-representative prototypes are separated apart. To alleviate catastrophic forgetting, our method preserves the embedding function from the samples to the previous metric space, through our proposed prototype reminding from previous tasks. Specifically, the reminding process is implemented by replaying a small number of samples from previous tasks and correspondingly matching their embedding to their nearest class-representative prototypes. Compared with recent continual learning methods, our contributions are fourfold: first, our method achieves the best memory retention capability while adapting quickly to new tasks. Second, our method uses metric learning for classification, and does not require adding in new neurons given new object classes. Third, our method is more memory efficient since only class-representative prototypes need to be recalled. Fourth, our method suggests a promising solution for few-shot continual learning. Without tampering with the performance on initial tasks, our method learns novel concepts given a few training examples of each class in new tasks.},
  keywords = {[bayes],[cifar],[imagenet],[mnist],/unread,\#nosource,⛔ No DOI found},
  annotation = {\_eprint: 1905.09447}
}

@article{zhang2022,
  title = {Continual {{Sequence Generation}} with {{Adaptive Compositional Modules}}},
  author = {Zhang, Yanzhe and Wang, Xuezhi and Yang, Diyi},
  year = {2022},
  journal = {ACL},
  eprint = {2203.10652},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2203.10652},
  urldate = {2022-04-06},
  abstract = {Continual learning is essential for real-world deployment when there is a need to quickly adapt the model to new tasks without forgetting knowledge of old tasks. Existing work on continual sequence generation either always reuses existing parameters to learn new tasks, which is vulnerable to catastrophic forgetting on dissimilar tasks, or blindly adds new parameters for every new task, which could prevent knowledge sharing between similar tasks. To get the best of both worlds, in this work, we propose continual sequence generation with adaptive compositional modules to adaptively add modules in transformer architectures and compose both old and new modules for new tasks. We also incorporate pseudo experience replay to facilitate knowledge transfer in those shared modules. Experiment results on various sequences of generation tasks show that our framework can adaptively add modules or reuse modules based on task similarity, outperforming state-of-the-art baselines in terms of both performance and parameter efficiency. We make our code public at https://github.com/GT-SALT/Adaptive-Compositional-Modules.},
  archiveprefix = {arXiv},
  keywords = {[nlp],/unread,⛔ No DOI found,Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  annotation = {ZSCC:00006},
  note = {Comment: 15 pages, ACL 2022}
}

@article{zhao2020,
  title = {Few-{{Shot Class-Incremental Learning}} via {{Feature Space Composition}}},
  author = {Zhao, H. and Fu, Y. and Li, X. and Li, S. and Omar, B. and Li, X.},
  year = {2020},
  journal = {arXiv},
  url = {https://arxiv.org/abs/2006.15524},
  abstract = {As a challenging problem in machine learning, few-shot class-incremental learning asynchronously learns a sequence of tasks, acquiring the new knowledge from new tasks (with limited new samples) while keeping the learned knowledge from previous tasks (with old samples discarded). In general, existing approaches resort to one unified feature space for balancing old-knowledge preserving and new-knowledge adaptation. With a limited embedding capacity of feature representation, the unified feature space often makes the learner suffer from semantic drift or overfitting as the number of tasks increases. With this motivation, we propose a novel few-shot class-incremental learning pipeline based on a composite representation space, which makes old-knowledge preserving and new-knowledge adaptation mutually compatible by feature space composition (enlarging the embedding capacity). The composite representation space is generated by integrating two space components (i.e. stable base knowledge space and dynamic lifelong-learning knowledge space) in terms of distance metric construction. With the composite feature space, our method performs remarkably well on the CUB200 and CIFAR100 datasets, outperforming the state-of-the-art algorithms by 10.58\% and 14.65\% respectively.},
  keywords = {[cifar],[cubs],/unread,\#nosource,⛔ No DOI found}
}

@article{zhao2021,
  title = {A {{Consciousness-Inspired Planning Agent}} for {{Model-Based Reinforcement Learning}}},
  author = {Zhao, Mingde and Liu, Zhen and Luan, Sitao and Zhang, Shuyuan and Precup, Doina and Bengio, Yoshua},
  year = {2021},
  journal = {arXiv},
  eprint = {2106.02097},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2106.02097},
  urldate = {2021-10-11},
  abstract = {We present an end-to-end, model-based deep reinforcement learning agent which dynamically attends to relevant parts of its state, in order to plan and to generalize better out-of-distribution. The agent uses a bottleneck mechanism over a set-based representation to force the number of entities to which the agent attends at each planning step to be small. In experiments, we investigate the bottleneck mechanism with several sets of customized environments featuring different challenges. We consistently observe that the design allows the planning agents to generalize their learned task-solving abilities in compatible unseen environments by attending to the relevant objects, leading to better out-of-distribution performance.},
  archiveprefix = {arXiv},
  keywords = {/unread,\#nosource,⛔ No DOI found,Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  note = {Comment: Post-review NeurIPS version, accepted}
}

@inproceedings{zhu2019,
  title = {Frosting {{Weights}} for {{Better Continual Training}}},
  booktitle = {2019 18th {{IEEE International Conference On Machine Learning And Applications}} ({{ICMLA}})},
  author = {Zhu, Xiaofeng and Liu, Feng and Trajcevski, Goce and Wang, Dingding},
  year = {2019},
  number = {1},
  pages = {506--510},
  publisher = {{IEEE}},
  doi = {10/gmjtfz},
  url = {https://ieeexplore.ieee.org/document/8999083/},
  abstract = {Training a neural network model can be a lifelong learning process and is a computationally intensive one. A severe adverse effect that may occur in deep neural network models is that they can suffer from catastrophic forgetting during retraining on new data. To avoid such disruptions in the continuous learning, one appealing property is the additive nature of ensemble models. In this paper, we propose two generic ensemble approaches, gradient boosting and meta-learning, to solve the catastrophic forgetting problem in tuning pre-trained neural network models.},
  isbn = {978-1-72814-550-1},
  keywords = {[cifar],[mnist],/unread,\#nosource},
  annotation = {\_eprint: 2001.01829}
}


